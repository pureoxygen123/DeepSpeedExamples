[2024-06-20 10:30:55,611] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: Permanently added '[172.28.16.12]:12222' (ECDSA) to the list of known hosts.
[2024-06-20 10:31:02,799] [INFO] [runner.py:463:main] Using IP address of 10.22.192.82 for node head
[2024-06-20 10:31:02,800] [INFO] [multinode_runner.py:80:get_cmd] Running on the following workers: head,worker1
[2024-06-20 10:31:02,800] [INFO] [runner.py:571:main] cmd = pdsh -S -f 1024 -w head,worker1 export PYTHONUNBUFFERED=0; export NCCL_IB_PCI_RELAXED_ORDERING=1; export NCCL_VERSION=2.16.2-1; export NCCL_SOCKET_IFNAME=eth1; export NCCL_DEBUG=INFO; export NCCL_IB_HCA=mlx5_1:1,mlx5_2:1,mlx5_3:1,mlx5_4:1; export NCCL_IB_GID_INDEX=7; export NCCL_IB_TIMEOUT=23; export NCCL_IB_DISABLE=0; export NCCL_IB_RETRY_CNT=7; export PYTHONPATH=/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning;  cd /fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning; /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/bin/python -u -m deepspeed.launcher.launch --world_info=eyJoZWFkIjogWzAsIDEsIDIsIDMsIDQsIDUsIDYsIDddLCAid29ya2VyMSI6IFswLCAxLCAyLCAzLCA0LCA1LCA2LCA3XX0= --node_rank=%n --master_addr=10.22.192.82 --master_port=29500 main.py --data_path Dahoas/full-hh-rlhf --data_split 2,4,4 --actor_model_name_or_path /fs-computility/llm/chenyang2/OpenLLMAI/Llama-2-7b-sft-model-ocra-500k --critic_model_name_or_path /fs-computility/llm/chenyang2/hf_model/OpenLLMAI/Llama-2-7b-rm-anthropic_hh-lmsys-oasst-webgpt/models--OpenLLMAI--Llama-2-7b-rm-anthropic_hh-lmsys-oasst-webgpt/snapshots/a982afeed00fac9767d53aecde5b88947b1be194 --num_padding_at_beginning 1 --per_device_generation_batch_size 8 --per_device_training_batch_size 8 --generation_batches 4 --gradient_accumulation_steps 4 --ppo_epochs 1 --max_answer_seq_len 1024 --max_prompt_seq_len 1024 --actor_learning_rate 9.65e-6 --critic_learning_rate 5e-6 --actor_weight_decay 0.1 --critic_weight_decay 0.1 --num_train_epochs 1 --lr_scheduler_type cosine --actor_gradient_checkpointing --critic_gradient_checkpointing --actor_dropout 0.0 --num_warmup_steps 0 --deepspeed --seed 1234 --actor_zero_stage 3 --critic_zero_stage 3 --enable_hybrid_engine --output_dir ./output_step3_llama
head: Warning: Permanently added '[172.28.16.12]:12222' (ECDSA) to the list of known hosts.
worker1: Warning: Permanently added '[172.28.16.15]:12222' (ECDSA) to the list of known hosts.
head: [2024-06-20 10:31:07,256] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
worker1: [2024-06-20 10:31:07,278] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
head: [2024-06-20 10:31:09,416] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.16.2-1+cuda11.8
head: [2024-06-20 10:31:09,416] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.16.2-1
head: [2024-06-20 10:31:09,416] [INFO] [launch.py:138:main] 0 NCCL_IB_PCI_RELAXED_ORDERING=1
head: [2024-06-20 10:31:09,416] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.16.2-1
head: [2024-06-20 10:31:09,416] [INFO] [launch.py:138:main] 0 NCCL_SOCKET_IFNAME=eth1
head: [2024-06-20 10:31:09,416] [INFO] [launch.py:138:main] 0 NCCL_DEBUG=INFO
head: [2024-06-20 10:31:09,416] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev
head: [2024-06-20 10:31:09,416] [INFO] [launch.py:138:main] 0 NCCL_IB_HCA=mlx5_1:1,mlx5_2:1,mlx5_3:1,mlx5_4:1
head: [2024-06-20 10:31:09,416] [INFO] [launch.py:138:main] 0 NCCL_IB_GID_INDEX=7
head: [2024-06-20 10:31:09,416] [INFO] [launch.py:138:main] 0 NCCL_IB_TIMEOUT=23
head: [2024-06-20 10:31:09,416] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.16.2-1+cuda11.8
head: [2024-06-20 10:31:09,416] [INFO] [launch.py:138:main] 0 NCCL_IB_DISABLE=0
head: [2024-06-20 10:31:09,416] [INFO] [launch.py:138:main] 0 NCCL_IB_RETRY_CNT=7
head: [2024-06-20 10:31:09,416] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2
head: [2024-06-20 10:31:09,416] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.16.2-1
head: [2024-06-20 10:31:09,416] [INFO] [launch.py:145:main] WORLD INFO DICT: {'head': [0, 1, 2, 3, 4, 5, 6, 7], 'worker1': [0, 1, 2, 3, 4, 5, 6, 7]}
head: [2024-06-20 10:31:09,416] [INFO] [launch.py:151:main] nnodes=2, num_local_procs=8, node_rank=0
head: [2024-06-20 10:31:09,416] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'head': [0, 1, 2, 3, 4, 5, 6, 7], 'worker1': [8, 9, 10, 11, 12, 13, 14, 15]})
head: [2024-06-20 10:31:09,416] [INFO] [launch.py:163:main] dist_world_size=16
head: [2024-06-20 10:31:09,416] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
worker1: [2024-06-20 10:31:09,889] [INFO] [launch.py:138:main] 1 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.16.2-1+cuda11.8
worker1: [2024-06-20 10:31:09,890] [INFO] [launch.py:138:main] 1 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.16.2-1
worker1: [2024-06-20 10:31:09,890] [INFO] [launch.py:138:main] 1 NCCL_IB_PCI_RELAXED_ORDERING=1
worker1: [2024-06-20 10:31:09,890] [INFO] [launch.py:138:main] 1 NCCL_VERSION=2.16.2-1
worker1: [2024-06-20 10:31:09,890] [INFO] [launch.py:138:main] 1 NCCL_SOCKET_IFNAME=eth1
worker1: [2024-06-20 10:31:09,890] [INFO] [launch.py:138:main] 1 NCCL_DEBUG=INFO
worker1: [2024-06-20 10:31:09,890] [INFO] [launch.py:138:main] 1 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev
worker1: [2024-06-20 10:31:09,890] [INFO] [launch.py:138:main] 1 NCCL_IB_HCA=mlx5_1:1,mlx5_2:1,mlx5_3:1,mlx5_4:1
worker1: [2024-06-20 10:31:09,890] [INFO] [launch.py:138:main] 1 NCCL_IB_GID_INDEX=7
worker1: [2024-06-20 10:31:09,890] [INFO] [launch.py:138:main] 1 NCCL_IB_TIMEOUT=23
worker1: [2024-06-20 10:31:09,890] [INFO] [launch.py:138:main] 1 NV_LIBNCCL_PACKAGE=libnccl2=2.16.2-1+cuda11.8
worker1: [2024-06-20 10:31:09,890] [INFO] [launch.py:138:main] 1 NCCL_IB_DISABLE=0
worker1: [2024-06-20 10:31:09,890] [INFO] [launch.py:138:main] 1 NCCL_IB_RETRY_CNT=7
worker1: [2024-06-20 10:31:09,890] [INFO] [launch.py:138:main] 1 NV_LIBNCCL_PACKAGE_NAME=libnccl2
worker1: [2024-06-20 10:31:09,890] [INFO] [launch.py:138:main] 1 NV_LIBNCCL_PACKAGE_VERSION=2.16.2-1
worker1: [2024-06-20 10:31:09,890] [INFO] [launch.py:145:main] WORLD INFO DICT: {'head': [0, 1, 2, 3, 4, 5, 6, 7], 'worker1': [0, 1, 2, 3, 4, 5, 6, 7]}
worker1: [2024-06-20 10:31:09,890] [INFO] [launch.py:151:main] nnodes=2, num_local_procs=8, node_rank=1
worker1: [2024-06-20 10:31:09,890] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'head': [0, 1, 2, 3, 4, 5, 6, 7], 'worker1': [8, 9, 10, 11, 12, 13, 14, 15]})
worker1: [2024-06-20 10:31:09,890] [INFO] [launch.py:163:main] dist_world_size=16
worker1: [2024-06-20 10:31:09,890] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
head: [2024-06-20 10:31:12,468] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
head: [2024-06-20 10:31:12,470] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
head: [2024-06-20 10:31:12,477] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
head: [2024-06-20 10:31:12,485] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
head: [2024-06-20 10:31:12,487] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
head: [2024-06-20 10:31:12,495] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
head: [2024-06-20 10:31:12,498] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
head: [2024-06-20 10:31:12,514] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
worker1: [2024-06-20 10:31:12,892] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
worker1: [2024-06-20 10:31:12,983] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
worker1: [2024-06-20 10:31:12,988] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
worker1: [2024-06-20 10:31:13,000] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
worker1: [2024-06-20 10:31:13,013] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
worker1: [2024-06-20 10:31:13,019] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
worker1: [2024-06-20 10:31:13,526] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
worker1: [2024-06-20 10:31:13,531] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
head:   warnings.warn(
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
head:   warnings.warn(
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
head:   warnings.warn(
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
worker1:   warnings.warn(
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
head:   warnings.warn(
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
head:   warnings.warn(
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
head:   warnings.warn(
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
head:   warnings.warn(
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
head:   warnings.warn(
head: [2024-06-20 10:31:14,897] [INFO] [comm.py:637:init_distributed] cdb=None
head: [2024-06-20 10:31:14,946] [INFO] [comm.py:637:init_distributed] cdb=None
worker1: [2024-06-20 10:31:14,972] [INFO] [comm.py:637:init_distributed] cdb=None
head: [2024-06-20 10:31:14,977] [INFO] [comm.py:637:init_distributed] cdb=None
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
worker1:   warnings.warn(
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
worker1:   warnings.warn(
head: [2024-06-20 10:31:15,014] [INFO] [comm.py:637:init_distributed] cdb=None
head: [2024-06-20 10:31:15,029] [INFO] [comm.py:637:init_distributed] cdb=None
head: [2024-06-20 10:31:15,031] [INFO] [comm.py:637:init_distributed] cdb=None
head: [2024-06-20 10:31:15,032] [INFO] [comm.py:637:init_distributed] cdb=None
head: [2024-06-20 10:31:15,032] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
head: [2024-06-20 10:31:15,094] [INFO] [comm.py:637:init_distributed] cdb=None
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
worker1:   warnings.warn(
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
worker1:   warnings.warn(
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
worker1:   warnings.warn(
worker1: [2024-06-20 10:31:15,312] [INFO] [comm.py:637:init_distributed] cdb=None
worker1: [2024-06-20 10:31:15,339] [INFO] [comm.py:637:init_distributed] cdb=None
worker1: [2024-06-20 10:31:15,445] [INFO] [comm.py:637:init_distributed] cdb=None
worker1: [2024-06-20 10:31:15,503] [INFO] [comm.py:637:init_distributed] cdb=None
worker1: [2024-06-20 10:31:15,564] [INFO] [comm.py:637:init_distributed] cdb=None
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
worker1:   warnings.warn(
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
worker1:   warnings.warn(
worker1: [2024-06-20 10:31:15,927] [INFO] [comm.py:637:init_distributed] cdb=None
worker1: [2024-06-20 10:31:16,009] [INFO] [comm.py:637:init_distributed] cdb=None
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:388573 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:388573 [0] NCCL INFO Bootstrap : Using eth1:172.28.16.12<0>
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:388573 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:388573 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:388573 [0] NCCL INFO cudaDriverVersion 11080
head: NCCL version 2.18.6+cuda11.8
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:388578 [5] NCCL INFO cudaDriverVersion 11080
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:388577 [4] NCCL INFO cudaDriverVersion 11080
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:388579 [6] NCCL INFO cudaDriverVersion 11080
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:388576 [3] NCCL INFO cudaDriverVersion 11080
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:388580 [7] NCCL INFO cudaDriverVersion 11080
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:388578 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:388579 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:388577 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:388576 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:388580 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:192639 [1] NCCL INFO cudaDriverVersion 11080
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:192640 [2] NCCL INFO cudaDriverVersion 11080
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:192645 [7] NCCL INFO cudaDriverVersion 11080
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:192638 [0] NCCL INFO cudaDriverVersion 11080
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:192644 [6] NCCL INFO cudaDriverVersion 11080
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:192641 [3] NCCL INFO cudaDriverVersion 11080
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:192643 [5] NCCL INFO cudaDriverVersion 11080
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:192638 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:192644 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:192643 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:192639 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:192645 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:192641 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:192640 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:192642 [4] NCCL INFO cudaDriverVersion 11080
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:192642 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:388579 [6] NCCL INFO Bootstrap : Using eth1:172.28.16.12<0>
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:388578 [5] NCCL INFO Bootstrap : Using eth1:172.28.16.12<0>
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:388577 [4] NCCL INFO Bootstrap : Using eth1:172.28.16.12<0>
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:388580 [7] NCCL INFO Bootstrap : Using eth1:172.28.16.12<0>
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:388576 [3] NCCL INFO Bootstrap : Using eth1:172.28.16.12<0>
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:388578 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:388578 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:388579 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:388577 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:388579 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:388577 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:388576 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:388580 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:388580 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:388576 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:192644 [6] NCCL INFO Bootstrap : Using eth1:172.28.16.15<0>
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:192639 [1] NCCL INFO Bootstrap : Using eth1:172.28.16.15<0>
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:192641 [3] NCCL INFO Bootstrap : Using eth1:172.28.16.15<0>
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:192643 [5] NCCL INFO Bootstrap : Using eth1:172.28.16.15<0>
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:192638 [0] NCCL INFO Bootstrap : Using eth1:172.28.16.15<0>
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:192640 [2] NCCL INFO Bootstrap : Using eth1:172.28.16.15<0>
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:192645 [7] NCCL INFO Bootstrap : Using eth1:172.28.16.15<0>
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:192642 [4] NCCL INFO Bootstrap : Using eth1:172.28.16.15<0>
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:192644 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:192638 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:192643 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:192644 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:192638 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:192643 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:192641 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:192640 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:192641 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:192640 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:192639 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:192642 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:192639 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:192642 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:192645 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:192645 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:388574 [1] NCCL INFO cudaDriverVersion 11080
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:388574 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:388574 [1] NCCL INFO Bootstrap : Using eth1:172.28.16.12<0>
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:388574 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:388574 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:390123 [0] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:390123 [0] NCCL INFO P2P plugin IBext
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:390123 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:390124 [5] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:390124 [5] NCCL INFO P2P plugin IBext
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:390124 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:390125 [4] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:390125 [4] NCCL INFO P2P plugin IBext
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:390125 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:390126 [6] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:390126 [6] NCCL INFO P2P plugin IBext
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:390126 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:390127 [3] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:390127 [3] NCCL INFO P2P plugin IBext
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:390128 [7] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:390128 [7] NCCL INFO P2P plugin IBext
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:390127 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:390128 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:390129 [1] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:390129 [1] NCCL INFO P2P plugin IBext
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:390129 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:194156 [6] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:194156 [6] NCCL INFO P2P plugin IBext
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:194156 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:194157 [3] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:194157 [3] NCCL INFO P2P plugin IBext
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:194157 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:194158 [0] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:194158 [0] NCCL INFO P2P plugin IBext
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:194158 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:194159 [1] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:194159 [1] NCCL INFO P2P plugin IBext
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:194159 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:194160 [7] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:194160 [7] NCCL INFO P2P plugin IBext
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:194160 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:194161 [5] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:194161 [5] NCCL INFO P2P plugin IBext
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:194161 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:194162 [4] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:194162 [4] NCCL INFO P2P plugin IBext
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:194162 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:194163 [2] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:194163 [2] NCCL INFO P2P plugin IBext
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:194163 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:390123 [0] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:390123 [0] NCCL INFO NET/IB : Using [0]mlx5_1:1/RoCE [1]mlx5_2:1/RoCE [2]mlx5_3:1/RoCE [3]mlx5_4:1/RoCE [RO]; OOB eth1:172.28.16.12<0>
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:390123 [0] NCCL INFO Using network IBext
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:390124 [5] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:390124 [5] NCCL INFO NET/IB : Using [0]mlx5_1:1/RoCE [1]mlx5_2:1/RoCE [2]mlx5_3:1/RoCE [3]mlx5_4:1/RoCE [RO]; OOB eth1:172.28.16.12<0>
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:390124 [5] NCCL INFO Using network IBext
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:390125 [4] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:390125 [4] NCCL INFO NET/IB : Using [0]mlx5_1:1/RoCE [1]mlx5_2:1/RoCE [2]mlx5_3:1/RoCE [3]mlx5_4:1/RoCE [RO]; OOB eth1:172.28.16.12<0>
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:390125 [4] NCCL INFO Using network IBext
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:390129 [1] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:390129 [1] NCCL INFO NET/IB : Using [0]mlx5_1:1/RoCE [1]mlx5_2:1/RoCE [2]mlx5_3:1/RoCE [3]mlx5_4:1/RoCE [RO]; OOB eth1:172.28.16.12<0>
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:390129 [1] NCCL INFO Using network IBext
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:390126 [6] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:390126 [6] NCCL INFO NET/IB : Using [0]mlx5_1:1/RoCE [1]mlx5_2:1/RoCE [2]mlx5_3:1/RoCE [3]mlx5_4:1/RoCE [RO]; OOB eth1:172.28.16.12<0>
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:390126 [6] NCCL INFO Using network IBext
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:390128 [7] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:390128 [7] NCCL INFO NET/IB : Using [0]mlx5_1:1/RoCE [1]mlx5_2:1/RoCE [2]mlx5_3:1/RoCE [3]mlx5_4:1/RoCE [RO]; OOB eth1:172.28.16.12<0>
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:390128 [7] NCCL INFO Using network IBext
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:390127 [3] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:390127 [3] NCCL INFO NET/IB : Using [0]mlx5_1:1/RoCE [1]mlx5_2:1/RoCE [2]mlx5_3:1/RoCE [3]mlx5_4:1/RoCE [RO]; OOB eth1:172.28.16.12<0>
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:390127 [3] NCCL INFO Using network IBext
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:194163 [2] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:194163 [2] NCCL INFO NET/IB : Using [0]mlx5_1:1/RoCE [1]mlx5_2:1/RoCE [2]mlx5_3:1/RoCE [3]mlx5_4:1/RoCE [RO]; OOB eth1:172.28.16.15<0>
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:194163 [2] NCCL INFO Using network IBext
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:194160 [7] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:194159 [1] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:194160 [7] NCCL INFO NET/IB : Using [0]mlx5_1:1/RoCE [1]mlx5_2:1/RoCE [2]mlx5_3:1/RoCE [3]mlx5_4:1/RoCE [RO]; OOB eth1:172.28.16.15<0>
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:194160 [7] NCCL INFO Using network IBext
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:194159 [1] NCCL INFO NET/IB : Using [0]mlx5_1:1/RoCE [1]mlx5_2:1/RoCE [2]mlx5_3:1/RoCE [3]mlx5_4:1/RoCE [RO]; OOB eth1:172.28.16.15<0>
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:194159 [1] NCCL INFO Using network IBext
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:194162 [4] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:194162 [4] NCCL INFO NET/IB : Using [0]mlx5_1:1/RoCE [1]mlx5_2:1/RoCE [2]mlx5_3:1/RoCE [3]mlx5_4:1/RoCE [RO]; OOB eth1:172.28.16.15<0>
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:194162 [4] NCCL INFO Using network IBext
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:194161 [5] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:194161 [5] NCCL INFO NET/IB : Using [0]mlx5_1:1/RoCE [1]mlx5_2:1/RoCE [2]mlx5_3:1/RoCE [3]mlx5_4:1/RoCE [RO]; OOB eth1:172.28.16.15<0>
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:194161 [5] NCCL INFO Using network IBext
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:388575 [2] NCCL INFO cudaDriverVersion 11080
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:388575 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:388575 [2] NCCL INFO Bootstrap : Using eth1:172.28.16.12<0>
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:388575 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:388575 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:194157 [3] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:194157 [3] NCCL INFO NET/IB : Using [0]mlx5_1:1/RoCE [1]mlx5_2:1/RoCE [2]mlx5_3:1/RoCE [3]mlx5_4:1/RoCE [RO]; OOB eth1:172.28.16.15<0>
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:194157 [3] NCCL INFO Using network IBext
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:194156 [6] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:194156 [6] NCCL INFO NET/IB : Using [0]mlx5_1:1/RoCE [1]mlx5_2:1/RoCE [2]mlx5_3:1/RoCE [3]mlx5_4:1/RoCE [RO]; OOB eth1:172.28.16.15<0>
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:194156 [6] NCCL INFO Using network IBext
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:194158 [0] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:194158 [0] NCCL INFO NET/IB : Using [0]mlx5_1:1/RoCE [1]mlx5_2:1/RoCE [2]mlx5_3:1/RoCE [3]mlx5_4:1/RoCE [RO]; OOB eth1:172.28.16.15<0>
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:194158 [0] NCCL INFO Using network IBext
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:390158 [2] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:390158 [2] NCCL INFO P2P plugin IBext
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:390158 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:390158 [2] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:390158 [2] NCCL INFO NET/IB : Using [0]mlx5_1:1/RoCE [1]mlx5_2:1/RoCE [2]mlx5_3:1/RoCE [3]mlx5_4:1/RoCE [RO]; OOB eth1:172.28.16.12<0>
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:390158 [2] NCCL INFO Using network IBext
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:194160 [7] NCCL INFO comm 0x55dc1df707a0 rank 15 nranks 16 cudaDev 7 nvmlDev 7 busId d8000 commId 0x429b0ef24a62a731 - Init START
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:194156 [6] NCCL INFO comm 0x559740c15550 rank 14 nranks 16 cudaDev 6 nvmlDev 6 busId d5000 commId 0x429b0ef24a62a731 - Init START
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:194161 [5] NCCL INFO comm 0x55bc37212780 rank 13 nranks 16 cudaDev 5 nvmlDev 5 busId b7000 commId 0x429b0ef24a62a731 - Init START
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:390158 [2] NCCL INFO comm 0x559622b15250 rank 2 nranks 16 cudaDev 2 nvmlDev 2 busId 69000 commId 0x429b0ef24a62a731 - Init START
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:390126 [6] NCCL INFO comm 0x561b4ee8ac20 rank 6 nranks 16 cudaDev 6 nvmlDev 6 busId d5000 commId 0x429b0ef24a62a731 - Init START
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:390128 [7] NCCL INFO comm 0x55ad380b4f50 rank 7 nranks 16 cudaDev 7 nvmlDev 7 busId d8000 commId 0x429b0ef24a62a731 - Init START
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:390129 [1] NCCL INFO comm 0x55b480f32be0 rank 1 nranks 16 cudaDev 1 nvmlDev 1 busId 52000 commId 0x429b0ef24a62a731 - Init START
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:390124 [5] NCCL INFO comm 0x55ec538406f0 rank 5 nranks 16 cudaDev 5 nvmlDev 5 busId b7000 commId 0x429b0ef24a62a731 - Init START
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:390125 [4] NCCL INFO comm 0x55cd1ef719b0 rank 4 nranks 16 cudaDev 4 nvmlDev 4 busId b3000 commId 0x429b0ef24a62a731 - Init START
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:390127 [3] NCCL INFO comm 0x5595fd4eea40 rank 3 nranks 16 cudaDev 3 nvmlDev 3 busId 6f000 commId 0x429b0ef24a62a731 - Init START
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:194162 [4] NCCL INFO comm 0x55cd0fab3ed0 rank 12 nranks 16 cudaDev 4 nvmlDev 4 busId b3000 commId 0x429b0ef24a62a731 - Init START
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:194157 [3] NCCL INFO comm 0x5561497e3400 rank 11 nranks 16 cudaDev 3 nvmlDev 3 busId 6f000 commId 0x429b0ef24a62a731 - Init START
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:194158 [0] NCCL INFO comm 0x558ebe0d5470 rank 8 nranks 16 cudaDev 0 nvmlDev 0 busId 4d000 commId 0x429b0ef24a62a731 - Init START
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:194163 [2] NCCL INFO comm 0x557df20117d0 rank 10 nranks 16 cudaDev 2 nvmlDev 2 busId 69000 commId 0x429b0ef24a62a731 - Init START
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:194159 [1] NCCL INFO comm 0x55b6f29f29a0 rank 9 nranks 16 cudaDev 1 nvmlDev 1 busId 52000 commId 0x429b0ef24a62a731 - Init START
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:390123 [0] NCCL INFO comm 0x55ef86a87a10 rank 0 nranks 16 cudaDev 0 nvmlDev 0 busId 4d000 commId 0x429b0ef24a62a731 - Init START
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:194157 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,00000000,ffffffff
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:194162 [4] NCCL INFO Setting affinity for GPU 4 to ffffffff,00000000,ffffffff,00000000
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:194161 [5] NCCL INFO Setting affinity for GPU 5 to ffffffff,00000000,ffffffff,00000000
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:194159 [1] NCCL INFO Setting affinity for GPU 1 to ffffffff,00000000,ffffffff
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:194158 [0] NCCL INFO Setting affinity for GPU 0 to ffffffff,00000000,ffffffff
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:194160 [7] NCCL INFO Setting affinity for GPU 7 to ffffffff,00000000,ffffffff,00000000
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:194163 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff,00000000,ffffffff
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:194156 [6] NCCL INFO Setting affinity for GPU 6 to ffffffff,00000000,ffffffff,00000000
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:390126 [6] NCCL INFO Setting affinity for GPU 6 to ffffffff,00000000,ffffffff,00000000
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:390128 [7] NCCL INFO Setting affinity for GPU 7 to ffffffff,00000000,ffffffff,00000000
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:390124 [5] NCCL INFO Setting affinity for GPU 5 to ffffffff,00000000,ffffffff,00000000
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:390125 [4] NCCL INFO Setting affinity for GPU 4 to ffffffff,00000000,ffffffff,00000000
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:390123 [0] NCCL INFO Setting affinity for GPU 0 to ffffffff,00000000,ffffffff
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:390129 [1] NCCL INFO Setting affinity for GPU 1 to ffffffff,00000000,ffffffff
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:390158 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff,00000000,ffffffff
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:390127 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,00000000,ffffffff
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:390158 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/10/-1->2->-1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->10 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:390158 [2] NCCL INFO P2P Chunksize set to 131072
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:194160 [7] NCCL INFO Trees [0] -1/-1/-1->15->14 [1] 8/-1/-1->15->14 [2] 8/-1/-1->15->14 [3] 8/-1/-1->15->14 [4] -1/-1/-1->15->14 [5] 8/-1/-1->15->14 [6] 8/-1/-1->15->14 [7] 8/-1/-1->15->14
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:194160 [7] NCCL INFO P2P Chunksize set to 131072
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:390129 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] -1/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] -1/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:194156 [6] NCCL INFO Trees [0] 15/-1/-1->14->13 [1] 15/-1/-1->14->13 [2] 15/-1/-1->14->13 [3] 15/-1/-1->14->6 [4] 15/-1/-1->14->13 [5] 15/-1/-1->14->13 [6] 15/-1/-1->14->13 [7] 15/6/-1->14->-1
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:194161 [5] NCCL INFO Trees [0] 14/-1/-1->13->12 [1] 14/-1/-1->13->12 [2] 14/-1/-1->13->12 [3] -1/-1/-1->13->12 [4] 14/-1/-1->13->12 [5] 14/-1/-1->13->12 [6] 14/-1/-1->13->12 [7] -1/-1/-1->13->12
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:194156 [6] NCCL INFO P2P Chunksize set to 131072
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:194161 [5] NCCL INFO P2P Chunksize set to 131072
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:194162 [4] NCCL INFO Trees [0] 13/-1/-1->12->11 [1] 13/-1/-1->12->11 [2] 13/-1/-1->12->4 [3] 13/-1/-1->12->11 [4] 13/-1/-1->12->11 [5] 13/-1/-1->12->11 [6] 13/4/-1->12->-1 [7] 13/-1/-1->12->11
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:390128 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] 0/-1/-1->7->6 [2] 0/-1/-1->7->6 [3] 0/-1/-1->7->6 [4] -1/-1/-1->7->6 [5] 0/-1/-1->7->6 [6] 0/-1/-1->7->6 [7] 0/-1/-1->7->6
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:390129 [1] NCCL INFO P2P Chunksize set to 131072
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:194162 [4] NCCL INFO P2P Chunksize set to 131072
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:390127 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2 [2] -1/-1/-1->3->2 [3] 4/-1/-1->3->2 [4] 4/-1/-1->3->2 [5] 4/-1/-1->3->2 [6] -1/-1/-1->3->2 [7] 4/-1/-1->3->2
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:390128 [7] NCCL INFO P2P Chunksize set to 131072
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:390127 [3] NCCL INFO P2P Chunksize set to 131072
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:390125 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3 [2] 5/12/-1->4->-1 [3] 5/-1/-1->4->3 [4] 5/-1/-1->4->3 [5] 5/-1/-1->4->3 [6] 5/-1/-1->4->12 [7] 5/-1/-1->4->3
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:390124 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] 6/-1/-1->5->4 [3] -1/-1/-1->5->4 [4] 6/-1/-1->5->4 [5] 6/-1/-1->5->4 [6] 6/-1/-1->5->4 [7] -1/-1/-1->5->4
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:194159 [1] NCCL INFO Trees [0] 10/-1/-1->9->8 [1] -1/-1/-1->9->8 [2] 10/-1/-1->9->8 [3] 10/-1/-1->9->8 [4] 10/-1/-1->9->8 [5] -1/-1/-1->9->8 [6] 10/-1/-1->9->8 [7] 10/-1/-1->9->8
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:194163 [2] NCCL INFO Trees [0] 11/-1/-1->10->9 [1] 11/-1/-1->10->2 [2] 11/-1/-1->10->9 [3] 11/-1/-1->10->9 [4] 11/-1/-1->10->9 [5] 11/2/-1->10->-1 [6] 11/-1/-1->10->9 [7] 11/-1/-1->10->9
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:194159 [1] NCCL INFO P2P Chunksize set to 131072
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:194163 [2] NCCL INFO P2P Chunksize set to 131072
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:390126 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->5 [3] 7/14/-1->6->-1 [4] 7/-1/-1->6->5 [5] 7/-1/-1->6->5 [6] 7/-1/-1->6->5 [7] 7/-1/-1->6->14
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:390125 [4] NCCL INFO P2P Chunksize set to 131072
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:390123 [0] NCCL INFO Channel 00/08 :    0   7   6   5   4   3   2   1   8  15  14  13  12  11  10   9
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:390124 [5] NCCL INFO P2P Chunksize set to 131072
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:390126 [6] NCCL INFO P2P Chunksize set to 131072
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:390123 [0] NCCL INFO Channel 01/08 :    0   3  10  15  14  13  12   9   8  11   2   7   6   5   4   1
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:390123 [0] NCCL INFO Channel 02/08 :    0   7   6   5  12  11  10   9   8  15  14  13   4   3   2   1
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:194157 [3] NCCL INFO Trees [0] 12/-1/-1->11->10 [1] 12/-1/-1->11->10 [2] -1/-1/-1->11->10 [3] 12/-1/-1->11->10 [4] 12/-1/-1->11->10 [5] 12/-1/-1->11->10 [6] -1/-1/-1->11->10 [7] 12/-1/-1->11->10
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:194157 [3] NCCL INFO P2P Chunksize set to 131072
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:194158 [0] NCCL INFO Trees [0] 9/-1/-1->8->0 [1] 9/-1/-1->8->15 [2] 9/-1/-1->8->15 [3] 9/-1/-1->8->15 [4] 9/0/-1->8->-1 [5] 9/-1/-1->8->15 [6] 9/-1/-1->8->15 [7] 9/-1/-1->8->15
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:194158 [0] NCCL INFO P2P Chunksize set to 131072
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:390123 [0] NCCL INFO Channel 03/08 :    0   5   4   7  14  11  10   9   8  13  12  15   6   3   2   1
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:390123 [0] NCCL INFO Channel 04/08 :    0   7   6   5   4   3   2   1   8  15  14  13  12  11  10   9
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:390123 [0] NCCL INFO Channel 05/08 :    0   3  10  15  14  13  12   9   8  11   2   7   6   5   4   1
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:390123 [0] NCCL INFO Channel 06/08 :    0   7   6   5  12  11  10   9   8  15  14  13   4   3   2   1
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:390123 [0] NCCL INFO Channel 07/08 :    0   5   4   7  14  11  10   9   8  13  12  15   6   3   2   1
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:390123 [0] NCCL INFO Trees [0] 1/8/-1->0->-1 [1] 1/-1/-1->0->7 [2] 1/-1/-1->0->7 [3] 1/-1/-1->0->7 [4] 1/-1/-1->0->8 [5] 1/-1/-1->0->7 [6] 1/-1/-1->0->7 [7] 1/-1/-1->0->7
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:390123 [0] NCCL INFO P2P Chunksize set to 131072
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:390158 [2] NCCL INFO Channel 01/0 : 2[2] -> 7[7] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:194162 [4] NCCL INFO Channel 03/0 : 12[4] -> 15[7] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:390125 [4] NCCL INFO Channel 03/0 : 4[4] -> 7[7] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:194163 [2] NCCL INFO Channel 01/0 : 10[2] -> 15[7] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:194158 [0] NCCL INFO Channel 01/0 : 8[0] -> 11[3] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:390123 [0] NCCL INFO Channel 01/0 : 0[0] -> 3[3] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:194156 [6] NCCL INFO Channel 03/0 : 7[7] -> 14[6] [receive] via NET/IBext/3/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:194162 [4] NCCL INFO Channel 07/0 : 12[4] -> 15[7] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:194156 [6] NCCL INFO Channel 07/0 : 7[7] -> 14[6] [receive] via NET/IBext/3/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:390158 [2] NCCL INFO Channel 05/0 : 2[2] -> 7[7] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:390129 [1] NCCL INFO Channel 00/0 : 1[1] -> 8[0] [send] via NET/IBext/0/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:390129 [1] NCCL INFO Channel 04/0 : 1[1] -> 8[0] [send] via NET/IBext/0/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:194159 [1] NCCL INFO Channel 00/0 : 9[1] -> 0[0] [send] via NET/IBext/0/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:390126 [6] NCCL INFO Channel 03/0 : 15[7] -> 6[6] [receive] via NET/IBext/3/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:194159 [1] NCCL INFO Channel 04/0 : 9[1] -> 0[0] [send] via NET/IBext/0/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:390126 [6] NCCL INFO Channel 07/0 : 15[7] -> 6[6] [receive] via NET/IBext/3/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:194163 [2] NCCL INFO Channel 05/0 : 10[2] -> 15[7] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:194158 [0] NCCL INFO Channel 05/0 : 8[0] -> 11[3] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:390125 [4] NCCL INFO Channel 07/0 : 4[4] -> 7[7] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:390123 [0] NCCL INFO Channel 05/0 : 0[0] -> 3[3] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:194158 [0] NCCL INFO Channel 03/0 : 8[0] -> 13[5] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:194158 [0] NCCL INFO Channel 07/0 : 8[0] -> 13[5] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:390123 [0] NCCL INFO Channel 03/0 : 0[0] -> 5[5] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:390123 [0] NCCL INFO Channel 07/0 : 0[0] -> 5[5] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:194162 [4] NCCL INFO Channel 02/0 : 5[5] -> 12[4] [receive] via NET/IBext/2/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:194162 [4] NCCL INFO Channel 06/0 : 5[5] -> 12[4] [receive] via NET/IBext/2/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:194157 [3] NCCL INFO Channel 01/0 : 11[3] -> 2[2] [send] via NET/IBext/1/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:194157 [3] NCCL INFO Channel 05/0 : 11[3] -> 2[2] [send] via NET/IBext/1/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:194161 [5] NCCL INFO Channel 02/0 : 13[5] -> 4[4] [send] via NET/IBext/2/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:194161 [5] NCCL INFO Channel 06/0 : 13[5] -> 4[4] [send] via NET/IBext/2/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:194163 [2] NCCL INFO Channel 01/0 : 3[3] -> 10[2] [receive] via NET/IBext/1/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:194163 [2] NCCL INFO Channel 05/0 : 3[3] -> 10[2] [receive] via NET/IBext/1/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:194158 [0] NCCL INFO Channel 00/0 : 1[1] -> 8[0] [receive] via NET/IBext/0/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:390127 [3] NCCL INFO Channel 01/0 : 3[3] -> 10[2] [send] via NET/IBext/1/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:390127 [3] NCCL INFO Channel 05/0 : 3[3] -> 10[2] [send] via NET/IBext/1/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:194158 [0] NCCL INFO Channel 04/0 : 1[1] -> 8[0] [receive] via NET/IBext/0/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:194158 [0] NCCL INFO Channel 00/0 : 8[0] -> 15[7] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:194158 [0] NCCL INFO Channel 02/0 : 8[0] -> 15[7] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:194160 [7] NCCL INFO Channel 03/0 : 15[7] -> 6[6] [send] via NET/IBext/3/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:194160 [7] NCCL INFO Channel 07/0 : 15[7] -> 6[6] [send] via NET/IBext/3/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:194158 [0] NCCL INFO Channel 04/0 : 8[0] -> 15[7] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:390126 [6] NCCL INFO Channel 03/0 : 6[6] -> 3[3] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:390158 [2] NCCL INFO Channel 01/0 : 11[3] -> 2[2] [receive] via NET/IBext/1/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:390158 [2] NCCL INFO Channel 05/0 : 11[3] -> 2[2] [receive] via NET/IBext/1/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:194158 [0] NCCL INFO Channel 06/0 : 8[0] -> 15[7] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:390124 [5] NCCL INFO Channel 02/0 : 5[5] -> 12[4] [send] via NET/IBext/2/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:390123 [0] NCCL INFO Channel 00/0 : 9[1] -> 0[0] [receive] via NET/IBext/0/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:390124 [5] NCCL INFO Channel 06/0 : 5[5] -> 12[4] [send] via NET/IBext/2/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:390123 [0] NCCL INFO Channel 04/0 : 9[1] -> 0[0] [receive] via NET/IBext/0/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:390123 [0] NCCL INFO Channel 00/0 : 0[0] -> 7[7] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:194162 [4] NCCL INFO Channel 01/0 : 12[4] -> 9[1] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:194160 [7] NCCL INFO Channel 00/0 : 15[7] -> 14[6] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:390125 [4] NCCL INFO Channel 02/0 : 13[5] -> 4[4] [receive] via NET/IBext/2/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:390125 [4] NCCL INFO Channel 06/0 : 13[5] -> 4[4] [receive] via NET/IBext/2/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:390125 [4] NCCL INFO Channel 01/0 : 4[4] -> 1[1] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:390123 [0] NCCL INFO Channel 02/0 : 0[0] -> 7[7] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:390126 [6] NCCL INFO Channel 07/0 : 6[6] -> 3[3] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:194160 [7] NCCL INFO Channel 01/0 : 15[7] -> 14[6] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:390128 [7] NCCL INFO Channel 03/0 : 7[7] -> 14[6] [send] via NET/IBext/3/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:390128 [7] NCCL INFO Channel 07/0 : 7[7] -> 14[6] [send] via NET/IBext/3/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:194156 [6] NCCL INFO Channel 03/0 : 14[6] -> 11[3] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:194163 [2] NCCL INFO Channel 00/0 : 10[2] -> 9[1] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:194160 [7] NCCL INFO Channel 02/0 : 15[7] -> 14[6] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:390125 [4] NCCL INFO Channel 05/0 : 4[4] -> 1[1] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:390123 [0] NCCL INFO Channel 04/0 : 0[0] -> 7[7] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:194162 [4] NCCL INFO Channel 05/0 : 12[4] -> 9[1] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:194163 [2] NCCL INFO Channel 02/0 : 10[2] -> 9[1] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:194160 [7] NCCL INFO Channel 04/0 : 15[7] -> 14[6] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:194156 [6] NCCL INFO Channel 07/0 : 14[6] -> 11[3] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:390123 [0] NCCL INFO Channel 06/0 : 0[0] -> 7[7] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:194163 [2] NCCL INFO Channel 03/0 : 10[2] -> 9[1] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:194160 [7] NCCL INFO Channel 05/0 : 15[7] -> 14[6] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:194163 [2] NCCL INFO Channel 04/0 : 10[2] -> 9[1] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:194160 [7] NCCL INFO Channel 06/0 : 15[7] -> 14[6] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:390128 [7] NCCL INFO Channel 00/0 : 7[7] -> 6[6] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:194163 [2] NCCL INFO Channel 06/0 : 10[2] -> 9[1] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:194161 [5] NCCL INFO Channel 00/0 : 13[5] -> 12[4] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:194163 [2] NCCL INFO Channel 07/0 : 10[2] -> 9[1] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:390128 [7] NCCL INFO Channel 01/0 : 7[7] -> 6[6] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:194161 [5] NCCL INFO Channel 01/0 : 13[5] -> 12[4] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:390158 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:194157 [3] NCCL INFO Channel 00/0 : 11[3] -> 10[2] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:194161 [5] NCCL INFO Channel 03/0 : 13[5] -> 12[4] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:390124 [5] NCCL INFO Channel 00/0 : 5[5] -> 4[4] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:390127 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:390128 [7] NCCL INFO Channel 02/0 : 7[7] -> 6[6] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:194157 [3] NCCL INFO Channel 02/0 : 11[3] -> 10[2] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:390158 [2] NCCL INFO Channel 02/0 : 2[2] -> 1[1] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:194161 [5] NCCL INFO Channel 04/0 : 13[5] -> 12[4] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:390124 [5] NCCL INFO Channel 01/0 : 5[5] -> 4[4] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:390127 [3] NCCL INFO Channel 02/0 : 3[3] -> 2[2] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:194157 [3] NCCL INFO Channel 03/0 : 11[3] -> 10[2] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:194161 [5] NCCL INFO Channel 05/0 : 13[5] -> 12[4] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:194156 [6] NCCL INFO Channel 00/0 : 14[6] -> 13[5] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:390128 [7] NCCL INFO Channel 04/0 : 7[7] -> 6[6] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:390158 [2] NCCL INFO Channel 03/0 : 2[2] -> 1[1] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:194157 [3] NCCL INFO Channel 04/0 : 11[3] -> 10[2] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:194161 [5] NCCL INFO Channel 07/0 : 13[5] -> 12[4] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:194156 [6] NCCL INFO Channel 01/0 : 14[6] -> 13[5] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:390127 [3] NCCL INFO Channel 03/0 : 3[3] -> 2[2] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:194157 [3] NCCL INFO Channel 06/0 : 11[3] -> 10[2] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:390124 [5] NCCL INFO Channel 03/0 : 5[5] -> 4[4] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:390126 [6] NCCL INFO Channel 00/0 : 6[6] -> 5[5] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:194156 [6] NCCL INFO Channel 02/0 : 14[6] -> 13[5] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:390128 [7] NCCL INFO Channel 05/0 : 7[7] -> 6[6] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:390158 [2] NCCL INFO Channel 04/0 : 2[2] -> 1[1] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:194157 [3] NCCL INFO Channel 07/0 : 11[3] -> 10[2] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:194156 [6] NCCL INFO Channel 04/0 : 14[6] -> 13[5] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:194162 [4] NCCL INFO Channel 00/0 : 12[4] -> 11[3] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:194159 [1] NCCL INFO Channel 01/0 : 9[1] -> 8[0] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:390127 [3] NCCL INFO Channel 04/0 : 3[3] -> 2[2] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:390124 [5] NCCL INFO Channel 04/0 : 5[5] -> 4[4] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:390126 [6] NCCL INFO Channel 01/0 : 6[6] -> 5[5] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:194156 [6] NCCL INFO Channel 05/0 : 14[6] -> 13[5] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:194162 [4] NCCL INFO Channel 02/0 : 12[4] -> 11[3] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:194159 [1] NCCL INFO Channel 02/0 : 9[1] -> 8[0] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:390125 [4] NCCL INFO Channel 00/0 : 4[4] -> 3[3] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:390128 [7] NCCL INFO Channel 06/0 : 7[7] -> 6[6] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:390129 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:194156 [6] NCCL INFO Channel 06/0 : 14[6] -> 13[5] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:390158 [2] NCCL INFO Channel 06/0 : 2[2] -> 1[1] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:194159 [1] NCCL INFO Channel 03/0 : 9[1] -> 8[0] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:194162 [4] NCCL INFO Channel 04/0 : 12[4] -> 11[3] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:390127 [3] NCCL INFO Channel 06/0 : 3[3] -> 2[2] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:390124 [5] NCCL INFO Channel 05/0 : 5[5] -> 4[4] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:390126 [6] NCCL INFO Channel 02/0 : 6[6] -> 5[5] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:194159 [1] NCCL INFO Channel 05/0 : 9[1] -> 8[0] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:194162 [4] NCCL INFO Channel 06/0 : 12[4] -> 11[3] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:390125 [4] NCCL INFO Channel 02/0 : 4[4] -> 3[3] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:390129 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:390158 [2] NCCL INFO Channel 07/0 : 2[2] -> 1[1] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:194159 [1] NCCL INFO Channel 06/0 : 9[1] -> 8[0] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:390127 [3] NCCL INFO Channel 07/0 : 3[3] -> 2[2] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:390124 [5] NCCL INFO Channel 07/0 : 5[5] -> 4[4] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:390126 [6] NCCL INFO Channel 04/0 : 6[6] -> 5[5] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:390125 [4] NCCL INFO Channel 04/0 : 4[4] -> 3[3] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:390129 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:390126 [6] NCCL INFO Channel 05/0 : 6[6] -> 5[5] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:390125 [4] NCCL INFO Channel 06/0 : 4[4] -> 3[3] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:390129 [1] NCCL INFO Channel 05/0 : 1[1] -> 0[0] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:194159 [1] NCCL INFO Channel 07/0 : 9[1] -> 8[0] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:390126 [6] NCCL INFO Channel 06/0 : 6[6] -> 5[5] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:390129 [1] NCCL INFO Channel 06/0 : 1[1] -> 0[0] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:194197 [5] NCCL INFO NCCL_IB_GID_INDEX set by environment to 7.
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:194204 [3] NCCL INFO NCCL_IB_GID_INDEX set by environment to 7.
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:390129 [1] NCCL INFO Channel 07/0 : 1[1] -> 0[0] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:194201 [1] NCCL INFO NCCL_IB_GID_INDEX set by environment to 7.
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:390169 [4] NCCL INFO NCCL_IB_GID_INDEX set by environment to 7.
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:390170 [3] NCCL INFO NCCL_IB_GID_INDEX set by environment to 7.
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:390169 [4] NCCL INFO NCCL_IB_TIMEOUT set by environment to 23.
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:390169 [4] NCCL INFO NCCL_IB_RETRY_CNT set by environment to 7.
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:390166 [1] NCCL INFO NCCL_IB_GID_INDEX set by environment to 7.
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:390167 [5] NCCL INFO NCCL_IB_GID_INDEX set by environment to 7.
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:194200 [4] NCCL INFO NCCL_IB_GID_INDEX set by environment to 7.
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:194198 [7] NCCL INFO NCCL_IB_GID_INDEX set by environment to 7.
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:390171 [6] NCCL INFO NCCL_IB_GID_INDEX set by environment to 7.
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:194200 [4] NCCL INFO NCCL_IB_TIMEOUT set by environment to 23.
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:194200 [4] NCCL INFO NCCL_IB_RETRY_CNT set by environment to 7.
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:194202 [2] NCCL INFO NCCL_IB_GID_INDEX set by environment to 7.
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:390171 [6] NCCL INFO NCCL_IB_TIMEOUT set by environment to 23.
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:390171 [6] NCCL INFO NCCL_IB_RETRY_CNT set by environment to 7.
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:194202 [2] NCCL INFO NCCL_IB_TIMEOUT set by environment to 23.
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:194202 [2] NCCL INFO NCCL_IB_RETRY_CNT set by environment to 7.
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:390165 [2] NCCL INFO NCCL_IB_GID_INDEX set by environment to 7.
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:390165 [2] NCCL INFO NCCL_IB_TIMEOUT set by environment to 23.
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:390165 [2] NCCL INFO NCCL_IB_RETRY_CNT set by environment to 7.
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:194203 [0] NCCL INFO NCCL_IB_GID_INDEX set by environment to 7.
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:194203 [0] NCCL INFO NCCL_IB_TIMEOUT set by environment to 23.
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:194203 [0] NCCL INFO NCCL_IB_RETRY_CNT set by environment to 7.
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:390168 [7] NCCL INFO NCCL_IB_GID_INDEX set by environment to 7.
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:194199 [6] NCCL INFO NCCL_IB_GID_INDEX set by environment to 7.
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:194199 [6] NCCL INFO NCCL_IB_TIMEOUT set by environment to 23.
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:194199 [6] NCCL INFO NCCL_IB_RETRY_CNT set by environment to 7.
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:390172 [0] NCCL INFO NCCL_IB_GID_INDEX set by environment to 7.
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:390172 [0] NCCL INFO NCCL_IB_TIMEOUT set by environment to 23.
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:390172 [0] NCCL INFO NCCL_IB_RETRY_CNT set by environment to 7.
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:194161 [5] NCCL INFO Connected all rings
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:390129 [1] NCCL INFO Connected all rings
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:390127 [3] NCCL INFO Connected all rings
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:194157 [3] NCCL INFO Connected all rings
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:194159 [1] NCCL INFO Connected all rings
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:390126 [6] NCCL INFO Connected all rings
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:194160 [7] NCCL INFO Connected all rings
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:390124 [5] NCCL INFO Connected all rings
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:194158 [0] NCCL INFO Connected all rings
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:194158 [0] NCCL INFO Channel 00/0 : 8[0] -> 9[1] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:194158 [0] NCCL INFO Channel 01/0 : 8[0] -> 9[1] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:390129 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:390128 [7] NCCL INFO Connected all rings
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:390129 [1] NCCL INFO Channel 02/0 : 1[1] -> 2[2] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:194158 [0] NCCL INFO Channel 02/0 : 8[0] -> 9[1] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:194161 [5] NCCL INFO Channel 00/0 : 13[5] -> 14[6] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:390127 [3] NCCL INFO Channel 00/0 : 3[3] -> 4[4] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:390129 [1] NCCL INFO Channel 03/0 : 1[1] -> 2[2] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:390125 [4] NCCL INFO Connected all rings
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:194162 [4] NCCL INFO Connected all rings
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:194158 [0] NCCL INFO Channel 03/0 : 8[0] -> 9[1] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:390127 [3] NCCL INFO Channel 01/0 : 3[3] -> 4[4] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:390129 [1] NCCL INFO Channel 04/0 : 1[1] -> 2[2] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:194161 [5] NCCL INFO Channel 01/0 : 13[5] -> 14[6] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:390126 [6] NCCL INFO Channel 00/0 : 6[6] -> 7[7] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:194163 [2] NCCL INFO Connected all rings
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:390127 [3] NCCL INFO Channel 03/0 : 3[3] -> 4[4] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:390129 [1] NCCL INFO Channel 06/0 : 1[1] -> 2[2] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:194158 [0] NCCL INFO Channel 04/0 : 8[0] -> 9[1] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:194161 [5] NCCL INFO Channel 02/0 : 13[5] -> 14[6] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:390126 [6] NCCL INFO Channel 01/0 : 6[6] -> 7[7] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:390158 [2] NCCL INFO Connected all rings
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:390127 [3] NCCL INFO Channel 04/0 : 3[3] -> 4[4] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:390129 [1] NCCL INFO Channel 07/0 : 1[1] -> 2[2] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:194157 [3] NCCL INFO Channel 00/0 : 11[3] -> 12[4] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:390126 [6] NCCL INFO Channel 02/0 : 6[6] -> 7[7] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:390123 [0] NCCL INFO Connected all rings
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:390123 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:194158 [0] NCCL INFO Channel 05/0 : 8[0] -> 9[1] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:390127 [3] NCCL INFO Channel 05/0 : 3[3] -> 4[4] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:194161 [5] NCCL INFO Channel 04/0 : 13[5] -> 14[6] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:390126 [6] NCCL INFO Channel 03/0 : 6[6] -> 7[7] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:390124 [5] NCCL INFO Channel 00/0 : 5[5] -> 6[6] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:194156 [6] NCCL INFO Connected all rings
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:390123 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:390127 [3] NCCL INFO Channel 07/0 : 3[3] -> 4[4] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:194157 [3] NCCL INFO Channel 01/0 : 11[3] -> 12[4] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:194158 [0] NCCL INFO Channel 06/0 : 8[0] -> 9[1] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:390126 [6] NCCL INFO Channel 04/0 : 6[6] -> 7[7] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:194159 [1] NCCL INFO Channel 00/0 : 9[1] -> 10[2] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:194161 [5] NCCL INFO Channel 05/0 : 13[5] -> 14[6] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:390124 [5] NCCL INFO Channel 01/0 : 5[5] -> 6[6] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:390123 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:194157 [3] NCCL INFO Channel 03/0 : 11[3] -> 12[4] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:390125 [4] NCCL INFO Channel 00/0 : 4[4] -> 5[5] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:194158 [0] NCCL INFO Channel 07/0 : 8[0] -> 9[1] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:390126 [6] NCCL INFO Channel 05/0 : 6[6] -> 7[7] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:390124 [5] NCCL INFO Channel 02/0 : 5[5] -> 6[6] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:390123 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:194159 [1] NCCL INFO Channel 02/0 : 9[1] -> 10[2] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:194161 [5] NCCL INFO Channel 06/0 : 13[5] -> 14[6] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:390125 [4] NCCL INFO Channel 01/0 : 4[4] -> 5[5] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:390126 [6] NCCL INFO Channel 06/0 : 6[6] -> 7[7] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:390124 [5] NCCL INFO Channel 04/0 : 5[5] -> 6[6] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:194157 [3] NCCL INFO Channel 04/0 : 11[3] -> 12[4] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:390123 [0] NCCL INFO Channel 04/0 : 0[0] -> 1[1] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:194159 [1] NCCL INFO Channel 03/0 : 9[1] -> 10[2] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:390125 [4] NCCL INFO Channel 02/0 : 4[4] -> 5[5] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:390126 [6] NCCL INFO Channel 07/0 : 6[6] -> 7[7] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:390124 [5] NCCL INFO Channel 05/0 : 5[5] -> 6[6] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:194157 [3] NCCL INFO Channel 05/0 : 11[3] -> 12[4] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:194162 [4] NCCL INFO Channel 00/0 : 12[4] -> 13[5] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:390123 [0] NCCL INFO Channel 05/0 : 0[0] -> 1[1] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:390158 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:194159 [1] NCCL INFO Channel 04/0 : 9[1] -> 10[2] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:390125 [4] NCCL INFO Channel 03/0 : 4[4] -> 5[5] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:194163 [2] NCCL INFO Channel 00/0 : 10[2] -> 11[3] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:390124 [5] NCCL INFO Channel 06/0 : 5[5] -> 6[6] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:194157 [3] NCCL INFO Channel 07/0 : 11[3] -> 12[4] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:194162 [4] NCCL INFO Channel 01/0 : 12[4] -> 13[5] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:390158 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:390123 [0] NCCL INFO Channel 06/0 : 0[0] -> 1[1] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:194159 [1] NCCL INFO Channel 06/0 : 9[1] -> 10[2] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:390125 [4] NCCL INFO Channel 04/0 : 4[4] -> 5[5] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:194163 [2] NCCL INFO Channel 01/0 : 10[2] -> 11[3] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:390126 [6] NCCL INFO Channel 03/0 : 14[6] -> 6[6] [receive] via NET/IBext/3/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:390126 [6] NCCL INFO Channel 07/0 : 14[6] -> 6[6] [receive] via NET/IBext/3/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:194162 [4] NCCL INFO Channel 02/0 : 12[4] -> 13[5] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:390126 [6] NCCL INFO Channel 03/0 : 6[6] -> 14[6] [send] via NET/IBext/3/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:390126 [6] NCCL INFO Channel 07/0 : 6[6] -> 14[6] [send] via NET/IBext/3/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:390158 [2] NCCL INFO Channel 02/0 : 2[2] -> 3[3] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:390123 [0] NCCL INFO Channel 07/0 : 0[0] -> 1[1] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:194159 [1] NCCL INFO Channel 07/0 : 9[1] -> 10[2] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:194163 [2] NCCL INFO Channel 02/0 : 10[2] -> 11[3] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:390125 [4] NCCL INFO Channel 05/0 : 4[4] -> 5[5] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:194156 [6] NCCL INFO Channel 00/0 : 14[6] -> 15[7] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:194162 [4] NCCL INFO Channel 03/0 : 12[4] -> 13[5] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:390158 [2] NCCL INFO Channel 03/0 : 2[2] -> 3[3] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:390123 [0] NCCL INFO Channel 01/0 : 0[0] -> 7[7] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:194158 [0] NCCL INFO Channel 01/0 : 8[0] -> 15[7] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:194163 [2] NCCL INFO Channel 03/0 : 10[2] -> 11[3] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:390125 [4] NCCL INFO Channel 06/0 : 4[4] -> 5[5] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:194156 [6] NCCL INFO Channel 01/0 : 14[6] -> 15[7] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:194162 [4] NCCL INFO Channel 04/0 : 12[4] -> 13[5] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:390158 [2] NCCL INFO Channel 04/0 : 2[2] -> 3[3] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:390123 [0] NCCL INFO Channel 03/0 : 0[0] -> 7[7] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:194158 [0] NCCL INFO Channel 03/0 : 8[0] -> 15[7] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:194163 [2] NCCL INFO Channel 04/0 : 10[2] -> 11[3] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:390125 [4] NCCL INFO Channel 07/0 : 4[4] -> 5[5] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:194156 [6] NCCL INFO Channel 02/0 : 14[6] -> 15[7] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:194162 [4] NCCL INFO Channel 05/0 : 12[4] -> 13[5] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:390158 [2] NCCL INFO Channel 05/0 : 2[2] -> 3[3] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:390123 [0] NCCL INFO Channel 05/0 : 0[0] -> 7[7] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:390124 [5] NCCL INFO Channel 02/0 : 5[5] -> 4[4] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:390125 [4] NCCL INFO Channel 02/0 : 12[4] -> 4[4] [receive] via NET/IBext/2/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:390125 [4] NCCL INFO Channel 06/0 : 12[4] -> 4[4] [receive] via NET/IBext/2/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:390125 [4] NCCL INFO Channel 02/0 : 4[4] -> 12[4] [send] via NET/IBext/2/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:194163 [2] NCCL INFO Channel 05/0 : 10[2] -> 11[3] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:194158 [0] NCCL INFO Channel 05/0 : 8[0] -> 15[7] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:390125 [4] NCCL INFO Channel 06/0 : 4[4] -> 12[4] [send] via NET/IBext/2/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:390158 [2] NCCL INFO Channel 06/0 : 2[2] -> 3[3] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:390123 [0] NCCL INFO Channel 07/0 : 0[0] -> 7[7] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:194156 [6] NCCL INFO Channel 03/0 : 14[6] -> 15[7] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:194162 [4] NCCL INFO Channel 06/0 : 12[4] -> 13[5] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:390124 [5] NCCL INFO Channel 06/0 : 5[5] -> 4[4] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:390158 [2] NCCL INFO Channel 07/0 : 2[2] -> 3[3] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:390128 [7] NCCL INFO Channel 01/0 : 7[7] -> 0[0] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:194163 [2] NCCL INFO Channel 06/0 : 10[2] -> 11[3] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:194158 [0] NCCL INFO Channel 07/0 : 8[0] -> 15[7] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:390123 [0] NCCL INFO Channel 00/0 : 8[0] -> 0[0] [receive] via NET/IBext/0/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:390123 [0] NCCL INFO Channel 04/0 : 8[0] -> 0[0] [receive] via NET/IBext/0/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:390123 [0] NCCL INFO Channel 00/0 : 0[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:390123 [0] NCCL INFO Channel 04/0 : 0[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:194156 [6] NCCL INFO Channel 04/0 : 14[6] -> 15[7] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:194162 [4] NCCL INFO Channel 07/0 : 12[4] -> 13[5] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:390129 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:390128 [7] NCCL INFO Channel 02/0 : 7[7] -> 0[0] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:390158 [2] NCCL INFO Channel 01/0 : 10[2] -> 2[2] [receive] via NET/IBext/1/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:390158 [2] NCCL INFO Channel 05/0 : 10[2] -> 2[2] [receive] via NET/IBext/1/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:390158 [2] NCCL INFO Channel 01/0 : 2[2] -> 10[2] [send] via NET/IBext/1/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:390158 [2] NCCL INFO Channel 05/0 : 2[2] -> 10[2] [send] via NET/IBext/1/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:194163 [2] NCCL INFO Channel 07/0 : 10[2] -> 11[3] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:194156 [6] NCCL INFO Channel 05/0 : 14[6] -> 15[7] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:194162 [4] NCCL INFO Channel 02/0 : 4[4] -> 12[4] [receive] via NET/IBext/2/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:390129 [1] NCCL INFO Channel 04/0 : 1[1] -> 0[0] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:390128 [7] NCCL INFO Channel 03/0 : 7[7] -> 0[0] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:194162 [4] NCCL INFO Channel 06/0 : 4[4] -> 12[4] [receive] via NET/IBext/2/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:194162 [4] NCCL INFO Channel 02/0 : 12[4] -> 4[4] [send] via NET/IBext/2/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:194162 [4] NCCL INFO Channel 06/0 : 12[4] -> 4[4] [send] via NET/IBext/2/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:194159 [1] NCCL INFO Channel 00/0 : 9[1] -> 8[0] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:194163 [2] NCCL INFO Channel 01/0 : 2[2] -> 10[2] [receive] via NET/IBext/1/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:194163 [2] NCCL INFO Channel 05/0 : 2[2] -> 10[2] [receive] via NET/IBext/1/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:194163 [2] NCCL INFO Channel 01/0 : 10[2] -> 2[2] [send] via NET/IBext/1/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:194163 [2] NCCL INFO Channel 05/0 : 10[2] -> 2[2] [send] via NET/IBext/1/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:194156 [6] NCCL INFO Channel 06/0 : 14[6] -> 15[7] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:390128 [7] NCCL INFO Channel 05/0 : 7[7] -> 0[0] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:194159 [1] NCCL INFO Channel 04/0 : 9[1] -> 8[0] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:194156 [6] NCCL INFO Channel 07/0 : 14[6] -> 15[7] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:390128 [7] NCCL INFO Channel 06/0 : 7[7] -> 0[0] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:390125 [4] NCCL INFO Channel 01/0 : 4[4] -> 3[3] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:194162 [4] NCCL INFO Channel 01/0 : 12[4] -> 11[3] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:194161 [5] NCCL INFO Channel 02/0 : 13[5] -> 12[4] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:194156 [6] NCCL INFO Channel 03/0 : 6[6] -> 14[6] [receive] via NET/IBext/3/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:194156 [6] NCCL INFO Channel 07/0 : 6[6] -> 14[6] [receive] via NET/IBext/3/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:194156 [6] NCCL INFO Channel 03/0 : 14[6] -> 6[6] [send] via NET/IBext/3/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:194156 [6] NCCL INFO Channel 07/0 : 14[6] -> 6[6] [send] via NET/IBext/3/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:194162 [4] NCCL INFO Channel 03/0 : 12[4] -> 11[3] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:390127 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:390128 [7] NCCL INFO Channel 07/0 : 7[7] -> 0[0] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:194161 [5] NCCL INFO Channel 06/0 : 13[5] -> 12[4] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:390125 [4] NCCL INFO Channel 03/0 : 4[4] -> 3[3] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:194157 [3] NCCL INFO Channel 01/0 : 11[3] -> 10[2] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:194162 [4] NCCL INFO Channel 05/0 : 12[4] -> 11[3] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:194157 [3] NCCL INFO Channel 05/0 : 11[3] -> 10[2] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:194162 [4] NCCL INFO Channel 07/0 : 12[4] -> 11[3] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:390127 [3] NCCL INFO Channel 05/0 : 3[3] -> 2[2] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:390125 [4] NCCL INFO Channel 05/0 : 4[4] -> 3[3] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:194160 [7] NCCL INFO Channel 01/0 : 15[7] -> 8[0] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:194158 [0] NCCL INFO Channel 00/0 : 0[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:194158 [0] NCCL INFO Channel 04/0 : 0[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:194158 [0] NCCL INFO Channel 00/0 : 8[0] -> 0[0] [send] via NET/IBext/0/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:194158 [0] NCCL INFO Channel 04/0 : 8[0] -> 0[0] [send] via NET/IBext/0/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:390125 [4] NCCL INFO Channel 07/0 : 4[4] -> 3[3] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:194160 [7] NCCL INFO Channel 02/0 : 15[7] -> 8[0] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:194160 [7] NCCL INFO Channel 03/0 : 15[7] -> 8[0] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:194160 [7] NCCL INFO Channel 05/0 : 15[7] -> 8[0] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:194160 [7] NCCL INFO Channel 06/0 : 15[7] -> 8[0] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:390128 [7] NCCL INFO Channel 03/0 : 7[7] -> 6[6] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:194160 [7] NCCL INFO Channel 07/0 : 15[7] -> 8[0] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:390128 [7] NCCL INFO Channel 07/0 : 7[7] -> 6[6] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:194160 [7] NCCL INFO Channel 03/0 : 15[7] -> 14[6] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:194160 [7] NCCL INFO Channel 07/0 : 15[7] -> 14[6] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:194161 [5] NCCL INFO Connected all trees
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:194161 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:194161 [5] NCCL INFO 8 coll channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:390124 [5] NCCL INFO Connected all trees
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:390124 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:390124 [5] NCCL INFO 8 coll channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:194157 [3] NCCL INFO Connected all trees
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:194157 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:194157 [3] NCCL INFO 8 coll channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:390127 [3] NCCL INFO Connected all trees
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:390127 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:390127 [3] NCCL INFO 8 coll channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:390129 [1] NCCL INFO Connected all trees
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:390129 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:390129 [1] NCCL INFO 8 coll channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:194159 [1] NCCL INFO Connected all trees
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:194159 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:194159 [1] NCCL INFO 8 coll channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:390128 [7] NCCL INFO Connected all trees
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:390128 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:390128 [7] NCCL INFO 8 coll channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:194160 [7] NCCL INFO Connected all trees
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:194160 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:194160 [7] NCCL INFO 8 coll channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:390158 [2] NCCL INFO Connected all trees
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:390158 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:390158 [2] NCCL INFO 8 coll channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:194163 [2] NCCL INFO Connected all trees
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:194163 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:194163 [2] NCCL INFO 8 coll channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:390125 [4] NCCL INFO Connected all trees
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:390125 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:390125 [4] NCCL INFO 8 coll channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:194162 [4] NCCL INFO Connected all trees
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:194162 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:194162 [4] NCCL INFO 8 coll channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:390126 [6] NCCL INFO Connected all trees
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:390126 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:390126 [6] NCCL INFO 8 coll channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:194156 [6] NCCL INFO Connected all trees
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:194156 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:194156 [6] NCCL INFO 8 coll channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:194158 [0] NCCL INFO Connected all trees
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:194158 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:194158 [0] NCCL INFO 8 coll channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:390123 [0] NCCL INFO Connected all trees
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:390123 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:390123 [0] NCCL INFO 8 coll channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:194160 [7] NCCL INFO comm 0x55dc1df707a0 rank 15 nranks 16 cudaDev 7 nvmlDev 7 busId d8000 commId 0x429b0ef24a62a731 - Init COMPLETE
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:194157 [3] NCCL INFO comm 0x5561497e3400 rank 11 nranks 16 cudaDev 3 nvmlDev 3 busId 6f000 commId 0x429b0ef24a62a731 - Init COMPLETE
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:194161 [5] NCCL INFO comm 0x55bc37212780 rank 13 nranks 16 cudaDev 5 nvmlDev 5 busId b7000 commId 0x429b0ef24a62a731 - Init COMPLETE
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:194156 [6] NCCL INFO comm 0x559740c15550 rank 14 nranks 16 cudaDev 6 nvmlDev 6 busId d5000 commId 0x429b0ef24a62a731 - Init COMPLETE
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:194159 [1] NCCL INFO comm 0x55b6f29f29a0 rank 9 nranks 16 cudaDev 1 nvmlDev 1 busId 52000 commId 0x429b0ef24a62a731 - Init COMPLETE
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:194162 [4] NCCL INFO comm 0x55cd0fab3ed0 rank 12 nranks 16 cudaDev 4 nvmlDev 4 busId b3000 commId 0x429b0ef24a62a731 - Init COMPLETE
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:194158 [0] NCCL INFO comm 0x558ebe0d5470 rank 8 nranks 16 cudaDev 0 nvmlDev 0 busId 4d000 commId 0x429b0ef24a62a731 - Init COMPLETE
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:194163 [2] NCCL INFO comm 0x557df20117d0 rank 10 nranks 16 cudaDev 2 nvmlDev 2 busId 69000 commId 0x429b0ef24a62a731 - Init COMPLETE
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:390128 [7] NCCL INFO comm 0x55ad380b4f50 rank 7 nranks 16 cudaDev 7 nvmlDev 7 busId d8000 commId 0x429b0ef24a62a731 - Init COMPLETE
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:390126 [6] NCCL INFO comm 0x561b4ee8ac20 rank 6 nranks 16 cudaDev 6 nvmlDev 6 busId d5000 commId 0x429b0ef24a62a731 - Init COMPLETE
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:390127 [3] NCCL INFO comm 0x5595fd4eea40 rank 3 nranks 16 cudaDev 3 nvmlDev 3 busId 6f000 commId 0x429b0ef24a62a731 - Init COMPLETE
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:390124 [5] NCCL INFO comm 0x55ec538406f0 rank 5 nranks 16 cudaDev 5 nvmlDev 5 busId b7000 commId 0x429b0ef24a62a731 - Init COMPLETE
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:390129 [1] NCCL INFO comm 0x55b480f32be0 rank 1 nranks 16 cudaDev 1 nvmlDev 1 busId 52000 commId 0x429b0ef24a62a731 - Init COMPLETE
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:390158 [2] NCCL INFO comm 0x559622b15250 rank 2 nranks 16 cudaDev 2 nvmlDev 2 busId 69000 commId 0x429b0ef24a62a731 - Init COMPLETE
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:390125 [4] NCCL INFO comm 0x55cd1ef719b0 rank 4 nranks 16 cudaDev 4 nvmlDev 4 busId b3000 commId 0x429b0ef24a62a731 - Init COMPLETE
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:390123 [0] NCCL INFO comm 0x55ef86a87a10 rank 0 nranks 16 cudaDev 0 nvmlDev 0 busId 4d000 commId 0x429b0ef24a62a731 - Init COMPLETE
worker1: Setting model_config.attention_dropout to 0.0
worker1: Setting model_config.attention_dropout to 0.0
head: Setting model_config.attention_dropout to 0.0
worker1: Setting model_config.attention_dropout to 0.0
head: Setting model_config.attention_dropout to 0.0
worker1: Setting model_config.attention_dropout to 0.0
worker1: Setting model_config.attention_dropout to 0.0
worker1: Setting model_config.attention_dropout to 0.0
head: Setting model_config.attention_dropout to 0.0
worker1: Setting model_config.attention_dropout to 0.0
head: ************************[start] Initializing Actor Model [start] *************************
head: Setting model_config.attention_dropout to 0.0
head: Setting model_config.attention_dropout to 0.0
worker1: Setting model_config.attention_dropout to 0.0
head: Setting model_config.attention_dropout to 0.0
head: Setting model_config.attention_dropout to 0.0
head: Setting model_config.attention_dropout to 0.0
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:390175 [3] NCCL INFO NCCL_IB_TIMEOUT set by environment to 23.
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:390175 [3] NCCL INFO NCCL_IB_RETRY_CNT set by environment to 7.
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:194208 [3] NCCL INFO NCCL_IB_TIMEOUT set by environment to 23.
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:194208 [3] NCCL INFO NCCL_IB_RETRY_CNT set by environment to 7.
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:390177 [5] NCCL INFO NCCL_IB_TIMEOUT set by environment to 23.
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:390177 [5] NCCL INFO NCCL_IB_RETRY_CNT set by environment to 7.
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:390173 [1] NCCL INFO NCCL_IB_TIMEOUT set by environment to 23.
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:390173 [1] NCCL INFO NCCL_IB_RETRY_CNT set by environment to 7.
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:194209 [5] NCCL INFO NCCL_IB_TIMEOUT set by environment to 23.
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:194209 [5] NCCL INFO NCCL_IB_RETRY_CNT set by environment to 7.
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:390180 [7] NCCL INFO NCCL_IB_TIMEOUT set by environment to 23.
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:390180 [7] NCCL INFO NCCL_IB_RETRY_CNT set by environment to 7.
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:194212 [7] NCCL INFO NCCL_IB_TIMEOUT set by environment to 23.
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:194212 [7] NCCL INFO NCCL_IB_RETRY_CNT set by environment to 7.
head: [2024-06-20 10:31:28,460] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:194206 [1] NCCL INFO NCCL_IB_TIMEOUT set by environment to 23.
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:194206 [1] NCCL INFO NCCL_IB_RETRY_CNT set by environment to 7.
worker1: Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.08s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.09s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.04s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.14s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.15s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.18s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.15s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.22s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.48s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.52s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.50s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.51s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.49s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.52s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.51s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.55s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.48s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.43s/it]
worker1: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.50s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.46s/it]
worker1: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.49s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.46s/it]
worker1: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.48s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.46s/it]
worker1: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.49s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.46s/it]
worker1: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.51s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.47s/it]
worker1: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.52s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.48s/it]
head: Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.14s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.06s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.05s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.17s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.09s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.25s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.27s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:03,  1.99s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.53s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.54s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.50s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.50s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.50s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.54s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.56s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.91s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.50s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.48s/it]
head: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.49s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.48s/it]
head: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.51s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.48s/it]
head: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.52s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.49s/it]
head: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.50s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.46s/it]
worker1: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.53s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.49s/it]
head: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.50s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.45s/it]
head: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.51s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.47s/it]
worker1: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
worker1: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
worker1: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
worker1: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
worker1: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
worker1: Detected CUDA files, patching ldflags
worker1: Emitting ninja build file /root/.cache/torch_extensions/py310_cu118/fused_adam/build.ninja...
worker1: Building extension module fused_adam...
worker1: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
worker1: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
worker1: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
worker1: ninja: no work to do.
worker1: Loading extension module fused_adam...
worker1: Time to load fused_adam op: 0.06669926643371582 seconds
worker1: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
worker1: Detected CUDA files, patching ldflags
worker1: Emitting ninja build file /root/.cache/torch_extensions/py310_cu118/fused_adam/build.ninja...
worker1: Building extension module fused_adam...
worker1: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
head: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
head: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
head: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
worker1: ninja: no work to do.
worker1: Loading extension module fused_adam...
worker1: Time to load fused_adam op: 0.06355690956115723 seconds
head: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
head: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
head: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
head: Detected CUDA files, patching ldflags
head: Emitting ninja build file /root/.cache/torch_extensions/py310_cu118/fused_adam/build.ninja...
head: Building extension module fused_adam...
head: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
head: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
head: ninja: no work to do.
worker1: Loading extension module fused_adam...
worker1: Time to load fused_adam op: 0.20121550559997559 seconds
worker1: Loading extension module fused_adam...
worker1: Time to load fused_adam op: 0.2011256217956543 seconds
head: Loading extension module fused_adam...
head: Time to load fused_adam op: 0.08211374282836914 seconds
worker1: Loading extension module fused_adam...
worker1: Time to load fused_adam op: 0.2011251449584961 seconds
head: Loading extension module fused_adam...
worker1: Loading extension module fused_adam...
head: Time to load fused_adam op: 0.1013038158416748 seconds
worker1: Time to load fused_adam op: 0.20120716094970703 seconds
head: Loading extension module fused_adam...
head: Time to load fused_adam op: 0.10113763809204102 seconds
worker1: Loading extension module fused_adam...
worker1: Time to load fused_adam op: 0.2010936737060547 seconds
worker1: Loading extension module fused_adam...
worker1: Time to load fused_adam op: 0.2012193202972412 seconds
head: Loading extension module fused_adam...
head: Time to load fused_adam op: 0.10156440734863281 seconds
head: Loading extension module fused_adam...
head: Time to load fused_adam op: 0.1011660099029541 seconds
head: Loading extension module fused_adam...
head: Time to load fused_adam op: 0.10117769241333008 seconds
head: Loading extension module fused_adam...
head: Time to load fused_adam op: 0.10165739059448242 seconds
head: Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.68s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.75s/it]
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
worker1:   self._dummy_overflow_buf = get_accelerator().IntTensor([0])
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
worker1:   self._dummy_overflow_buf = get_accelerator().IntTensor([0])
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
worker1:   self._dummy_overflow_buf = get_accelerator().IntTensor([0])
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
worker1:   self._dummy_overflow_buf = get_accelerator().IntTensor([0])
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
worker1:   self._dummy_overflow_buf = get_accelerator().IntTensor([0])
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
worker1:   self._dummy_overflow_buf = get_accelerator().IntTensor([0])
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
worker1:   self._dummy_overflow_buf = get_accelerator().IntTensor([0])
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
head:   self._dummy_overflow_buf = get_accelerator().IntTensor([0])
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
worker1:   self._dummy_overflow_buf = get_accelerator().IntTensor([0])
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
head:   self._dummy_overflow_buf = get_accelerator().IntTensor([0])
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
head:   self._dummy_overflow_buf = get_accelerator().IntTensor([0])
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
head:   self._dummy_overflow_buf = get_accelerator().IntTensor([0])
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
head:   self._dummy_overflow_buf = get_accelerator().IntTensor([0])
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
head:   self._dummy_overflow_buf = get_accelerator().IntTensor([0])
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
head:   self._dummy_overflow_buf = get_accelerator().IntTensor([0])
head: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
head: Detected CUDA files, patching ldflags
head: Emitting ninja build file /root/.cache/torch_extensions/py310_cu118/fused_adam/build.ninja...
head: Building extension module fused_adam...
head: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
head: ninja: no work to do.
head: Loading extension module fused_adam...
head: Time to load fused_adam op: 0.06663632392883301 seconds
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
head:   self._dummy_overflow_buf = get_accelerator().IntTensor([0])
head: [2024-06-20 10:31:34,022] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.6+b1e9b0fa, git-hash=b1e9b0fa, git-branch=main
head: [2024-06-20 10:31:34,041] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
head: [2024-06-20 10:31:34,042] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
head: [2024-06-20 10:31:34,042] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
head: [2024-06-20 10:31:34,054] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
head: [2024-06-20 10:31:34,054] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
head: [2024-06-20 10:31:34,054] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
head: [2024-06-20 10:31:34,054] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 3 optimizer
head: [2024-06-20 10:31:34,351] [INFO] [utils.py:791:see_memory_usage] Stage 3 initialize beginning
head: [2024-06-20 10:31:34,352] [INFO] [utils.py:792:see_memory_usage] MA 1.31 GB         Max_MA 1.89 GB         CA 12.42 GB         Max_CA 12 GB 
head: [2024-06-20 10:31:34,352] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 28.88 GB, percent = 1.5%
head: [2024-06-20 10:31:34,354] [INFO] [stage3.py:127:__init__] Reduce bucket size 500,000,000
head: [2024-06-20 10:31:34,354] [INFO] [stage3.py:128:__init__] Prefetch bucket size 30000000
head: [2024-06-20 10:31:34,605] [INFO] [utils.py:791:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
head: [2024-06-20 10:31:34,606] [INFO] [utils.py:792:see_memory_usage] MA 1.31 GB         Max_MA 1.31 GB         CA 12.42 GB         Max_CA 12 GB 
head: [2024-06-20 10:31:34,607] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 28.88 GB, percent = 1.5%
head: Parameter Offload: Total persistent parameters: 266240 in 65 params
head: [2024-06-20 10:31:34,878] [INFO] [utils.py:791:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
head: [2024-06-20 10:31:34,879] [INFO] [utils.py:792:see_memory_usage] MA 0.85 GB         Max_MA 1.33 GB         CA 12.42 GB         Max_CA 12 GB 
head: [2024-06-20 10:31:34,879] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 28.88 GB, percent = 1.5%
head: [2024-06-20 10:31:35,132] [INFO] [utils.py:791:see_memory_usage] Before creating fp16 partitions
head: [2024-06-20 10:31:35,132] [INFO] [utils.py:792:see_memory_usage] MA 0.85 GB         Max_MA 0.85 GB         CA 12.42 GB         Max_CA 12 GB 
head: [2024-06-20 10:31:35,133] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 28.88 GB, percent = 1.5%
head: [2024-06-20 10:31:36,739] [INFO] [utils.py:791:see_memory_usage] After creating fp16 partitions: 2
head: [2024-06-20 10:31:36,740] [INFO] [utils.py:792:see_memory_usage] MA 0.85 GB         Max_MA 0.85 GB         CA 0.85 GB         Max_CA 12 GB 
head: [2024-06-20 10:31:36,740] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 28.88 GB, percent = 1.5%
head: [2024-06-20 10:31:36,990] [INFO] [utils.py:791:see_memory_usage] Before creating fp32 partitions
head: [2024-06-20 10:31:36,990] [INFO] [utils.py:792:see_memory_usage] MA 0.85 GB         Max_MA 0.85 GB         CA 0.85 GB         Max_CA 1 GB 
head: [2024-06-20 10:31:36,991] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 28.88 GB, percent = 1.5%
head: [2024-06-20 10:31:37,285] [INFO] [utils.py:791:see_memory_usage] After creating fp32 partitions
head: [2024-06-20 10:31:37,286] [INFO] [utils.py:792:see_memory_usage] MA 2.42 GB         Max_MA 3.2 GB         CA 3.21 GB         Max_CA 3 GB 
head: [2024-06-20 10:31:37,286] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 28.88 GB, percent = 1.5%
head: [2024-06-20 10:31:37,554] [INFO] [utils.py:791:see_memory_usage] Before initializing optimizer states
head: [2024-06-20 10:31:37,555] [INFO] [utils.py:792:see_memory_usage] MA 2.42 GB         Max_MA 2.42 GB         CA 3.21 GB         Max_CA 3 GB 
head: [2024-06-20 10:31:37,555] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 28.88 GB, percent = 1.5%
head: [2024-06-20 10:31:37,829] [INFO] [utils.py:791:see_memory_usage] After initializing optimizer states
head: [2024-06-20 10:31:37,829] [INFO] [utils.py:792:see_memory_usage] MA 5.55 GB         Max_MA 7.12 GB         CA 7.92 GB         Max_CA 8 GB 
head: [2024-06-20 10:31:37,830] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 28.88 GB, percent = 1.5%
head: [2024-06-20 10:31:37,830] [INFO] [stage3.py:479:_setup_for_real_optimizer] optimizer state initialized
head: [2024-06-20 10:31:38,290] [INFO] [utils.py:791:see_memory_usage] After initializing ZeRO optimizer
head: [2024-06-20 10:31:38,291] [INFO] [utils.py:792:see_memory_usage] MA 7.27 GB         Max_MA 7.76 GB         CA 11.51 GB         Max_CA 12 GB 
head: [2024-06-20 10:31:38,291] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 28.88 GB, percent = 1.5%
head: [2024-06-20 10:31:38,291] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam
head: [2024-06-20 10:31:38,291] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
head: [2024-06-20 10:31:38,291] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x7f5b6c1f40d0>
head: [2024-06-20 10:31:38,291] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[9.65e-06, 9.65e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
head: [2024-06-20 10:31:38,292] [INFO] [config.py:984:print] DeepSpeedEngine configuration:
head: [2024-06-20 10:31:38,293] [INFO] [config.py:988:print]   activation_checkpointing_config  {
head:     "partition_activations": false, 
head:     "contiguous_memory_optimization": false, 
head:     "cpu_checkpointing": false, 
head:     "number_checkpoints": null, 
head:     "synchronize_checkpoint_boundary": false, 
head:     "profile": false
head: }
head: [2024-06-20 10:31:38,293] [INFO] [config.py:988:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
head: [2024-06-20 10:31:38,293] [INFO] [config.py:988:print]   amp_enabled .................. False
head: [2024-06-20 10:31:38,293] [INFO] [config.py:988:print]   amp_params ................... False
head: [2024-06-20 10:31:38,293] [INFO] [config.py:988:print]   autotuning_config ............ {
head:     "enabled": false, 
head:     "start_step": null, 
head:     "end_step": null, 
head:     "metric_path": null, 
head:     "arg_mappings": null, 
head:     "metric": "throughput", 
head:     "model_info": null, 
head:     "results_dir": "autotuning_results", 
head:     "exps_dir": "autotuning_exps", 
head:     "overwrite": true, 
head:     "fast": true, 
head:     "start_profile_step": 3, 
head:     "end_profile_step": 5, 
head:     "tuner_type": "gridsearch", 
head:     "tuner_early_stopping": 5, 
head:     "tuner_num_trials": 50, 
head:     "model_info_path": null, 
head:     "mp_size": 1, 
head:     "max_train_batch_size": null, 
head:     "min_train_batch_size": 1, 
head:     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
head:     "min_train_micro_batch_size_per_gpu": 1, 
head:     "num_tuning_micro_batch_sizes": 3
head: }
head: [2024-06-20 10:31:38,293] [INFO] [config.py:988:print]   bfloat16_enabled ............. False
head: [2024-06-20 10:31:38,293] [INFO] [config.py:988:print]   checkpoint_parallel_write_pipeline  False
head: [2024-06-20 10:31:38,293] [INFO] [config.py:988:print]   checkpoint_tag_validation_enabled  True
head: [2024-06-20 10:31:38,293] [INFO] [config.py:988:print]   checkpoint_tag_validation_fail  False
head: [2024-06-20 10:31:38,293] [INFO] [config.py:988:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f5c0c038550>
head: [2024-06-20 10:31:38,293] [INFO] [config.py:988:print]   communication_data_type ...... None
head: [2024-06-20 10:31:38,293] [INFO] [config.py:988:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
head: [2024-06-20 10:31:38,293] [INFO] [config.py:988:print]   curriculum_enabled_legacy .... False
head: [2024-06-20 10:31:38,293] [INFO] [config.py:988:print]   curriculum_params_legacy ..... False
head: [2024-06-20 10:31:38,293] [INFO] [config.py:988:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
head: [2024-06-20 10:31:38,293] [INFO] [config.py:988:print]   data_efficiency_enabled ...... False
head: [2024-06-20 10:31:38,293] [INFO] [config.py:988:print]   dataloader_drop_last ......... False
head: [2024-06-20 10:31:38,293] [INFO] [config.py:988:print]   disable_allgather ............ False
head: [2024-06-20 10:31:38,293] [INFO] [config.py:988:print]   dump_state ................... False
head: [2024-06-20 10:31:38,293] [INFO] [config.py:988:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 100, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
head: [2024-06-20 10:31:38,293] [INFO] [config.py:988:print]   eigenvalue_enabled ........... False
head: [2024-06-20 10:31:38,293] [INFO] [config.py:988:print]   eigenvalue_gas_boundary_resolution  1
head: [2024-06-20 10:31:38,293] [INFO] [config.py:988:print]   eigenvalue_layer_name ........ bert.encoder.layer
head: [2024-06-20 10:31:38,293] [INFO] [config.py:988:print]   eigenvalue_layer_num ......... 0
head: [2024-06-20 10:31:38,293] [INFO] [config.py:988:print]   eigenvalue_max_iter .......... 100
head: [2024-06-20 10:31:38,293] [INFO] [config.py:988:print]   eigenvalue_stability ......... 1e-06
head: [2024-06-20 10:31:38,293] [INFO] [config.py:988:print]   eigenvalue_tol ............... 0.01
head: [2024-06-20 10:31:38,293] [INFO] [config.py:988:print]   eigenvalue_verbose ........... False
head: [2024-06-20 10:31:38,293] [INFO] [config.py:988:print]   elasticity_enabled ........... False
head: [2024-06-20 10:31:38,294] [INFO] [config.py:988:print]   flops_profiler_config ........ {
head:     "enabled": false, 
head:     "recompute_fwd_factor": 0.0, 
head:     "profile_step": 1, 
head:     "module_depth": -1, 
head:     "top_modules": 1, 
head:     "detailed": true, 
head:     "output_file": null
head: }
head: [2024-06-20 10:31:38,294] [INFO] [config.py:988:print]   fp16_auto_cast ............... False
head: [2024-06-20 10:31:38,294] [INFO] [config.py:988:print]   fp16_enabled ................. True
head: [2024-06-20 10:31:38,294] [INFO] [config.py:988:print]   fp16_master_weights_and_gradients  False
head: [2024-06-20 10:31:38,294] [INFO] [config.py:988:print]   global_rank .................. 0
head: [2024-06-20 10:31:38,294] [INFO] [config.py:988:print]   grad_accum_dtype ............. None
head: [2024-06-20 10:31:38,294] [INFO] [config.py:988:print]   gradient_accumulation_steps .. 4
head: [2024-06-20 10:31:38,294] [INFO] [config.py:988:print]   gradient_clipping ............ 1.0
head: [2024-06-20 10:31:38,294] [INFO] [config.py:988:print]   gradient_predivide_factor .... 1.0
head: [2024-06-20 10:31:38,294] [INFO] [config.py:988:print]   graph_harvesting ............. False
head: [2024-06-20 10:31:38,294] [INFO] [config.py:988:print]   hybrid_engine ................ enabled=True max_out_tokens=2048 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
head: [2024-06-20 10:31:38,294] [INFO] [config.py:988:print]   initial_dynamic_scale ........ 65536
head: [2024-06-20 10:31:38,294] [INFO] [config.py:988:print]   load_universal_checkpoint .... False
head: [2024-06-20 10:31:38,294] [INFO] [config.py:988:print]   loss_scale ................... 0
head: [2024-06-20 10:31:38,294] [INFO] [config.py:988:print]   memory_breakdown ............. False
head: [2024-06-20 10:31:38,294] [INFO] [config.py:988:print]   mics_hierarchial_params_gather  False
head: [2024-06-20 10:31:38,294] [INFO] [config.py:988:print]   mics_shard_size .............. -1
head: [2024-06-20 10:31:38,294] [INFO] [config.py:988:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='step3_tensorboard/ds_tensorboard_logs/', job_name='step3_actor_tensorboard') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
head: [2024-06-20 10:31:38,294] [INFO] [config.py:988:print]   nebula_config ................ {
head:     "enabled": false, 
head:     "persistent_storage_path": null, 
head:     "persistent_time_interval": 100, 
head:     "num_of_version_in_retention": 2, 
head:     "enable_nebula_load": true, 
head:     "load_path": null
head: }
head: [2024-06-20 10:31:38,294] [INFO] [config.py:988:print]   optimizer_legacy_fusion ...... False
head: [2024-06-20 10:31:38,294] [INFO] [config.py:988:print]   optimizer_name ............... None
head: [2024-06-20 10:31:38,294] [INFO] [config.py:988:print]   optimizer_params ............. None
head: [2024-06-20 10:31:38,294] [INFO] [config.py:988:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
head: [2024-06-20 10:31:38,294] [INFO] [config.py:988:print]   pld_enabled .................. False
head: [2024-06-20 10:31:38,294] [INFO] [config.py:988:print]   pld_params ................... False
head: [2024-06-20 10:31:38,294] [INFO] [config.py:988:print]   prescale_gradients ........... False
head: [2024-06-20 10:31:38,294] [INFO] [config.py:988:print]   scheduler_name ............... None
head: [2024-06-20 10:31:38,294] [INFO] [config.py:988:print]   scheduler_params ............. None
head: [2024-06-20 10:31:38,294] [INFO] [config.py:988:print]   seq_parallel_communication_data_type  torch.float32
head: [2024-06-20 10:31:38,294] [INFO] [config.py:988:print]   sparse_attention ............. None
head: [2024-06-20 10:31:38,294] [INFO] [config.py:988:print]   sparse_gradients_enabled ..... False
head: [2024-06-20 10:31:38,294] [INFO] [config.py:988:print]   steps_per_print .............. 10
head: [2024-06-20 10:31:38,294] [INFO] [config.py:988:print]   train_batch_size ............. 512
head: [2024-06-20 10:31:38,294] [INFO] [config.py:988:print]   train_micro_batch_size_per_gpu  8
head: [2024-06-20 10:31:38,294] [INFO] [config.py:988:print]   use_data_before_expert_parallel_  False
head: [2024-06-20 10:31:38,294] [INFO] [config.py:988:print]   use_node_local_storage ....... False
head: [2024-06-20 10:31:38,294] [INFO] [config.py:988:print]   wall_clock_breakdown ......... False
head: [2024-06-20 10:31:38,294] [INFO] [config.py:988:print]   weight_quantization_config ... None
head: [2024-06-20 10:31:38,294] [INFO] [config.py:988:print]   world_size ................... 16
head: [2024-06-20 10:31:38,294] [INFO] [config.py:988:print]   zero_allow_untested_optimizer  False
head: [2024-06-20 10:31:38,294] [INFO] [config.py:988:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=30000000 param_persistence_threshold=10000 model_persistence_threshold=sys.maxsize max_live_parameters=30000000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=False pipeline_loading_checkpoint=False override_module_apply=True
head: [2024-06-20 10:31:38,294] [INFO] [config.py:988:print]   zero_enabled ................. True
head: [2024-06-20 10:31:38,294] [INFO] [config.py:988:print]   zero_force_ds_cpu_optimizer .. True
head: [2024-06-20 10:31:38,295] [INFO] [config.py:988:print]   zero_optimization_stage ...... 3
head: [2024-06-20 10:31:38,295] [INFO] [config.py:974:print_user_config]   json = {
head:     "train_batch_size": 512, 
head:     "train_micro_batch_size_per_gpu": 8, 
head:     "steps_per_print": 10, 
head:     "zero_optimization": {
head:         "stage": 3, 
head:         "offload_param": {
head:             "device": "none"
head:         }, 
head:         "offload_optimizer": {
head:             "device": "none"
head:         }, 
head:         "stage3_param_persistence_threshold": 1.000000e+04, 
head:         "stage3_max_live_parameters": 3.000000e+07, 
head:         "stage3_prefetch_bucket_size": 3.000000e+07, 
head:         "memory_efficient_linear": false
head:     }, 
head:     "fp16": {
head:         "enabled": true, 
head:         "loss_scale_window": 100, 
head:         "min_loss_scale": 1
head:     }, 
head:     "gradient_clipping": 1.0, 
head:     "prescale_gradients": false, 
head:     "wall_clock_breakdown": false, 
head:     "hybrid_engine": {
head:         "enabled": true, 
head:         "max_out_tokens": 2.048000e+03, 
head:         "inference_tp_size": 1, 
head:         "release_inference_cache": false, 
head:         "pin_parameters": true, 
head:         "tp_gather_partition_size": 8
head:     }, 
head:     "tensorboard": {
head:         "enabled": false, 
head:         "output_path": "step3_tensorboard/ds_tensorboard_logs/", 
head:         "job_name": "step3_actor_tensorboard"
head:     }
head: }
head: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
head: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
head: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
head: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
worker1: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
worker1: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
head: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
worker1: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
worker1: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
head: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
worker1: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
worker1: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
worker1: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
worker1: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
head: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
head: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
head: Detected CUDA files, patching ldflags
head: Emitting ninja build file /root/.cache/torch_extensions/py310_cu118/transformer_inference/build.ninja...
head: Building extension module transformer_inference...
head: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
worker1: Detected CUDA files, patching ldflags
worker1: Emitting ninja build file /root/.cache/torch_extensions/py310_cu118/transformer_inference/build.ninja...
worker1: Building extension module transformer_inference...
worker1: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
head: ninja: no work to do.
head: Loading extension module transformer_inference...
head: Time to load transformer_inference op: 0.07007050514221191 seconds
head: [2024-06-20 10:31:38,391] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed-Inference config: {'layer_id': 0, 'hidden_size': 4096, 'intermediate_size': 11008, 'heads': 32, 'num_hidden_layers': -1, 'dtype': torch.float16, 'pre_layer_norm': True, 'norm_type': <NormType.RMSNorm: 3>, 'local_rank': -1, 'stochastic_mode': False, 'epsilon': 1e-05, 'mp_size': 1, 'scale_attention': True, 'triangular_masking': True, 'local_attention': False, 'window_size': 1, 'rotary_dim': 128, 'rotate_half': True, 'rotate_every_two': False, 'return_tuple': True, 'mlp_after_attn': True, 'mlp_act_func_type': <ActivationFuncType.GATED_SILU: 4>, 'specialized_mode': False, 'training_mp_size': 1, 'bigscience_bloom': False, 'max_out_tokens': 2048, 'min_out_tokens': 2048, 'scale_attn_by_inverse_layer_idx': False, 'enable_qkv_quantization': False, 'use_mup': False, 'return_single_tuple': False, 'set_empty_params': True, 'transposed_mode': True, 'use_triton': False, 'triton_autotune': False, 'num_kv': 32, 'rope_theta': 10000.0}
worker1: ninja: no work to do.
worker1: Loading extension module transformer_inference...
worker1: Time to load transformer_inference op: 0.0693361759185791 seconds
head: Loading extension module transformer_inference...
head: ******************[end] Initialized Actor Model [end] (duration: 10.42s)******************
head: *************************[start] Initializing Ref Model [start] **************************
head: Loading extension module transformer_inference...
head: Time to load transformer_inference op: 0.10504531860351562 seconds
head: Time to load transformer_inference op: 0.10552430152893066 seconds
head: Loading extension module transformer_inference...
head: Time to load transformer_inference op: 0.1047215461730957 seconds
worker1: Loading extension module transformer_inference...
head: Loading extension module transformer_inference...
worker1: Loading extension module transformer_inference...
head: Time to load transformer_inference op: 0.1043701171875 seconds
worker1: Time to load transformer_inference op: 0.10493588447570801 seconds
worker1: Time to load transformer_inference op: 0.1049349308013916 seconds
worker1: Loading extension module transformer_inference...
head: Loading extension module transformer_inference...
worker1: Loading extension module transformer_inference...
worker1: Loading extension module transformer_inference...
head: Loading extension module transformer_inference...
worker1: Loading extension module transformer_inference...
worker1: Loading extension module transformer_inference...
worker1: Time to load transformer_inference op: 0.10504817962646484 seconds
worker1: Time to load transformer_inference op: 0.1048891544342041 seconds
head: Time to load transformer_inference op: 0.10504937171936035 seconds
head: Loading extension module transformer_inference...
head: Time to load transformer_inference op: 0.10477876663208008 seconds
worker1: Time to load transformer_inference op: 0.10565590858459473 seconds
worker1: Time to load transformer_inference op: 0.10550665855407715 seconds
worker1: Time to load transformer_inference op: 0.10685873031616211 seconds
head: Time to load transformer_inference op: 0.10530567169189453 seconds
head: [2024-06-20 10:31:38,604] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 582, num_elems = 13.48B
worker1: Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.05s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.08s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.12s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.09s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.12s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.13s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.11s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.13s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.56s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.59s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.57s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.56s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.59s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.60s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.59s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.60s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.48s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.46s/it]
worker1: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.49s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.47s/it]
worker1: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.48s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.46s/it]
worker1: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.48s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.46s/it]
worker1: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.51s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.48s/it]
head: Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.07s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.09s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.14s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.15s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.17s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.23s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.24s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:02<00:04,  2.10s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.57s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.62s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.60s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.64s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.66s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.63s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.67s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.92s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.50s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.48s/it]
head: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.51s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.48s/it]
worker1: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.49s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.47s/it]
worker1: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.51s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.48s/it]
head: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.50s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.49s/it]
head: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.51s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.50s/it]
worker1: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.54s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.50s/it]
head: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.53s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.50s/it]
head: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.54s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.53s/it]
head: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.55s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.53s/it]
head: Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.68s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.77s/it]
head: [2024-06-20 10:31:43,950] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.6+b1e9b0fa, git-hash=b1e9b0fa, git-branch=main
head: [2024-06-20 10:31:43,976] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
head: [2024-06-20 10:31:43,978] [INFO] [logging.py:96:log_dist] [Rank 0] Creating ZeRO Offload
head: [2024-06-20 10:31:44,257] [INFO] [utils.py:791:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
head: [2024-06-20 10:31:44,257] [INFO] [utils.py:792:see_memory_usage] MA 8.84 GB         Max_MA 9.42 GB         CA 18.48 GB         Max_CA 18 GB 
head: [2024-06-20 10:31:44,258] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 28.92 GB, percent = 1.5%
head: Parameter Offload: Total persistent parameters: 266240 in 65 params
head: [2024-06-20 10:31:44,560] [INFO] [utils.py:791:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
head: [2024-06-20 10:31:44,560] [INFO] [utils.py:792:see_memory_usage] MA 8.38 GB         Max_MA 8.86 GB         CA 18.48 GB         Max_CA 18 GB 
head: [2024-06-20 10:31:44,561] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 28.92 GB, percent = 1.5%
head: [2024-06-20 10:31:44,562] [INFO] [config.py:984:print] DeepSpeedEngine configuration:
head: [2024-06-20 10:31:44,562] [INFO] [config.py:988:print]   activation_checkpointing_config  {
head:     "partition_activations": false, 
head:     "contiguous_memory_optimization": false, 
head:     "cpu_checkpointing": false, 
head:     "number_checkpoints": null, 
head:     "synchronize_checkpoint_boundary": false, 
head:     "profile": false
head: }
head: [2024-06-20 10:31:44,562] [INFO] [config.py:988:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
head: [2024-06-20 10:31:44,562] [INFO] [config.py:988:print]   amp_enabled .................. False
head: [2024-06-20 10:31:44,562] [INFO] [config.py:988:print]   amp_params ................... False
head: [2024-06-20 10:31:44,562] [INFO] [config.py:988:print]   autotuning_config ............ {
head:     "enabled": false, 
head:     "start_step": null, 
head:     "end_step": null, 
head:     "metric_path": null, 
head:     "arg_mappings": null, 
head:     "metric": "throughput", 
head:     "model_info": null, 
head:     "results_dir": "autotuning_results", 
head:     "exps_dir": "autotuning_exps", 
head:     "overwrite": true, 
head:     "fast": true, 
head:     "start_profile_step": 3, 
head:     "end_profile_step": 5, 
head:     "tuner_type": "gridsearch", 
head:     "tuner_early_stopping": 5, 
head:     "tuner_num_trials": 50, 
head:     "model_info_path": null, 
head:     "mp_size": 1, 
head:     "max_train_batch_size": null, 
head:     "min_train_batch_size": 1, 
head:     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
head:     "min_train_micro_batch_size_per_gpu": 1, 
head:     "num_tuning_micro_batch_sizes": 3
head: }
head: [2024-06-20 10:31:44,562] [INFO] [config.py:988:print]   bfloat16_enabled ............. False
head: [2024-06-20 10:31:44,562] [INFO] [config.py:988:print]   checkpoint_parallel_write_pipeline  False
head: [2024-06-20 10:31:44,562] [INFO] [config.py:988:print]   checkpoint_tag_validation_enabled  True
head: [2024-06-20 10:31:44,562] [INFO] [config.py:988:print]   checkpoint_tag_validation_fail  False
head: [2024-06-20 10:31:44,562] [INFO] [config.py:988:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f5c30102e30>
head: [2024-06-20 10:31:44,562] [INFO] [config.py:988:print]   communication_data_type ...... None
head: [2024-06-20 10:31:44,562] [INFO] [config.py:988:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
head: [2024-06-20 10:31:44,563] [INFO] [config.py:988:print]   curriculum_enabled_legacy .... False
head: [2024-06-20 10:31:44,563] [INFO] [config.py:988:print]   curriculum_params_legacy ..... False
head: [2024-06-20 10:31:44,563] [INFO] [config.py:988:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
head: [2024-06-20 10:31:44,563] [INFO] [config.py:988:print]   data_efficiency_enabled ...... False
head: [2024-06-20 10:31:44,563] [INFO] [config.py:988:print]   dataloader_drop_last ......... False
head: [2024-06-20 10:31:44,563] [INFO] [config.py:988:print]   disable_allgather ............ False
head: [2024-06-20 10:31:44,563] [INFO] [config.py:988:print]   dump_state ................... False
head: [2024-06-20 10:31:44,563] [INFO] [config.py:988:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
head: [2024-06-20 10:31:44,563] [INFO] [config.py:988:print]   eigenvalue_enabled ........... False
head: [2024-06-20 10:31:44,563] [INFO] [config.py:988:print]   eigenvalue_gas_boundary_resolution  1
head: [2024-06-20 10:31:44,563] [INFO] [config.py:988:print]   eigenvalue_layer_name ........ bert.encoder.layer
head: [2024-06-20 10:31:44,563] [INFO] [config.py:988:print]   eigenvalue_layer_num ......... 0
head: [2024-06-20 10:31:44,563] [INFO] [config.py:988:print]   eigenvalue_max_iter .......... 100
head: [2024-06-20 10:31:44,563] [INFO] [config.py:988:print]   eigenvalue_stability ......... 1e-06
head: [2024-06-20 10:31:44,563] [INFO] [config.py:988:print]   eigenvalue_tol ............... 0.01
head: [2024-06-20 10:31:44,563] [INFO] [config.py:988:print]   eigenvalue_verbose ........... False
head: [2024-06-20 10:31:44,563] [INFO] [config.py:988:print]   elasticity_enabled ........... False
head: [2024-06-20 10:31:44,563] [INFO] [config.py:988:print]   flops_profiler_config ........ {
head:     "enabled": false, 
head:     "recompute_fwd_factor": 0.0, 
head:     "profile_step": 1, 
head:     "module_depth": -1, 
head:     "top_modules": 1, 
head:     "detailed": true, 
head:     "output_file": null
head: }
head: [2024-06-20 10:31:44,563] [INFO] [config.py:988:print]   fp16_auto_cast ............... False
head: [2024-06-20 10:31:44,563] [INFO] [config.py:988:print]   fp16_enabled ................. True
head: [2024-06-20 10:31:44,563] [INFO] [config.py:988:print]   fp16_master_weights_and_gradients  False
head: [2024-06-20 10:31:44,563] [INFO] [config.py:988:print]   global_rank .................. 0
head: [2024-06-20 10:31:44,563] [INFO] [config.py:988:print]   grad_accum_dtype ............. None
head: [2024-06-20 10:31:44,563] [INFO] [config.py:988:print]   gradient_accumulation_steps .. 4
head: [2024-06-20 10:31:44,563] [INFO] [config.py:988:print]   gradient_clipping ............ 1.0
head: [2024-06-20 10:31:44,563] [INFO] [config.py:988:print]   gradient_predivide_factor .... 1.0
head: [2024-06-20 10:31:44,563] [INFO] [config.py:988:print]   graph_harvesting ............. False
head: [2024-06-20 10:31:44,563] [INFO] [config.py:988:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
head: [2024-06-20 10:31:44,563] [INFO] [config.py:988:print]   initial_dynamic_scale ........ 65536
head: [2024-06-20 10:31:44,563] [INFO] [config.py:988:print]   load_universal_checkpoint .... False
head: [2024-06-20 10:31:44,563] [INFO] [config.py:988:print]   loss_scale ................... 0
head: [2024-06-20 10:31:44,563] [INFO] [config.py:988:print]   memory_breakdown ............. False
head: [2024-06-20 10:31:44,563] [INFO] [config.py:988:print]   mics_hierarchial_params_gather  False
head: [2024-06-20 10:31:44,563] [INFO] [config.py:988:print]   mics_shard_size .............. -1
head: [2024-06-20 10:31:44,563] [INFO] [config.py:988:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
head: [2024-06-20 10:31:44,563] [INFO] [config.py:988:print]   nebula_config ................ {
head:     "enabled": false, 
head:     "persistent_storage_path": null, 
head:     "persistent_time_interval": 100, 
head:     "num_of_version_in_retention": 2, 
head:     "enable_nebula_load": true, 
head:     "load_path": null
head: }
head: [2024-06-20 10:31:44,563] [INFO] [config.py:988:print]   optimizer_legacy_fusion ...... False
head: [2024-06-20 10:31:44,563] [INFO] [config.py:988:print]   optimizer_name ............... None
head: [2024-06-20 10:31:44,563] [INFO] [config.py:988:print]   optimizer_params ............. None
head: [2024-06-20 10:31:44,563] [INFO] [config.py:988:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
head: [2024-06-20 10:31:44,563] [INFO] [config.py:988:print]   pld_enabled .................. False
head: [2024-06-20 10:31:44,563] [INFO] [config.py:988:print]   pld_params ................... False
head: [2024-06-20 10:31:44,563] [INFO] [config.py:988:print]   prescale_gradients ........... False
head: [2024-06-20 10:31:44,563] [INFO] [config.py:988:print]   scheduler_name ............... None
head: [2024-06-20 10:31:44,563] [INFO] [config.py:988:print]   scheduler_params ............. None
head: [2024-06-20 10:31:44,563] [INFO] [config.py:988:print]   seq_parallel_communication_data_type  torch.float32
head: [2024-06-20 10:31:44,564] [INFO] [config.py:988:print]   sparse_attention ............. None
head: [2024-06-20 10:31:44,564] [INFO] [config.py:988:print]   sparse_gradients_enabled ..... False
head: [2024-06-20 10:31:44,564] [INFO] [config.py:988:print]   steps_per_print .............. 10
head: [2024-06-20 10:31:44,564] [INFO] [config.py:988:print]   train_batch_size ............. 512
head: [2024-06-20 10:31:44,564] [INFO] [config.py:988:print]   train_micro_batch_size_per_gpu  8
head: [2024-06-20 10:31:44,564] [INFO] [config.py:988:print]   use_data_before_expert_parallel_  False
head: [2024-06-20 10:31:44,564] [INFO] [config.py:988:print]   use_node_local_storage ....... False
head: [2024-06-20 10:31:44,564] [INFO] [config.py:988:print]   wall_clock_breakdown ......... False
head: [2024-06-20 10:31:44,564] [INFO] [config.py:988:print]   weight_quantization_config ... None
head: [2024-06-20 10:31:44,564] [INFO] [config.py:988:print]   world_size ................... 16
head: [2024-06-20 10:31:44,564] [INFO] [config.py:988:print]   zero_allow_untested_optimizer  False
head: [2024-06-20 10:31:44,564] [INFO] [config.py:988:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=10000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=False pipeline_loading_checkpoint=False override_module_apply=True
head: [2024-06-20 10:31:44,564] [INFO] [config.py:988:print]   zero_enabled ................. True
head: [2024-06-20 10:31:44,564] [INFO] [config.py:988:print]   zero_force_ds_cpu_optimizer .. True
head: [2024-06-20 10:31:44,564] [INFO] [config.py:988:print]   zero_optimization_stage ...... 3
head: [2024-06-20 10:31:44,564] [INFO] [config.py:974:print_user_config]   json = {
head:     "train_batch_size": 512, 
head:     "train_micro_batch_size_per_gpu": 8, 
head:     "steps_per_print": 10, 
head:     "zero_optimization": {
head:         "stage": 3, 
head:         "stage3_param_persistence_threshold": 1.000000e+04, 
head:         "offload_param": {
head:             "device": "none"
head:         }, 
head:         "memory_efficient_linear": false
head:     }, 
head:     "fp16": {
head:         "enabled": true, 
head:         "min_loss_scale": 1
head:     }, 
head:     "gradient_clipping": 1.0, 
head:     "prescale_gradients": false, 
head:     "wall_clock_breakdown": false
head: }
head: *******************[end] Initialized Ref Model [end] (duration: 6.13s)********************
head: ************************[start] Initializing Critic Model [start] ************************
head: [2024-06-20 10:31:44,753] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 872, num_elems = 20.08B
worker1: Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.12s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.12s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.12s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.17s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.17s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.23s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.25s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.26s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.59s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.60s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.57s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.59s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.61s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.59s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.63s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.64s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.59s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.55s/it]
worker1: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.60s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.55s/it]
worker1: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.60s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.55s/it]
worker1: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.59s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.55s/it]
worker1: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.58s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.55s/it]
worker1: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.60s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.56s/it]
worker1: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.60s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.56s/it]
head: Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.18s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.18s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.19s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.21s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.25s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.30s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.33s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:02<00:04,  2.02s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.60s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.62s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.63s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.64s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.63s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.64s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.66s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.99s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.61s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.57s/it]
head: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.60s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.57s/it]
head: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.59s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.57s/it]
worker1: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.62s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.58s/it]
head: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.61s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.58s/it]
head: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.64s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.61s/it]
head: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.67s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.61s/it]
head: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.68s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.62s/it]
head: Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.70s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.79s/it]
head: >Creating model from_config took 5.593735694885254 seconds
head: [2024-06-20 10:31:50,165] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.6+b1e9b0fa, git-hash=b1e9b0fa, git-branch=main
head: [2024-06-20 10:31:50,239] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
head: [2024-06-20 10:31:50,241] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
head: [2024-06-20 10:31:50,241] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
head: [2024-06-20 10:31:50,250] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
head: [2024-06-20 10:31:50,250] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
head: [2024-06-20 10:31:50,250] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
head: [2024-06-20 10:31:50,250] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 3 optimizer
head: [2024-06-20 10:31:50,538] [INFO] [utils.py:791:see_memory_usage] Stage 3 initialize beginning
head: [2024-06-20 10:31:50,539] [INFO] [utils.py:792:see_memory_usage] MA 9.45 GB         Max_MA 10.02 GB         CA 18.54 GB         Max_CA 19 GB 
head: [2024-06-20 10:31:50,539] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 28.93 GB, percent = 1.5%
head: [2024-06-20 10:31:50,541] [INFO] [stage3.py:127:__init__] Reduce bucket size 500,000,000
head: [2024-06-20 10:31:50,541] [INFO] [stage3.py:128:__init__] Prefetch bucket size 30000000
head: [2024-06-20 10:31:50,826] [INFO] [utils.py:791:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
head: [2024-06-20 10:31:50,827] [INFO] [utils.py:792:see_memory_usage] MA 9.45 GB         Max_MA 9.45 GB         CA 18.54 GB         Max_CA 19 GB 
head: [2024-06-20 10:31:50,827] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 28.93 GB, percent = 1.5%
head: Parameter Offload: Total persistent parameters: 270336 in 66 params
head: [2024-06-20 10:31:51,118] [INFO] [utils.py:791:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
head: [2024-06-20 10:31:51,119] [INFO] [utils.py:792:see_memory_usage] MA 9.22 GB         Max_MA 9.47 GB         CA 18.54 GB         Max_CA 19 GB 
head: [2024-06-20 10:31:51,119] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 28.93 GB, percent = 1.5%
head: [2024-06-20 10:31:51,401] [INFO] [utils.py:791:see_memory_usage] Before creating fp16 partitions
head: [2024-06-20 10:31:51,402] [INFO] [utils.py:792:see_memory_usage] MA 9.22 GB         Max_MA 9.22 GB         CA 18.54 GB         Max_CA 19 GB 
head: [2024-06-20 10:31:51,402] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 28.93 GB, percent = 1.5%
head: [2024-06-20 10:31:52,924] [INFO] [utils.py:791:see_memory_usage] After creating fp16 partitions: 2
head: [2024-06-20 10:31:52,925] [INFO] [utils.py:792:see_memory_usage] MA 9.22 GB         Max_MA 9.22 GB         CA 12.99 GB         Max_CA 19 GB 
head: [2024-06-20 10:31:52,926] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 28.93 GB, percent = 1.5%
head: [2024-06-20 10:31:53,240] [INFO] [utils.py:791:see_memory_usage] Before creating fp32 partitions
head: [2024-06-20 10:31:53,241] [INFO] [utils.py:792:see_memory_usage] MA 9.22 GB         Max_MA 9.22 GB         CA 12.99 GB         Max_CA 13 GB 
head: [2024-06-20 10:31:53,242] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 28.93 GB, percent = 1.5%
head: [2024-06-20 10:31:53,534] [INFO] [utils.py:791:see_memory_usage] After creating fp32 partitions
head: [2024-06-20 10:31:53,535] [INFO] [utils.py:792:see_memory_usage] MA 10.76 GB         Max_MA 11.53 GB         CA 15.29 GB         Max_CA 15 GB 
head: [2024-06-20 10:31:53,535] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 28.93 GB, percent = 1.5%
head: [2024-06-20 10:31:53,805] [INFO] [utils.py:791:see_memory_usage] Before initializing optimizer states
head: [2024-06-20 10:31:53,805] [INFO] [utils.py:792:see_memory_usage] MA 10.76 GB         Max_MA 10.76 GB         CA 15.29 GB         Max_CA 15 GB 
head: [2024-06-20 10:31:53,806] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 28.93 GB, percent = 1.5%
head: [2024-06-20 10:31:54,085] [INFO] [utils.py:791:see_memory_usage] After initializing optimizer states
head: [2024-06-20 10:31:54,086] [INFO] [utils.py:792:see_memory_usage] MA 13.83 GB         Max_MA 15.37 GB         CA 19.91 GB         Max_CA 20 GB 
head: [2024-06-20 10:31:54,086] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 28.93 GB, percent = 1.5%
head: [2024-06-20 10:31:54,086] [INFO] [stage3.py:479:_setup_for_real_optimizer] optimizer state initialized
head: [2024-06-20 10:31:54,581] [INFO] [utils.py:791:see_memory_usage] After initializing ZeRO optimizer
head: [2024-06-20 10:31:54,582] [INFO] [utils.py:792:see_memory_usage] MA 15.53 GB         Max_MA 16.02 GB         CA 20.33 GB         Max_CA 20 GB 
head: [2024-06-20 10:31:54,582] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 28.94 GB, percent = 1.5%
head: [2024-06-20 10:31:54,583] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam
head: [2024-06-20 10:31:54,583] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
head: [2024-06-20 10:31:54,583] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x7f5bec2fb2e0>
head: [2024-06-20 10:31:54,583] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[5e-06, 5e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
head: [2024-06-20 10:31:54,584] [INFO] [config.py:984:print] DeepSpeedEngine configuration:
head: [2024-06-20 10:31:54,584] [INFO] [config.py:988:print]   activation_checkpointing_config  {
head:     "partition_activations": false, 
head:     "contiguous_memory_optimization": false, 
head:     "cpu_checkpointing": false, 
head:     "number_checkpoints": null, 
head:     "synchronize_checkpoint_boundary": false, 
head:     "profile": false
head: }
head: [2024-06-20 10:31:54,584] [INFO] [config.py:988:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
head: [2024-06-20 10:31:54,584] [INFO] [config.py:988:print]   amp_enabled .................. False
head: [2024-06-20 10:31:54,584] [INFO] [config.py:988:print]   amp_params ................... False
head: [2024-06-20 10:31:54,584] [INFO] [config.py:988:print]   autotuning_config ............ {
head:     "enabled": false, 
head:     "start_step": null, 
head:     "end_step": null, 
head:     "metric_path": null, 
head:     "arg_mappings": null, 
head:     "metric": "throughput", 
head:     "model_info": null, 
head:     "results_dir": "autotuning_results", 
head:     "exps_dir": "autotuning_exps", 
head:     "overwrite": true, 
head:     "fast": true, 
head:     "start_profile_step": 3, 
head:     "end_profile_step": 5, 
head:     "tuner_type": "gridsearch", 
head:     "tuner_early_stopping": 5, 
head:     "tuner_num_trials": 50, 
head:     "model_info_path": null, 
head:     "mp_size": 1, 
head:     "max_train_batch_size": null, 
head:     "min_train_batch_size": 1, 
head:     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
head:     "min_train_micro_batch_size_per_gpu": 1, 
head:     "num_tuning_micro_batch_sizes": 3
head: }
head: [2024-06-20 10:31:54,584] [INFO] [config.py:988:print]   bfloat16_enabled ............. False
head: [2024-06-20 10:31:54,584] [INFO] [config.py:988:print]   checkpoint_parallel_write_pipeline  False
head: [2024-06-20 10:31:54,584] [INFO] [config.py:988:print]   checkpoint_tag_validation_enabled  True
head: [2024-06-20 10:31:54,584] [INFO] [config.py:988:print]   checkpoint_tag_validation_fail  False
head: [2024-06-20 10:31:54,584] [INFO] [config.py:988:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f5bec1f1cf0>
head: [2024-06-20 10:31:54,584] [INFO] [config.py:988:print]   communication_data_type ...... None
head: [2024-06-20 10:31:54,584] [INFO] [config.py:988:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
head: [2024-06-20 10:31:54,584] [INFO] [config.py:988:print]   curriculum_enabled_legacy .... False
head: [2024-06-20 10:31:54,584] [INFO] [config.py:988:print]   curriculum_params_legacy ..... False
head: [2024-06-20 10:31:54,584] [INFO] [config.py:988:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
head: [2024-06-20 10:31:54,584] [INFO] [config.py:988:print]   data_efficiency_enabled ...... False
head: [2024-06-20 10:31:54,584] [INFO] [config.py:988:print]   dataloader_drop_last ......... False
head: [2024-06-20 10:31:54,584] [INFO] [config.py:988:print]   disable_allgather ............ False
head: [2024-06-20 10:31:54,584] [INFO] [config.py:988:print]   dump_state ................... False
head: [2024-06-20 10:31:54,584] [INFO] [config.py:988:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 100, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
head: [2024-06-20 10:31:54,584] [INFO] [config.py:988:print]   eigenvalue_enabled ........... False
head: [2024-06-20 10:31:54,585] [INFO] [config.py:988:print]   eigenvalue_gas_boundary_resolution  1
head: [2024-06-20 10:31:54,585] [INFO] [config.py:988:print]   eigenvalue_layer_name ........ bert.encoder.layer
head: [2024-06-20 10:31:54,585] [INFO] [config.py:988:print]   eigenvalue_layer_num ......... 0
head: [2024-06-20 10:31:54,585] [INFO] [config.py:988:print]   eigenvalue_max_iter .......... 100
head: [2024-06-20 10:31:54,585] [INFO] [config.py:988:print]   eigenvalue_stability ......... 1e-06
head: [2024-06-20 10:31:54,585] [INFO] [config.py:988:print]   eigenvalue_tol ............... 0.01
head: [2024-06-20 10:31:54,585] [INFO] [config.py:988:print]   eigenvalue_verbose ........... False
head: [2024-06-20 10:31:54,585] [INFO] [config.py:988:print]   elasticity_enabled ........... False
head: [2024-06-20 10:31:54,585] [INFO] [config.py:988:print]   flops_profiler_config ........ {
head:     "enabled": false, 
head:     "recompute_fwd_factor": 0.0, 
head:     "profile_step": 1, 
head:     "module_depth": -1, 
head:     "top_modules": 1, 
head:     "detailed": true, 
head:     "output_file": null
head: }
head: [2024-06-20 10:31:54,585] [INFO] [config.py:988:print]   fp16_auto_cast ............... False
head: [2024-06-20 10:31:54,585] [INFO] [config.py:988:print]   fp16_enabled ................. True
head: [2024-06-20 10:31:54,585] [INFO] [config.py:988:print]   fp16_master_weights_and_gradients  False
head: [2024-06-20 10:31:54,585] [INFO] [config.py:988:print]   global_rank .................. 0
head: [2024-06-20 10:31:54,585] [INFO] [config.py:988:print]   grad_accum_dtype ............. None
head: [2024-06-20 10:31:54,585] [INFO] [config.py:988:print]   gradient_accumulation_steps .. 4
head: [2024-06-20 10:31:54,585] [INFO] [config.py:988:print]   gradient_clipping ............ 1.0
head: [2024-06-20 10:31:54,585] [INFO] [config.py:988:print]   gradient_predivide_factor .... 1.0
head: [2024-06-20 10:31:54,585] [INFO] [config.py:988:print]   graph_harvesting ............. False
head: [2024-06-20 10:31:54,585] [INFO] [config.py:988:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
head: [2024-06-20 10:31:54,585] [INFO] [config.py:988:print]   initial_dynamic_scale ........ 65536
head: [2024-06-20 10:31:54,585] [INFO] [config.py:988:print]   load_universal_checkpoint .... False
head: [2024-06-20 10:31:54,585] [INFO] [config.py:988:print]   loss_scale ................... 0
head: [2024-06-20 10:31:54,585] [INFO] [config.py:988:print]   memory_breakdown ............. False
head: [2024-06-20 10:31:54,585] [INFO] [config.py:988:print]   mics_hierarchial_params_gather  False
head: [2024-06-20 10:31:54,585] [INFO] [config.py:988:print]   mics_shard_size .............. -1
head: [2024-06-20 10:31:54,585] [INFO] [config.py:988:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='step3_tensorboard/ds_tensorboard_logs/', job_name='step3_critic_tensorboard') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
head: [2024-06-20 10:31:54,585] [INFO] [config.py:988:print]   nebula_config ................ {
head:     "enabled": false, 
head:     "persistent_storage_path": null, 
head:     "persistent_time_interval": 100, 
head:     "num_of_version_in_retention": 2, 
head:     "enable_nebula_load": true, 
head:     "load_path": null
head: }
head: [2024-06-20 10:31:54,585] [INFO] [config.py:988:print]   optimizer_legacy_fusion ...... False
head: [2024-06-20 10:31:54,585] [INFO] [config.py:988:print]   optimizer_name ............... None
head: [2024-06-20 10:31:54,585] [INFO] [config.py:988:print]   optimizer_params ............. None
head: [2024-06-20 10:31:54,585] [INFO] [config.py:988:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
head: [2024-06-20 10:31:54,585] [INFO] [config.py:988:print]   pld_enabled .................. False
head: [2024-06-20 10:31:54,585] [INFO] [config.py:988:print]   pld_params ................... False
head: [2024-06-20 10:31:54,585] [INFO] [config.py:988:print]   prescale_gradients ........... False
head: [2024-06-20 10:31:54,585] [INFO] [config.py:988:print]   scheduler_name ............... None
head: [2024-06-20 10:31:54,585] [INFO] [config.py:988:print]   scheduler_params ............. None
head: [2024-06-20 10:31:54,585] [INFO] [config.py:988:print]   seq_parallel_communication_data_type  torch.float32
head: [2024-06-20 10:31:54,585] [INFO] [config.py:988:print]   sparse_attention ............. None
head: [2024-06-20 10:31:54,585] [INFO] [config.py:988:print]   sparse_gradients_enabled ..... False
head: [2024-06-20 10:31:54,585] [INFO] [config.py:988:print]   steps_per_print .............. 10
head: [2024-06-20 10:31:54,585] [INFO] [config.py:988:print]   train_batch_size ............. 512
head: [2024-06-20 10:31:54,585] [INFO] [config.py:988:print]   train_micro_batch_size_per_gpu  8
head: [2024-06-20 10:31:54,585] [INFO] [config.py:988:print]   use_data_before_expert_parallel_  False
head: [2024-06-20 10:31:54,585] [INFO] [config.py:988:print]   use_node_local_storage ....... False
head: [2024-06-20 10:31:54,585] [INFO] [config.py:988:print]   wall_clock_breakdown ......... False
head: [2024-06-20 10:31:54,586] [INFO] [config.py:988:print]   weight_quantization_config ... None
head: [2024-06-20 10:31:54,586] [INFO] [config.py:988:print]   world_size ................... 16
head: [2024-06-20 10:31:54,586] [INFO] [config.py:988:print]   zero_allow_untested_optimizer  False
head: [2024-06-20 10:31:54,586] [INFO] [config.py:988:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=30000000 param_persistence_threshold=10000 model_persistence_threshold=sys.maxsize max_live_parameters=30000000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=False pipeline_loading_checkpoint=False override_module_apply=True
head: [2024-06-20 10:31:54,586] [INFO] [config.py:988:print]   zero_enabled ................. True
head: [2024-06-20 10:31:54,586] [INFO] [config.py:988:print]   zero_force_ds_cpu_optimizer .. True
head: [2024-06-20 10:31:54,586] [INFO] [config.py:988:print]   zero_optimization_stage ...... 3
head: [2024-06-20 10:31:54,586] [INFO] [config.py:974:print_user_config]   json = {
head:     "train_batch_size": 512, 
head:     "train_micro_batch_size_per_gpu": 8, 
head:     "steps_per_print": 10, 
head:     "zero_optimization": {
head:         "stage": 3, 
head:         "offload_param": {
head:             "device": "none"
head:         }, 
head:         "offload_optimizer": {
head:             "device": "none"
head:         }, 
head:         "stage3_param_persistence_threshold": 1.000000e+04, 
head:         "stage3_max_live_parameters": 3.000000e+07, 
head:         "stage3_prefetch_bucket_size": 3.000000e+07, 
head:         "memory_efficient_linear": false
head:     }, 
head:     "fp16": {
head:         "enabled": true, 
head:         "loss_scale_window": 100, 
head:         "min_loss_scale": 1
head:     }, 
head:     "gradient_clipping": 1.0, 
head:     "prescale_gradients": false, 
head:     "wall_clock_breakdown": false, 
head:     "hybrid_engine": {
head:         "enabled": false, 
head:         "max_out_tokens": 512, 
head:         "inference_tp_size": 1, 
head:         "release_inference_cache": false, 
head:         "pin_parameters": true, 
head:         "tp_gather_partition_size": 8
head:     }, 
head:     "tensorboard": {
head:         "enabled": false, 
head:         "output_path": "step3_tensorboard/ds_tensorboard_logs/", 
head:         "job_name": "step3_critic_tensorboard"
head:     }
head: }
head: *****************[end] Initialized Critic Model [end] (duration: 10.02s)******************
head: ************************[start] Initializing Reward Model [start] ************************
head: [2024-06-20 10:31:54,741] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1162, num_elems = 26.69B
worker1: Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.15s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.16s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.21s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.23s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.24s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.26s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.27s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.31s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.58s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.56s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.57s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.58s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.58s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.59s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.59s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.51s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.49s/it]
worker1: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.49s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.49s/it]
worker1: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.50s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.49s/it]
head: Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.24s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.27s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.29s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.31s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.33s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.35s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.37s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:03,  1.98s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.60s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.60s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.62s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.63s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.64s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.68s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.68s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.89s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.51s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.50s/it]
head: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.47s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.50s/it]
worker1: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.52s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.50s/it]
worker1: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.53s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.50s/it]
head: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.50s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.51s/it]
worker1: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.53s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.51s/it]
head: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.52s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.51s/it]
worker1: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.53s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.52s/it]
worker1: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.54s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.52s/it]
head: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.53s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.52s/it]
head: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.54s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.54s/it]
head: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.54s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.54s/it]
head: Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.62s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.70s/it]
head: >Creating model from_config took 5.3178489208221436 seconds
head: [2024-06-20 10:31:59,904] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.6+b1e9b0fa, git-hash=b1e9b0fa, git-branch=main
head: [2024-06-20 10:31:59,926] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
head: [2024-06-20 10:31:59,928] [INFO] [logging.py:96:log_dist] [Rank 0] Creating ZeRO Offload
head: [2024-06-20 10:32:00,213] [INFO] [utils.py:791:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
head: [2024-06-20 10:32:00,214] [INFO] [utils.py:792:see_memory_usage] MA 16.6 GB         Max_MA 17.17 GB         CA 23.64 GB         Max_CA 24 GB 
head: [2024-06-20 10:32:00,214] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 29.02 GB, percent = 1.5%
head: Parameter Offload: Total persistent parameters: 270336 in 66 params
head: [2024-06-20 10:32:00,574] [INFO] [utils.py:791:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
head: [2024-06-20 10:32:00,574] [INFO] [utils.py:792:see_memory_usage] MA 16.37 GB         Max_MA 16.62 GB         CA 23.64 GB         Max_CA 24 GB 
head: [2024-06-20 10:32:00,575] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 29.01 GB, percent = 1.5%
head: [2024-06-20 10:32:00,576] [INFO] [config.py:984:print] DeepSpeedEngine configuration:
head: [2024-06-20 10:32:00,576] [INFO] [config.py:988:print]   activation_checkpointing_config  {
head:     "partition_activations": false, 
head:     "contiguous_memory_optimization": false, 
head:     "cpu_checkpointing": false, 
head:     "number_checkpoints": null, 
head:     "synchronize_checkpoint_boundary": false, 
head:     "profile": false
head: }
head: [2024-06-20 10:32:00,576] [INFO] [config.py:988:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
head: [2024-06-20 10:32:00,576] [INFO] [config.py:988:print]   amp_enabled .................. False
head: [2024-06-20 10:32:00,576] [INFO] [config.py:988:print]   amp_params ................... False
head: [2024-06-20 10:32:00,576] [INFO] [config.py:988:print]   autotuning_config ............ {
head:     "enabled": false, 
head:     "start_step": null, 
head:     "end_step": null, 
head:     "metric_path": null, 
head:     "arg_mappings": null, 
head:     "metric": "throughput", 
head:     "model_info": null, 
head:     "results_dir": "autotuning_results", 
head:     "exps_dir": "autotuning_exps", 
head:     "overwrite": true, 
head:     "fast": true, 
head:     "start_profile_step": 3, 
head:     "end_profile_step": 5, 
head:     "tuner_type": "gridsearch", 
head:     "tuner_early_stopping": 5, 
head:     "tuner_num_trials": 50, 
head:     "model_info_path": null, 
head:     "mp_size": 1, 
head:     "max_train_batch_size": null, 
head:     "min_train_batch_size": 1, 
head:     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
head:     "min_train_micro_batch_size_per_gpu": 1, 
head:     "num_tuning_micro_batch_sizes": 3
head: }
head: [2024-06-20 10:32:00,576] [INFO] [config.py:988:print]   bfloat16_enabled ............. False
head: [2024-06-20 10:32:00,576] [INFO] [config.py:988:print]   checkpoint_parallel_write_pipeline  False
head: [2024-06-20 10:32:00,576] [INFO] [config.py:988:print]   checkpoint_tag_validation_enabled  True
head: [2024-06-20 10:32:00,576] [INFO] [config.py:988:print]   checkpoint_tag_validation_fail  False
head: [2024-06-20 10:32:00,576] [INFO] [config.py:988:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f5b6c24ea10>
head: [2024-06-20 10:32:00,576] [INFO] [config.py:988:print]   communication_data_type ...... None
head: [2024-06-20 10:32:00,577] [INFO] [config.py:988:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
head: [2024-06-20 10:32:00,577] [INFO] [config.py:988:print]   curriculum_enabled_legacy .... False
head: [2024-06-20 10:32:00,577] [INFO] [config.py:988:print]   curriculum_params_legacy ..... False
head: [2024-06-20 10:32:00,577] [INFO] [config.py:988:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
head: [2024-06-20 10:32:00,577] [INFO] [config.py:988:print]   data_efficiency_enabled ...... False
head: [2024-06-20 10:32:00,577] [INFO] [config.py:988:print]   dataloader_drop_last ......... False
head: [2024-06-20 10:32:00,577] [INFO] [config.py:988:print]   disable_allgather ............ False
head: [2024-06-20 10:32:00,577] [INFO] [config.py:988:print]   dump_state ................... False
head: [2024-06-20 10:32:00,577] [INFO] [config.py:988:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
head: [2024-06-20 10:32:00,577] [INFO] [config.py:988:print]   eigenvalue_enabled ........... False
head: [2024-06-20 10:32:00,577] [INFO] [config.py:988:print]   eigenvalue_gas_boundary_resolution  1
head: [2024-06-20 10:32:00,577] [INFO] [config.py:988:print]   eigenvalue_layer_name ........ bert.encoder.layer
head: [2024-06-20 10:32:00,577] [INFO] [config.py:988:print]   eigenvalue_layer_num ......... 0
head: [2024-06-20 10:32:00,577] [INFO] [config.py:988:print]   eigenvalue_max_iter .......... 100
head: [2024-06-20 10:32:00,577] [INFO] [config.py:988:print]   eigenvalue_stability ......... 1e-06
head: [2024-06-20 10:32:00,577] [INFO] [config.py:988:print]   eigenvalue_tol ............... 0.01
head: [2024-06-20 10:32:00,577] [INFO] [config.py:988:print]   eigenvalue_verbose ........... False
head: [2024-06-20 10:32:00,577] [INFO] [config.py:988:print]   elasticity_enabled ........... False
head: [2024-06-20 10:32:00,577] [INFO] [config.py:988:print]   flops_profiler_config ........ {
head:     "enabled": false, 
head:     "recompute_fwd_factor": 0.0, 
head:     "profile_step": 1, 
head:     "module_depth": -1, 
head:     "top_modules": 1, 
head:     "detailed": true, 
head:     "output_file": null
head: }
head: [2024-06-20 10:32:00,577] [INFO] [config.py:988:print]   fp16_auto_cast ............... False
head: [2024-06-20 10:32:00,577] [INFO] [config.py:988:print]   fp16_enabled ................. True
head: [2024-06-20 10:32:00,577] [INFO] [config.py:988:print]   fp16_master_weights_and_gradients  False
head: [2024-06-20 10:32:00,577] [INFO] [config.py:988:print]   global_rank .................. 0
head: [2024-06-20 10:32:00,577] [INFO] [config.py:988:print]   grad_accum_dtype ............. None
head: [2024-06-20 10:32:00,577] [INFO] [config.py:988:print]   gradient_accumulation_steps .. 4
head: [2024-06-20 10:32:00,577] [INFO] [config.py:988:print]   gradient_clipping ............ 1.0
head: [2024-06-20 10:32:00,577] [INFO] [config.py:988:print]   gradient_predivide_factor .... 1.0
head: [2024-06-20 10:32:00,577] [INFO] [config.py:988:print]   graph_harvesting ............. False
head: [2024-06-20 10:32:00,577] [INFO] [config.py:988:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
head: [2024-06-20 10:32:00,577] [INFO] [config.py:988:print]   initial_dynamic_scale ........ 65536
head: [2024-06-20 10:32:00,577] [INFO] [config.py:988:print]   load_universal_checkpoint .... False
head: [2024-06-20 10:32:00,577] [INFO] [config.py:988:print]   loss_scale ................... 0
head: [2024-06-20 10:32:00,577] [INFO] [config.py:988:print]   memory_breakdown ............. False
head: [2024-06-20 10:32:00,577] [INFO] [config.py:988:print]   mics_hierarchial_params_gather  False
head: [2024-06-20 10:32:00,577] [INFO] [config.py:988:print]   mics_shard_size .............. -1
head: [2024-06-20 10:32:00,577] [INFO] [config.py:988:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
head: [2024-06-20 10:32:00,577] [INFO] [config.py:988:print]   nebula_config ................ {
head:     "enabled": false, 
head:     "persistent_storage_path": null, 
head:     "persistent_time_interval": 100, 
head:     "num_of_version_in_retention": 2, 
head:     "enable_nebula_load": true, 
head:     "load_path": null
head: }
head: [2024-06-20 10:32:00,577] [INFO] [config.py:988:print]   optimizer_legacy_fusion ...... False
head: [2024-06-20 10:32:00,577] [INFO] [config.py:988:print]   optimizer_name ............... None
head: [2024-06-20 10:32:00,577] [INFO] [config.py:988:print]   optimizer_params ............. None
head: [2024-06-20 10:32:00,577] [INFO] [config.py:988:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
head: [2024-06-20 10:32:00,577] [INFO] [config.py:988:print]   pld_enabled .................. False
head: [2024-06-20 10:32:00,577] [INFO] [config.py:988:print]   pld_params ................... False
head: [2024-06-20 10:32:00,578] [INFO] [config.py:988:print]   prescale_gradients ........... False
head: [2024-06-20 10:32:00,578] [INFO] [config.py:988:print]   scheduler_name ............... None
head: [2024-06-20 10:32:00,578] [INFO] [config.py:988:print]   scheduler_params ............. None
head: [2024-06-20 10:32:00,578] [INFO] [config.py:988:print]   seq_parallel_communication_data_type  torch.float32
head: [2024-06-20 10:32:00,578] [INFO] [config.py:988:print]   sparse_attention ............. None
head: [2024-06-20 10:32:00,578] [INFO] [config.py:988:print]   sparse_gradients_enabled ..... False
head: [2024-06-20 10:32:00,578] [INFO] [config.py:988:print]   steps_per_print .............. 10
head: [2024-06-20 10:32:00,578] [INFO] [config.py:988:print]   train_batch_size ............. 512
head: [2024-06-20 10:32:00,578] [INFO] [config.py:988:print]   train_micro_batch_size_per_gpu  8
head: [2024-06-20 10:32:00,578] [INFO] [config.py:988:print]   use_data_before_expert_parallel_  False
head: [2024-06-20 10:32:00,578] [INFO] [config.py:988:print]   use_node_local_storage ....... False
head: [2024-06-20 10:32:00,578] [INFO] [config.py:988:print]   wall_clock_breakdown ......... False
head: [2024-06-20 10:32:00,578] [INFO] [config.py:988:print]   weight_quantization_config ... None
head: [2024-06-20 10:32:00,578] [INFO] [config.py:988:print]   world_size ................... 16
head: [2024-06-20 10:32:00,578] [INFO] [config.py:988:print]   zero_allow_untested_optimizer  False
head: [2024-06-20 10:32:00,578] [INFO] [config.py:988:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=10000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=False pipeline_loading_checkpoint=False override_module_apply=True
head: [2024-06-20 10:32:00,578] [INFO] [config.py:988:print]   zero_enabled ................. True
head: [2024-06-20 10:32:00,578] [INFO] [config.py:988:print]   zero_force_ds_cpu_optimizer .. True
head: [2024-06-20 10:32:00,578] [INFO] [config.py:988:print]   zero_optimization_stage ...... 3
head: [2024-06-20 10:32:00,578] [INFO] [config.py:974:print_user_config]   json = {
head:     "train_batch_size": 512, 
head:     "train_micro_batch_size_per_gpu": 8, 
head:     "steps_per_print": 10, 
head:     "zero_optimization": {
head:         "stage": 3, 
head:         "stage3_param_persistence_threshold": 1.000000e+04, 
head:         "offload_param": {
head:             "device": "none"
head:         }, 
head:         "memory_efficient_linear": false
head:     }, 
head:     "fp16": {
head:         "enabled": true, 
head:         "min_loss_scale": 1
head:     }, 
head:     "gradient_clipping": 1.0, 
head:     "prescale_gradients": false, 
head:     "wall_clock_breakdown": false
head: }
head: ******************[end] Initialized Reward Model [end] (duration: 5.99s)******************
head: ***** Running training (total_iters=87) *****
head: Beginning of Epoch 1/1, Total Generation Batches 349
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
head:   warnings.warn(
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
head:   warnings.warn(
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
head:   warnings.warn(
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
head:   warnings.warn(
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
head:   warnings.warn(
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
head:   warnings.warn(
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
head:   warnings.warn(
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
head:   warnings.warn(
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
head:   warnings.warn(
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
head:   warnings.warn(
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
head:   warnings.warn(
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
worker1:   warnings.warn(
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
worker1:   warnings.warn(
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
head:   warnings.warn(
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
worker1:   warnings.warn(
head:   warnings.warn(
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
head:   warnings.warn(
worker1:   warnings.warn(
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
worker1:   warnings.warn(
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
head:   warnings.warn(
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
worker1:   warnings.warn(
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
head:   warnings.warn(
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
worker1:   warnings.warn(
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
worker1:   warnings.warn(
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
worker1:   warnings.warn(
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
worker1:   warnings.warn(
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
worker1:   warnings.warn(
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
worker1:   warnings.warn(
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
worker1:   warnings.warn(
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
worker1:   warnings.warn(
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
worker1:   warnings.warn(
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
worker1:   warnings.warn(
head: ------------------------------------------------------
head: Free memory : 48.242371 (GigaBytes)  
head: Total memory: 79.346863 (GigaBytes)  
head: Requested memory: 22.500000 (GigaBytes) 
head: Setting maximum total tokens (input + output) to 2048 
head: WorkSpace: 0x7f49a0000000 
head: ------------------------------------------------------
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
worker1:   warnings.warn(
head:   warnings.warn(
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
worker1:   warnings.warn(
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
worker1:   warnings.warn(
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
worker1:   warnings.warn(
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
worker1:   warnings.warn(
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
head:   warnings.warn(
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
worker1:   warnings.warn(
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
head:   warnings.warn(
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
worker1:   warnings.warn(
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
worker1:   warnings.warn(
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
worker1:   warnings.warn(
head:   warnings.warn(
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
worker1:   warnings.warn(
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
head:   warnings.warn(
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
head:   warnings.warn(
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
worker1:   warnings.warn(
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
worker1:   warnings.warn(
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
head:   warnings.warn(
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
worker1:   warnings.warn(
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
head:   warnings.warn(
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
head:   warnings.warn(
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
head:   warnings.warn(
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
worker1:   warnings.warn(
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
worker1:   warnings.warn(
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
head:   warnings.warn(
worker1:   warnings.warn(
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
head:   warnings.warn(
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
head:   warnings.warn(
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
head:   warnings.warn(
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
head:   warnings.warn(
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
head:   warnings.warn(
head: |E2E latency=48.14s |Gather latency=0.27s (0.56%) |Generate time=35.02s (72.74%) |Training time=0.00s (0.00%) |Others=13.12 (27.26%)|CurSamplesPerSec=2.66 |AvgSamplesPerSec=2.66
head: |E2E latency=47.51s |Gather latency=0.64s (1.35%) |Generate time=33.49s (70.48%) |Training time=0.00s (0.00%) |Others=14.02 (29.52%)|CurSamplesPerSec=2.69 |AvgSamplesPerSec=2.68
head: |E2E latency=48.33s |Gather latency=0.51s (1.05%) |Generate time=33.49s (69.29%) |Training time=0.00s (0.00%) |Others=14.84 (30.71%)|CurSamplesPerSec=2.65 |AvgSamplesPerSec=2.67
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:393006 [0] NCCL INFO Using network IBext
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:196239 [3] NCCL INFO Using network IBext
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:196240 [5] NCCL INFO Using network IBext
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:196241 [0] NCCL INFO Using network IBext
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:196242 [4] NCCL INFO Using network IBext
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:196244 [1] NCCL INFO Using network IBext
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:196243 [6] NCCL INFO Using network IBext
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:393007 [4] NCCL INFO Using network IBext
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:393008 [7] NCCL INFO Using network IBext
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:393009 [1] NCCL INFO Using network IBext
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:393010 [2] NCCL INFO Using network IBext
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:393011 [6] NCCL INFO Using network IBext
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:196245 [7] NCCL INFO Using network IBext
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:196246 [2] NCCL INFO Using network IBext
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:393014 [3] NCCL INFO Using network IBext
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:393015 [5] NCCL INFO Using network IBext
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:393008 [7] NCCL INFO comm 0x7f102c089610 rank 7 nranks 16 cudaDev 7 nvmlDev 7 busId d8000 commId 0x638f1c12735ff76d - Init START
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:393015 [5] NCCL INFO comm 0x7fbba4077740 rank 5 nranks 16 cudaDev 5 nvmlDev 5 busId b7000 commId 0x638f1c12735ff76d - Init START
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:393011 [6] NCCL INFO comm 0x7efcc807baf0 rank 6 nranks 16 cudaDev 6 nvmlDev 6 busId d5000 commId 0x638f1c12735ff76d - Init START
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:393009 [1] NCCL INFO comm 0x7fe26407b330 rank 1 nranks 16 cudaDev 1 nvmlDev 1 busId 52000 commId 0x638f1c12735ff76d - Init START
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:196245 [7] NCCL INFO comm 0x7f81e007bc90 rank 15 nranks 16 cudaDev 7 nvmlDev 7 busId d8000 commId 0x638f1c12735ff76d - Init START
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:196243 [6] NCCL INFO comm 0x7fdca007e8d0 rank 14 nranks 16 cudaDev 6 nvmlDev 6 busId d5000 commId 0x638f1c12735ff76d - Init START
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:393006 [0] NCCL INFO comm 0x7f5068084380 rank 0 nranks 16 cudaDev 0 nvmlDev 0 busId 4d000 commId 0x638f1c12735ff76d - Init START
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:393014 [3] NCCL INFO comm 0x7f6280077680 rank 3 nranks 16 cudaDev 3 nvmlDev 3 busId 6f000 commId 0x638f1c12735ff76d - Init START
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:393007 [4] NCCL INFO comm 0x7fb978089b80 rank 4 nranks 16 cudaDev 4 nvmlDev 4 busId b3000 commId 0x638f1c12735ff76d - Init START
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:393010 [2] NCCL INFO comm 0x7f92d807ca10 rank 2 nranks 16 cudaDev 2 nvmlDev 2 busId 69000 commId 0x638f1c12735ff76d - Init START
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:196240 [5] NCCL INFO comm 0x7f868807c660 rank 13 nranks 16 cudaDev 5 nvmlDev 5 busId b7000 commId 0x638f1c12735ff76d - Init START
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:196242 [4] NCCL INFO comm 0x7f1e88088950 rank 12 nranks 16 cudaDev 4 nvmlDev 4 busId b3000 commId 0x638f1c12735ff76d - Init START
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:196239 [3] NCCL INFO comm 0x7f34e007c9b0 rank 11 nranks 16 cudaDev 3 nvmlDev 3 busId 6f000 commId 0x638f1c12735ff76d - Init START
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:196246 [2] NCCL INFO comm 0x7fd50c07b9f0 rank 10 nranks 16 cudaDev 2 nvmlDev 2 busId 69000 commId 0x638f1c12735ff76d - Init START
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:196244 [1] NCCL INFO comm 0x7fc0a407df60 rank 9 nranks 16 cudaDev 1 nvmlDev 1 busId 52000 commId 0x638f1c12735ff76d - Init START
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:196241 [0] NCCL INFO comm 0x7f768407d120 rank 8 nranks 16 cudaDev 0 nvmlDev 0 busId 4d000 commId 0x638f1c12735ff76d - Init START
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:196244 [1] NCCL INFO Setting affinity for GPU 1 to ffffffff,00000000,ffffffff
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:196239 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,00000000,ffffffff
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:196241 [0] NCCL INFO Setting affinity for GPU 0 to ffffffff,00000000,ffffffff
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:196245 [7] NCCL INFO Setting affinity for GPU 7 to ffffffff,00000000,ffffffff,00000000
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:196243 [6] NCCL INFO Setting affinity for GPU 6 to ffffffff,00000000,ffffffff,00000000
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:196246 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff,00000000,ffffffff
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:196240 [5] NCCL INFO Setting affinity for GPU 5 to ffffffff,00000000,ffffffff,00000000
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:196242 [4] NCCL INFO Setting affinity for GPU 4 to ffffffff,00000000,ffffffff,00000000
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:393011 [6] NCCL INFO Setting affinity for GPU 6 to ffffffff,00000000,ffffffff,00000000
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:393008 [7] NCCL INFO Setting affinity for GPU 7 to ffffffff,00000000,ffffffff,00000000
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:393015 [5] NCCL INFO Setting affinity for GPU 5 to ffffffff,00000000,ffffffff,00000000
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:393014 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,00000000,ffffffff
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:393007 [4] NCCL INFO Setting affinity for GPU 4 to ffffffff,00000000,ffffffff,00000000
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:393006 [0] NCCL INFO Setting affinity for GPU 0 to ffffffff,00000000,ffffffff
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:393009 [1] NCCL INFO Setting affinity for GPU 1 to ffffffff,00000000,ffffffff
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:393010 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff,00000000,ffffffff
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:393015 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] 6/-1/-1->5->4 [3] -1/-1/-1->5->4 [4] 6/-1/-1->5->4 [5] 6/-1/-1->5->4 [6] 6/-1/-1->5->4 [7] -1/-1/-1->5->4
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:393015 [5] NCCL INFO P2P Chunksize set to 131072
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:393011 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->5 [3] 7/14/-1->6->-1 [4] 7/-1/-1->6->5 [5] 7/-1/-1->6->5 [6] 7/-1/-1->6->5 [7] 7/-1/-1->6->14
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:393011 [6] NCCL INFO P2P Chunksize set to 131072
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:393008 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] 0/-1/-1->7->6 [2] 0/-1/-1->7->6 [3] 0/-1/-1->7->6 [4] -1/-1/-1->7->6 [5] 0/-1/-1->7->6 [6] 0/-1/-1->7->6 [7] 0/-1/-1->7->6
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:393008 [7] NCCL INFO P2P Chunksize set to 131072
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:393007 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3 [2] 5/12/-1->4->-1 [3] 5/-1/-1->4->3 [4] 5/-1/-1->4->3 [5] 5/-1/-1->4->3 [6] 5/-1/-1->4->12 [7] 5/-1/-1->4->3
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:393007 [4] NCCL INFO P2P Chunksize set to 131072
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:196243 [6] NCCL INFO Trees [0] 15/-1/-1->14->13 [1] 15/-1/-1->14->13 [2] 15/-1/-1->14->13 [3] 15/-1/-1->14->6 [4] 15/-1/-1->14->13 [5] 15/-1/-1->14->13 [6] 15/-1/-1->14->13 [7] 15/6/-1->14->-1
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:196245 [7] NCCL INFO Trees [0] -1/-1/-1->15->14 [1] 8/-1/-1->15->14 [2] 8/-1/-1->15->14 [3] 8/-1/-1->15->14 [4] -1/-1/-1->15->14 [5] 8/-1/-1->15->14 [6] 8/-1/-1->15->14 [7] 8/-1/-1->15->14
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:196243 [6] NCCL INFO P2P Chunksize set to 131072
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:196245 [7] NCCL INFO P2P Chunksize set to 131072
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:196240 [5] NCCL INFO Trees [0] 14/-1/-1->13->12 [1] 14/-1/-1->13->12 [2] 14/-1/-1->13->12 [3] -1/-1/-1->13->12 [4] 14/-1/-1->13->12 [5] 14/-1/-1->13->12 [6] 14/-1/-1->13->12 [7] -1/-1/-1->13->12
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:196240 [5] NCCL INFO P2P Chunksize set to 131072
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:196242 [4] NCCL INFO Trees [0] 13/-1/-1->12->11 [1] 13/-1/-1->12->11 [2] 13/-1/-1->12->4 [3] 13/-1/-1->12->11 [4] 13/-1/-1->12->11 [5] 13/-1/-1->12->11 [6] 13/4/-1->12->-1 [7] 13/-1/-1->12->11
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:196241 [0] NCCL INFO Trees [0] 9/-1/-1->8->0 [1] 9/-1/-1->8->15 [2] 9/-1/-1->8->15 [3] 9/-1/-1->8->15 [4] 9/0/-1->8->-1 [5] 9/-1/-1->8->15 [6] 9/-1/-1->8->15 [7] 9/-1/-1->8->15
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:196246 [2] NCCL INFO Trees [0] 11/-1/-1->10->9 [1] 11/-1/-1->10->2 [2] 11/-1/-1->10->9 [3] 11/-1/-1->10->9 [4] 11/-1/-1->10->9 [5] 11/2/-1->10->-1 [6] 11/-1/-1->10->9 [7] 11/-1/-1->10->9
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:196242 [4] NCCL INFO P2P Chunksize set to 131072
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:196241 [0] NCCL INFO P2P Chunksize set to 131072
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:196246 [2] NCCL INFO P2P Chunksize set to 131072
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:196244 [1] NCCL INFO Trees [0] 10/-1/-1->9->8 [1] -1/-1/-1->9->8 [2] 10/-1/-1->9->8 [3] 10/-1/-1->9->8 [4] 10/-1/-1->9->8 [5] -1/-1/-1->9->8 [6] 10/-1/-1->9->8 [7] 10/-1/-1->9->8
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:393014 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2 [2] -1/-1/-1->3->2 [3] 4/-1/-1->3->2 [4] 4/-1/-1->3->2 [5] 4/-1/-1->3->2 [6] -1/-1/-1->3->2 [7] 4/-1/-1->3->2
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:196244 [1] NCCL INFO P2P Chunksize set to 131072
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:393014 [3] NCCL INFO P2P Chunksize set to 131072
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:196239 [3] NCCL INFO Trees [0] 12/-1/-1->11->10 [1] 12/-1/-1->11->10 [2] -1/-1/-1->11->10 [3] 12/-1/-1->11->10 [4] 12/-1/-1->11->10 [5] 12/-1/-1->11->10 [6] -1/-1/-1->11->10 [7] 12/-1/-1->11->10
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:196239 [3] NCCL INFO P2P Chunksize set to 131072
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:393010 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/10/-1->2->-1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->10 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:393010 [2] NCCL INFO P2P Chunksize set to 131072
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:393006 [0] NCCL INFO Channel 00/08 :    0   7   6   5   4   3   2   1   8  15  14  13  12  11  10   9
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:393009 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] -1/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] -1/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:393006 [0] NCCL INFO Channel 01/08 :    0   3  10  15  14  13  12   9   8  11   2   7   6   5   4   1
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:393009 [1] NCCL INFO P2P Chunksize set to 131072
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:393006 [0] NCCL INFO Channel 02/08 :    0   7   6   5  12  11  10   9   8  15  14  13   4   3   2   1
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:393006 [0] NCCL INFO Channel 03/08 :    0   5   4   7  14  11  10   9   8  13  12  15   6   3   2   1
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:393006 [0] NCCL INFO Channel 04/08 :    0   7   6   5   4   3   2   1   8  15  14  13  12  11  10   9
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:393006 [0] NCCL INFO Channel 05/08 :    0   3  10  15  14  13  12   9   8  11   2   7   6   5   4   1
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:393006 [0] NCCL INFO Channel 06/08 :    0   7   6   5  12  11  10   9   8  15  14  13   4   3   2   1
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:393006 [0] NCCL INFO Channel 07/08 :    0   5   4   7  14  11  10   9   8  13  12  15   6   3   2   1
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:393006 [0] NCCL INFO Trees [0] 1/8/-1->0->-1 [1] 1/-1/-1->0->7 [2] 1/-1/-1->0->7 [3] 1/-1/-1->0->7 [4] 1/-1/-1->0->8 [5] 1/-1/-1->0->7 [6] 1/-1/-1->0->7 [7] 1/-1/-1->0->7
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:393006 [0] NCCL INFO P2P Chunksize set to 131072
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:393007 [4] NCCL INFO Channel 03/0 : 4[4] -> 7[7] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:196242 [4] NCCL INFO Channel 03/0 : 12[4] -> 15[7] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:393010 [2] NCCL INFO Channel 01/0 : 2[2] -> 7[7] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:393006 [0] NCCL INFO Channel 01/0 : 0[0] -> 3[3] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:196246 [2] NCCL INFO Channel 01/0 : 10[2] -> 15[7] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:196241 [0] NCCL INFO Channel 01/0 : 8[0] -> 11[3] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:196243 [6] NCCL INFO Channel 03/0 : 7[7] -> 14[6] [receive] via NET/IBext/3/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:393011 [6] NCCL INFO Channel 03/0 : 15[7] -> 6[6] [receive] via NET/IBext/3/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:196243 [6] NCCL INFO Channel 07/0 : 7[7] -> 14[6] [receive] via NET/IBext/3/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:393011 [6] NCCL INFO Channel 07/0 : 15[7] -> 6[6] [receive] via NET/IBext/3/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:196244 [1] NCCL INFO Channel 00/0 : 9[1] -> 0[0] [send] via NET/IBext/0/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:393009 [1] NCCL INFO Channel 00/0 : 1[1] -> 8[0] [send] via NET/IBext/0/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:196244 [1] NCCL INFO Channel 04/0 : 9[1] -> 0[0] [send] via NET/IBext/0/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:393009 [1] NCCL INFO Channel 04/0 : 1[1] -> 8[0] [send] via NET/IBext/0/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:393007 [4] NCCL INFO Channel 07/0 : 4[4] -> 7[7] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:393010 [2] NCCL INFO Channel 05/0 : 2[2] -> 7[7] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:196242 [4] NCCL INFO Channel 07/0 : 12[4] -> 15[7] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:196241 [0] NCCL INFO Channel 05/0 : 8[0] -> 11[3] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:196246 [2] NCCL INFO Channel 05/0 : 10[2] -> 15[7] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:393006 [0] NCCL INFO Channel 05/0 : 0[0] -> 3[3] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:196241 [0] NCCL INFO Channel 03/0 : 8[0] -> 13[5] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:196241 [0] NCCL INFO Channel 07/0 : 8[0] -> 13[5] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:393006 [0] NCCL INFO Channel 03/0 : 0[0] -> 5[5] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:393007 [4] NCCL INFO Channel 02/0 : 13[5] -> 4[4] [receive] via NET/IBext/2/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:393007 [4] NCCL INFO Channel 06/0 : 13[5] -> 4[4] [receive] via NET/IBext/2/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:393006 [0] NCCL INFO Channel 07/0 : 0[0] -> 5[5] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:196242 [4] NCCL INFO Channel 02/0 : 5[5] -> 12[4] [receive] via NET/IBext/2/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:196242 [4] NCCL INFO Channel 06/0 : 5[5] -> 12[4] [receive] via NET/IBext/2/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:196239 [3] NCCL INFO Channel 01/0 : 11[3] -> 2[2] [send] via NET/IBext/1/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:196239 [3] NCCL INFO Channel 05/0 : 11[3] -> 2[2] [send] via NET/IBext/1/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:393010 [2] NCCL INFO Channel 01/0 : 11[3] -> 2[2] [receive] via NET/IBext/1/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:393010 [2] NCCL INFO Channel 05/0 : 11[3] -> 2[2] [receive] via NET/IBext/1/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:393014 [3] NCCL INFO Channel 01/0 : 3[3] -> 10[2] [send] via NET/IBext/1/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:393014 [3] NCCL INFO Channel 05/0 : 3[3] -> 10[2] [send] via NET/IBext/1/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:196240 [5] NCCL INFO Channel 02/0 : 13[5] -> 4[4] [send] via NET/IBext/2/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:196246 [2] NCCL INFO Channel 01/0 : 3[3] -> 10[2] [receive] via NET/IBext/1/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:196240 [5] NCCL INFO Channel 06/0 : 13[5] -> 4[4] [send] via NET/IBext/2/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:196246 [2] NCCL INFO Channel 05/0 : 3[3] -> 10[2] [receive] via NET/IBext/1/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:196241 [0] NCCL INFO Channel 00/0 : 1[1] -> 8[0] [receive] via NET/IBext/0/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:393007 [4] NCCL INFO Channel 01/0 : 4[4] -> 1[1] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:196241 [0] NCCL INFO Channel 04/0 : 1[1] -> 8[0] [receive] via NET/IBext/0/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:196241 [0] NCCL INFO Channel 00/0 : 8[0] -> 15[7] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:196245 [7] NCCL INFO Channel 03/0 : 15[7] -> 6[6] [send] via NET/IBext/3/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:196245 [7] NCCL INFO Channel 07/0 : 15[7] -> 6[6] [send] via NET/IBext/3/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:393011 [6] NCCL INFO Channel 03/0 : 6[6] -> 3[3] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:393007 [4] NCCL INFO Channel 05/0 : 4[4] -> 1[1] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:393015 [5] NCCL INFO Channel 02/0 : 5[5] -> 12[4] [send] via NET/IBext/2/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:196241 [0] NCCL INFO Channel 02/0 : 8[0] -> 15[7] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:393015 [5] NCCL INFO Channel 06/0 : 5[5] -> 12[4] [send] via NET/IBext/2/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:196242 [4] NCCL INFO Channel 01/0 : 12[4] -> 9[1] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:393006 [0] NCCL INFO Channel 00/0 : 9[1] -> 0[0] [receive] via NET/IBext/0/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:393006 [0] NCCL INFO Channel 04/0 : 9[1] -> 0[0] [receive] via NET/IBext/0/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:393006 [0] NCCL INFO Channel 00/0 : 0[0] -> 7[7] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:393008 [7] NCCL INFO Channel 03/0 : 7[7] -> 14[6] [send] via NET/IBext/3/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:393008 [7] NCCL INFO Channel 07/0 : 7[7] -> 14[6] [send] via NET/IBext/3/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:196243 [6] NCCL INFO Channel 03/0 : 14[6] -> 11[3] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:196241 [0] NCCL INFO Channel 04/0 : 8[0] -> 15[7] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:393011 [6] NCCL INFO Channel 07/0 : 6[6] -> 3[3] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:196242 [4] NCCL INFO Channel 05/0 : 12[4] -> 9[1] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:393006 [0] NCCL INFO Channel 02/0 : 0[0] -> 7[7] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:196243 [6] NCCL INFO Channel 07/0 : 14[6] -> 11[3] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:196241 [0] NCCL INFO Channel 06/0 : 8[0] -> 15[7] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:393006 [0] NCCL INFO Channel 04/0 : 0[0] -> 7[7] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:196245 [7] NCCL INFO Channel 00/0 : 15[7] -> 14[6] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:393006 [0] NCCL INFO Channel 06/0 : 0[0] -> 7[7] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:196245 [7] NCCL INFO Channel 01/0 : 15[7] -> 14[6] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:196240 [5] NCCL INFO Channel 00/0 : 13[5] -> 12[4] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:196246 [2] NCCL INFO Channel 00/0 : 10[2] -> 9[1] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:393008 [7] NCCL INFO Channel 00/0 : 7[7] -> 6[6] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:196245 [7] NCCL INFO Channel 02/0 : 15[7] -> 14[6] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:393010 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:196240 [5] NCCL INFO Channel 01/0 : 13[5] -> 12[4] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:196246 [2] NCCL INFO Channel 02/0 : 10[2] -> 9[1] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:196245 [7] NCCL INFO Channel 04/0 : 15[7] -> 14[6] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:393008 [7] NCCL INFO Channel 01/0 : 7[7] -> 6[6] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:393010 [2] NCCL INFO Channel 02/0 : 2[2] -> 1[1] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:393014 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:196246 [2] NCCL INFO Channel 03/0 : 10[2] -> 9[1] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:196240 [5] NCCL INFO Channel 03/0 : 13[5] -> 12[4] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:196239 [3] NCCL INFO Channel 00/0 : 11[3] -> 10[2] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:196245 [7] NCCL INFO Channel 05/0 : 15[7] -> 14[6] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:393015 [5] NCCL INFO Channel 00/0 : 5[5] -> 4[4] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:393008 [7] NCCL INFO Channel 02/0 : 7[7] -> 6[6] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:393010 [2] NCCL INFO Channel 03/0 : 2[2] -> 1[1] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:196246 [2] NCCL INFO Channel 04/0 : 10[2] -> 9[1] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:393014 [3] NCCL INFO Channel 02/0 : 3[3] -> 2[2] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:196240 [5] NCCL INFO Channel 04/0 : 13[5] -> 12[4] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:196239 [3] NCCL INFO Channel 02/0 : 11[3] -> 10[2] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:393015 [5] NCCL INFO Channel 01/0 : 5[5] -> 4[4] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:196245 [7] NCCL INFO Channel 06/0 : 15[7] -> 14[6] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:393008 [7] NCCL INFO Channel 04/0 : 7[7] -> 6[6] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:393010 [2] NCCL INFO Channel 04/0 : 2[2] -> 1[1] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:393014 [3] NCCL INFO Channel 03/0 : 3[3] -> 2[2] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:196246 [2] NCCL INFO Channel 06/0 : 10[2] -> 9[1] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:196240 [5] NCCL INFO Channel 05/0 : 13[5] -> 12[4] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:196243 [6] NCCL INFO Channel 00/0 : 14[6] -> 13[5] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:196239 [3] NCCL INFO Channel 03/0 : 11[3] -> 10[2] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:393015 [5] NCCL INFO Channel 03/0 : 5[5] -> 4[4] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:393008 [7] NCCL INFO Channel 05/0 : 7[7] -> 6[6] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:393010 [2] NCCL INFO Channel 06/0 : 2[2] -> 1[1] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:196246 [2] NCCL INFO Channel 07/0 : 10[2] -> 9[1] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:393014 [3] NCCL INFO Channel 04/0 : 3[3] -> 2[2] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:196240 [5] NCCL INFO Channel 07/0 : 13[5] -> 12[4] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:196243 [6] NCCL INFO Channel 01/0 : 14[6] -> 13[5] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:196239 [3] NCCL INFO Channel 04/0 : 11[3] -> 10[2] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:393015 [5] NCCL INFO Channel 04/0 : 5[5] -> 4[4] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:393008 [7] NCCL INFO Channel 06/0 : 7[7] -> 6[6] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:393010 [2] NCCL INFO Channel 07/0 : 2[2] -> 1[1] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:393014 [3] NCCL INFO Channel 06/0 : 3[3] -> 2[2] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:196243 [6] NCCL INFO Channel 02/0 : 14[6] -> 13[5] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:196239 [3] NCCL INFO Channel 06/0 : 11[3] -> 10[2] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:196243 [6] NCCL INFO Channel 04/0 : 14[6] -> 13[5] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:196239 [3] NCCL INFO Channel 07/0 : 11[3] -> 10[2] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:393015 [5] NCCL INFO Channel 05/0 : 5[5] -> 4[4] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:393014 [3] NCCL INFO Channel 07/0 : 3[3] -> 2[2] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:393011 [6] NCCL INFO Channel 00/0 : 6[6] -> 5[5] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:196243 [6] NCCL INFO Channel 05/0 : 14[6] -> 13[5] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:393007 [4] NCCL INFO Channel 00/0 : 4[4] -> 3[3] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:393015 [5] NCCL INFO Channel 07/0 : 5[5] -> 4[4] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:196244 [1] NCCL INFO Channel 01/0 : 9[1] -> 8[0] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:196242 [4] NCCL INFO Channel 00/0 : 12[4] -> 11[3] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:393011 [6] NCCL INFO Channel 01/0 : 6[6] -> 5[5] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:196243 [6] NCCL INFO Channel 06/0 : 14[6] -> 13[5] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:196242 [4] NCCL INFO Channel 02/0 : 12[4] -> 11[3] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:196244 [1] NCCL INFO Channel 02/0 : 9[1] -> 8[0] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:393007 [4] NCCL INFO Channel 02/0 : 4[4] -> 3[3] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:393011 [6] NCCL INFO Channel 02/0 : 6[6] -> 5[5] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:393009 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:196242 [4] NCCL INFO Channel 04/0 : 12[4] -> 11[3] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:196244 [1] NCCL INFO Channel 03/0 : 9[1] -> 8[0] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:393007 [4] NCCL INFO Channel 04/0 : 4[4] -> 3[3] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:393011 [6] NCCL INFO Channel 04/0 : 6[6] -> 5[5] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:393009 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:393007 [4] NCCL INFO Channel 06/0 : 4[4] -> 3[3] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:196242 [4] NCCL INFO Channel 06/0 : 12[4] -> 11[3] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:196244 [1] NCCL INFO Channel 05/0 : 9[1] -> 8[0] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:393011 [6] NCCL INFO Channel 05/0 : 6[6] -> 5[5] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:393009 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:393011 [6] NCCL INFO Channel 06/0 : 6[6] -> 5[5] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:196244 [1] NCCL INFO Channel 06/0 : 9[1] -> 8[0] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:393009 [1] NCCL INFO Channel 05/0 : 1[1] -> 0[0] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:196244 [1] NCCL INFO Channel 07/0 : 9[1] -> 8[0] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:393009 [1] NCCL INFO Channel 06/0 : 1[1] -> 0[0] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:393009 [1] NCCL INFO Channel 07/0 : 1[1] -> 0[0] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:196245 [7] NCCL INFO Connected all rings
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:196244 [1] NCCL INFO Connected all rings
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:196239 [3] NCCL INFO Connected all rings
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:393011 [6] NCCL INFO Connected all rings
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:196240 [5] NCCL INFO Connected all rings
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:393015 [5] NCCL INFO Connected all rings
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:196246 [2] NCCL INFO Connected all rings
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:393008 [7] NCCL INFO Connected all rings
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:393014 [3] NCCL INFO Connected all rings
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:196243 [6] NCCL INFO Connected all rings
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:393009 [1] NCCL INFO Connected all rings
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:393011 [6] NCCL INFO Channel 00/0 : 6[6] -> 7[7] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:196242 [4] NCCL INFO Connected all rings
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:196244 [1] NCCL INFO Channel 00/0 : 9[1] -> 10[2] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:196241 [0] NCCL INFO Connected all rings
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:196241 [0] NCCL INFO Channel 00/0 : 8[0] -> 9[1] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:196244 [1] NCCL INFO Channel 02/0 : 9[1] -> 10[2] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:196246 [2] NCCL INFO Channel 00/0 : 10[2] -> 11[3] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:393011 [6] NCCL INFO Channel 01/0 : 6[6] -> 7[7] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:196239 [3] NCCL INFO Channel 00/0 : 11[3] -> 12[4] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:196241 [0] NCCL INFO Channel 01/0 : 8[0] -> 9[1] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:196244 [1] NCCL INFO Channel 03/0 : 9[1] -> 10[2] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:196246 [2] NCCL INFO Channel 01/0 : 10[2] -> 11[3] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:196240 [5] NCCL INFO Channel 00/0 : 13[5] -> 14[6] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:196239 [3] NCCL INFO Channel 01/0 : 11[3] -> 12[4] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:196241 [0] NCCL INFO Channel 02/0 : 8[0] -> 9[1] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:393007 [4] NCCL INFO Connected all rings
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:196244 [1] NCCL INFO Channel 04/0 : 9[1] -> 10[2] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:196246 [2] NCCL INFO Channel 02/0 : 10[2] -> 11[3] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:393011 [6] NCCL INFO Channel 02/0 : 6[6] -> 7[7] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:196240 [5] NCCL INFO Channel 01/0 : 13[5] -> 14[6] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:196239 [3] NCCL INFO Channel 03/0 : 11[3] -> 12[4] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:196241 [0] NCCL INFO Channel 03/0 : 8[0] -> 9[1] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:196244 [1] NCCL INFO Channel 06/0 : 9[1] -> 10[2] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:196243 [6] NCCL INFO Channel 00/0 : 14[6] -> 15[7] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:393010 [2] NCCL INFO Connected all rings
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:196246 [2] NCCL INFO Channel 03/0 : 10[2] -> 11[3] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:196240 [5] NCCL INFO Channel 02/0 : 13[5] -> 14[6] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:196239 [3] NCCL INFO Channel 04/0 : 11[3] -> 12[4] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:196241 [0] NCCL INFO Channel 04/0 : 8[0] -> 9[1] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:196244 [1] NCCL INFO Channel 07/0 : 9[1] -> 10[2] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:196243 [6] NCCL INFO Channel 01/0 : 14[6] -> 15[7] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:196246 [2] NCCL INFO Channel 04/0 : 10[2] -> 11[3] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:393011 [6] NCCL INFO Channel 03/0 : 6[6] -> 7[7] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:196240 [5] NCCL INFO Channel 04/0 : 13[5] -> 14[6] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:196239 [3] NCCL INFO Channel 05/0 : 11[3] -> 12[4] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:196241 [0] NCCL INFO Channel 05/0 : 8[0] -> 9[1] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:196242 [4] NCCL INFO Channel 00/0 : 12[4] -> 13[5] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:196243 [6] NCCL INFO Channel 02/0 : 14[6] -> 15[7] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:196246 [2] NCCL INFO Channel 05/0 : 10[2] -> 11[3] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:196240 [5] NCCL INFO Channel 05/0 : 13[5] -> 14[6] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:196239 [3] NCCL INFO Channel 07/0 : 11[3] -> 12[4] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:196241 [0] NCCL INFO Channel 06/0 : 8[0] -> 9[1] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:196242 [4] NCCL INFO Channel 01/0 : 12[4] -> 13[5] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:196243 [6] NCCL INFO Channel 03/0 : 14[6] -> 15[7] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:393015 [5] NCCL INFO Channel 00/0 : 5[5] -> 6[6] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:196246 [2] NCCL INFO Channel 06/0 : 10[2] -> 11[3] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:196240 [5] NCCL INFO Channel 06/0 : 13[5] -> 14[6] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:393011 [6] NCCL INFO Channel 04/0 : 6[6] -> 7[7] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:196241 [0] NCCL INFO Channel 07/0 : 8[0] -> 9[1] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:196242 [4] NCCL INFO Channel 02/0 : 12[4] -> 13[5] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:196243 [6] NCCL INFO Channel 04/0 : 14[6] -> 15[7] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:196246 [2] NCCL INFO Channel 07/0 : 10[2] -> 11[3] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:196242 [4] NCCL INFO Channel 03/0 : 12[4] -> 13[5] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:196241 [0] NCCL INFO Channel 01/0 : 8[0] -> 15[7] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:196243 [6] NCCL INFO Channel 05/0 : 14[6] -> 15[7] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:196244 [1] NCCL INFO Channel 00/0 : 9[1] -> 8[0] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:196246 [2] NCCL INFO Channel 01/0 : 2[2] -> 10[2] [receive] via NET/IBext/1/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:196246 [2] NCCL INFO Channel 05/0 : 2[2] -> 10[2] [receive] via NET/IBext/1/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:196246 [2] NCCL INFO Channel 01/0 : 10[2] -> 2[2] [send] via NET/IBext/1/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:196246 [2] NCCL INFO Channel 05/0 : 10[2] -> 2[2] [send] via NET/IBext/1/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:393015 [5] NCCL INFO Channel 01/0 : 5[5] -> 6[6] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:196242 [4] NCCL INFO Channel 04/0 : 12[4] -> 13[5] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:196241 [0] NCCL INFO Channel 03/0 : 8[0] -> 15[7] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:196243 [6] NCCL INFO Channel 06/0 : 14[6] -> 15[7] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:196244 [1] NCCL INFO Channel 04/0 : 9[1] -> 8[0] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:196242 [4] NCCL INFO Channel 05/0 : 12[4] -> 13[5] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:196241 [0] NCCL INFO Channel 05/0 : 8[0] -> 15[7] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:196243 [6] NCCL INFO Channel 07/0 : 14[6] -> 15[7] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:393011 [6] NCCL INFO Channel 05/0 : 6[6] -> 7[7] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:196242 [4] NCCL INFO Channel 06/0 : 12[4] -> 13[5] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:196241 [0] NCCL INFO Channel 07/0 : 8[0] -> 15[7] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:196243 [6] NCCL INFO Channel 03/0 : 6[6] -> 14[6] [receive] via NET/IBext/3/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:196243 [6] NCCL INFO Channel 07/0 : 6[6] -> 14[6] [receive] via NET/IBext/3/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:196243 [6] NCCL INFO Channel 03/0 : 14[6] -> 6[6] [send] via NET/IBext/3/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:196243 [6] NCCL INFO Channel 07/0 : 14[6] -> 6[6] [send] via NET/IBext/3/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:196242 [4] NCCL INFO Channel 07/0 : 12[4] -> 13[5] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:393006 [0] NCCL INFO Connected all rings
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:393006 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:196240 [5] NCCL INFO Channel 02/0 : 13[5] -> 12[4] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:393015 [5] NCCL INFO Channel 02/0 : 5[5] -> 6[6] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:196242 [4] NCCL INFO Channel 02/0 : 4[4] -> 12[4] [receive] via NET/IBext/2/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:196242 [4] NCCL INFO Channel 06/0 : 4[4] -> 12[4] [receive] via NET/IBext/2/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:196242 [4] NCCL INFO Channel 02/0 : 12[4] -> 4[4] [send] via NET/IBext/2/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:196242 [4] NCCL INFO Channel 06/0 : 12[4] -> 4[4] [send] via NET/IBext/2/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:393014 [3] NCCL INFO Channel 00/0 : 3[3] -> 4[4] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:393011 [6] NCCL INFO Channel 06/0 : 6[6] -> 7[7] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:196240 [5] NCCL INFO Channel 06/0 : 13[5] -> 12[4] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:196245 [7] NCCL INFO Channel 01/0 : 15[7] -> 8[0] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:196241 [0] NCCL INFO Channel 00/0 : 0[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:196241 [0] NCCL INFO Channel 04/0 : 0[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:393006 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:196241 [0] NCCL INFO Channel 00/0 : 8[0] -> 0[0] [send] via NET/IBext/0/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:196241 [0] NCCL INFO Channel 04/0 : 8[0] -> 0[0] [send] via NET/IBext/0/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:196245 [7] NCCL INFO Channel 02/0 : 15[7] -> 8[0] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:393015 [5] NCCL INFO Channel 04/0 : 5[5] -> 6[6] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:196239 [3] NCCL INFO Channel 01/0 : 11[3] -> 10[2] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:196245 [7] NCCL INFO Channel 03/0 : 15[7] -> 8[0] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:393011 [6] NCCL INFO Channel 07/0 : 6[6] -> 7[7] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:393014 [3] NCCL INFO Channel 01/0 : 3[3] -> 4[4] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:196239 [3] NCCL INFO Channel 05/0 : 11[3] -> 10[2] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:196245 [7] NCCL INFO Channel 05/0 : 15[7] -> 8[0] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:196245 [7] NCCL INFO Channel 06/0 : 15[7] -> 8[0] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:393006 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:196245 [7] NCCL INFO Channel 07/0 : 15[7] -> 8[0] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:393007 [4] NCCL INFO Channel 00/0 : 4[4] -> 5[5] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:393009 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:393015 [5] NCCL INFO Channel 05/0 : 5[5] -> 6[6] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:393014 [3] NCCL INFO Channel 03/0 : 3[3] -> 4[4] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:393010 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:393006 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:393007 [4] NCCL INFO Channel 01/0 : 4[4] -> 5[5] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:393009 [1] NCCL INFO Channel 02/0 : 1[1] -> 2[2] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:393015 [5] NCCL INFO Channel 06/0 : 5[5] -> 6[6] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:393014 [3] NCCL INFO Channel 04/0 : 3[3] -> 4[4] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:393010 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:393006 [0] NCCL INFO Channel 04/0 : 0[0] -> 1[1] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:393007 [4] NCCL INFO Channel 02/0 : 4[4] -> 5[5] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:393009 [1] NCCL INFO Channel 03/0 : 1[1] -> 2[2] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:393011 [6] NCCL INFO Channel 03/0 : 14[6] -> 6[6] [receive] via NET/IBext/3/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:393011 [6] NCCL INFO Channel 07/0 : 14[6] -> 6[6] [receive] via NET/IBext/3/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:393011 [6] NCCL INFO Channel 03/0 : 6[6] -> 14[6] [send] via NET/IBext/3/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:393011 [6] NCCL INFO Channel 07/0 : 6[6] -> 14[6] [send] via NET/IBext/3/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:393014 [3] NCCL INFO Channel 05/0 : 3[3] -> 4[4] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:393010 [2] NCCL INFO Channel 02/0 : 2[2] -> 3[3] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:393006 [0] NCCL INFO Channel 05/0 : 0[0] -> 1[1] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:393007 [4] NCCL INFO Channel 03/0 : 4[4] -> 5[5] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:393009 [1] NCCL INFO Channel 04/0 : 1[1] -> 2[2] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:393014 [3] NCCL INFO Channel 07/0 : 3[3] -> 4[4] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:393010 [2] NCCL INFO Channel 03/0 : 2[2] -> 3[3] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:393006 [0] NCCL INFO Channel 06/0 : 0[0] -> 1[1] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:393007 [4] NCCL INFO Channel 04/0 : 4[4] -> 5[5] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:393009 [1] NCCL INFO Channel 06/0 : 1[1] -> 2[2] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:393010 [2] NCCL INFO Channel 04/0 : 2[2] -> 3[3] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:393006 [0] NCCL INFO Channel 07/0 : 0[0] -> 1[1] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:393007 [4] NCCL INFO Channel 05/0 : 4[4] -> 5[5] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:393009 [1] NCCL INFO Channel 07/0 : 1[1] -> 2[2] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:393010 [2] NCCL INFO Channel 05/0 : 2[2] -> 3[3] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:393007 [4] NCCL INFO Channel 06/0 : 4[4] -> 5[5] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:393006 [0] NCCL INFO Channel 01/0 : 0[0] -> 7[7] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:393010 [2] NCCL INFO Channel 06/0 : 2[2] -> 3[3] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:393007 [4] NCCL INFO Channel 07/0 : 4[4] -> 5[5] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:393006 [0] NCCL INFO Channel 03/0 : 0[0] -> 7[7] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:393010 [2] NCCL INFO Channel 07/0 : 2[2] -> 3[3] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:393015 [5] NCCL INFO Channel 02/0 : 5[5] -> 4[4] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:393006 [0] NCCL INFO Channel 05/0 : 0[0] -> 7[7] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:393007 [4] NCCL INFO Channel 02/0 : 12[4] -> 4[4] [receive] via NET/IBext/2/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:393007 [4] NCCL INFO Channel 06/0 : 12[4] -> 4[4] [receive] via NET/IBext/2/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:393007 [4] NCCL INFO Channel 02/0 : 4[4] -> 12[4] [send] via NET/IBext/2/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:393007 [4] NCCL INFO Channel 06/0 : 4[4] -> 12[4] [send] via NET/IBext/2/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:393009 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:393010 [2] NCCL INFO Channel 01/0 : 10[2] -> 2[2] [receive] via NET/IBext/1/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:393010 [2] NCCL INFO Channel 05/0 : 10[2] -> 2[2] [receive] via NET/IBext/1/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:393010 [2] NCCL INFO Channel 01/0 : 2[2] -> 10[2] [send] via NET/IBext/1/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:393010 [2] NCCL INFO Channel 05/0 : 2[2] -> 10[2] [send] via NET/IBext/1/GDRDMA
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:196242 [4] NCCL INFO Channel 01/0 : 12[4] -> 11[3] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:393006 [0] NCCL INFO Channel 07/0 : 0[0] -> 7[7] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:393015 [5] NCCL INFO Channel 06/0 : 5[5] -> 4[4] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:196242 [4] NCCL INFO Channel 03/0 : 12[4] -> 11[3] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:196242 [4] NCCL INFO Channel 05/0 : 12[4] -> 11[3] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:393009 [1] NCCL INFO Channel 04/0 : 1[1] -> 0[0] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:196242 [4] NCCL INFO Channel 07/0 : 12[4] -> 11[3] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:393008 [7] NCCL INFO Channel 01/0 : 7[7] -> 0[0] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:393006 [0] NCCL INFO Channel 00/0 : 8[0] -> 0[0] [receive] via NET/IBext/0/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:393006 [0] NCCL INFO Channel 04/0 : 8[0] -> 0[0] [receive] via NET/IBext/0/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:393006 [0] NCCL INFO Channel 00/0 : 0[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:393006 [0] NCCL INFO Channel 04/0 : 0[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:393007 [4] NCCL INFO Channel 01/0 : 4[4] -> 3[3] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:393008 [7] NCCL INFO Channel 02/0 : 7[7] -> 0[0] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:393007 [4] NCCL INFO Channel 03/0 : 4[4] -> 3[3] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:393008 [7] NCCL INFO Channel 03/0 : 7[7] -> 0[0] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:393007 [4] NCCL INFO Channel 05/0 : 4[4] -> 3[3] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:393014 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:393008 [7] NCCL INFO Channel 05/0 : 7[7] -> 0[0] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:393007 [4] NCCL INFO Channel 07/0 : 4[4] -> 3[3] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:393014 [3] NCCL INFO Channel 05/0 : 3[3] -> 2[2] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:393008 [7] NCCL INFO Channel 06/0 : 7[7] -> 0[0] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:393008 [7] NCCL INFO Channel 07/0 : 7[7] -> 0[0] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:393008 [7] NCCL INFO Channel 03/0 : 7[7] -> 6[6] via P2P/IPC/read
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:393008 [7] NCCL INFO Channel 07/0 : 7[7] -> 6[6] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:196245 [7] NCCL INFO Channel 03/0 : 15[7] -> 14[6] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:196245 [7] NCCL INFO Channel 07/0 : 15[7] -> 14[6] via P2P/IPC/read
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:196240 [5] NCCL INFO Connected all trees
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:196240 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:196240 [5] NCCL INFO 8 coll channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:393015 [5] NCCL INFO Connected all trees
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:393015 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:393015 [5] NCCL INFO 8 coll channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:196239 [3] NCCL INFO Connected all trees
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:196239 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:196239 [3] NCCL INFO 8 coll channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:393014 [3] NCCL INFO Connected all trees
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:393014 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:393014 [3] NCCL INFO 8 coll channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:393009 [1] NCCL INFO Connected all trees
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:393009 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:393009 [1] NCCL INFO 8 coll channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:196244 [1] NCCL INFO Connected all trees
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:196244 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:196244 [1] NCCL INFO 8 coll channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:393008 [7] NCCL INFO Connected all trees
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:393008 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:393008 [7] NCCL INFO 8 coll channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:196245 [7] NCCL INFO Connected all trees
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:196245 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:196245 [7] NCCL INFO 8 coll channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:196246 [2] NCCL INFO Connected all trees
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:196246 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:196246 [2] NCCL INFO 8 coll channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:196242 [4] NCCL INFO Connected all trees
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:196242 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:196242 [4] NCCL INFO 8 coll channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:393010 [2] NCCL INFO Connected all trees
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:393010 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:393010 [2] NCCL INFO 8 coll channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:393007 [4] NCCL INFO Connected all trees
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:393007 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:393007 [4] NCCL INFO 8 coll channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:196243 [6] NCCL INFO Connected all trees
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:196243 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:196243 [6] NCCL INFO 8 coll channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:196241 [0] NCCL INFO Connected all trees
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:196241 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:196241 [0] NCCL INFO 8 coll channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:393011 [6] NCCL INFO Connected all trees
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:393011 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:393011 [6] NCCL INFO 8 coll channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:393006 [0] NCCL INFO Connected all trees
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:393006 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:393006 [0] NCCL INFO 8 coll channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192643:196240 [5] NCCL INFO comm 0x7f868807c660 rank 13 nranks 16 cudaDev 5 nvmlDev 5 busId b7000 commId 0x638f1c12735ff76d - Init COMPLETE
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192645:196245 [7] NCCL INFO comm 0x7f81e007bc90 rank 15 nranks 16 cudaDev 7 nvmlDev 7 busId d8000 commId 0x638f1c12735ff76d - Init COMPLETE
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192639:196244 [1] NCCL INFO comm 0x7fc0a407df60 rank 9 nranks 16 cudaDev 1 nvmlDev 1 busId 52000 commId 0x638f1c12735ff76d - Init COMPLETE
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192638:196241 [0] NCCL INFO comm 0x7f768407d120 rank 8 nranks 16 cudaDev 0 nvmlDev 0 busId 4d000 commId 0x638f1c12735ff76d - Init COMPLETE
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192642:196242 [4] NCCL INFO comm 0x7f1e88088950 rank 12 nranks 16 cudaDev 4 nvmlDev 4 busId b3000 commId 0x638f1c12735ff76d - Init COMPLETE
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192641:196239 [3] NCCL INFO comm 0x7f34e007c9b0 rank 11 nranks 16 cudaDev 3 nvmlDev 3 busId 6f000 commId 0x638f1c12735ff76d - Init COMPLETE
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192640:196246 [2] NCCL INFO comm 0x7fd50c07b9f0 rank 10 nranks 16 cudaDev 2 nvmlDev 2 busId 69000 commId 0x638f1c12735ff76d - Init COMPLETE
worker1: t-20240620111711-c5m49-ray-job-worker-worker1-5vq4b:192644:196243 [6] NCCL INFO comm 0x7fdca007e8d0 rank 14 nranks 16 cudaDev 6 nvmlDev 6 busId d5000 commId 0x638f1c12735ff76d - Init COMPLETE
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388580:393008 [7] NCCL INFO comm 0x7f102c089610 rank 7 nranks 16 cudaDev 7 nvmlDev 7 busId d8000 commId 0x638f1c12735ff76d - Init COMPLETE
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388578:393015 [5] NCCL INFO comm 0x7fbba4077740 rank 5 nranks 16 cudaDev 5 nvmlDev 5 busId b7000 commId 0x638f1c12735ff76d - Init COMPLETE
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388576:393014 [3] NCCL INFO comm 0x7f6280077680 rank 3 nranks 16 cudaDev 3 nvmlDev 3 busId 6f000 commId 0x638f1c12735ff76d - Init COMPLETE
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388574:393009 [1] NCCL INFO comm 0x7fe26407b330 rank 1 nranks 16 cudaDev 1 nvmlDev 1 busId 52000 commId 0x638f1c12735ff76d - Init COMPLETE
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388579:393011 [6] NCCL INFO comm 0x7efcc807baf0 rank 6 nranks 16 cudaDev 6 nvmlDev 6 busId d5000 commId 0x638f1c12735ff76d - Init COMPLETE
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388577:393007 [4] NCCL INFO comm 0x7fb978089b80 rank 4 nranks 16 cudaDev 4 nvmlDev 4 busId b3000 commId 0x638f1c12735ff76d - Init COMPLETE
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388575:393010 [2] NCCL INFO comm 0x7f92d807ca10 rank 2 nranks 16 cudaDev 2 nvmlDev 2 busId 69000 commId 0x638f1c12735ff76d - Init COMPLETE
head: t-20240620111711-c5m49-ray-job-head-zkrsj:388573:393006 [0] NCCL INFO comm 0x7f5068084380 rank 0 nranks 16 cudaDev 0 nvmlDev 0 busId 4d000 commId 0x638f1c12735ff76d - Init COMPLETE
head: Invalidate trace cache @ step 454: expected module 1, but got module 453
head: Invalidate trace cache @ step 452: expected module 908, but got module 907
head: [2024-06-20 10:36:37,068] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1
head: [2024-06-20 10:36:47,732] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1
head: ========== [step:3] start ==========
head: Epoch: 0 | Step: 3 | PPO Epoch: 1 | Actor Loss: -0.0015932227834127843 | Critic Loss: 0.007982767536304891 | Unsupervised Loss: 0.0
head: End-to-End => Latency: 231.59s, TFLOPs: 36.05, Samples/sec: 2.21, Time/seq 0.45s, Batch Size: 512, Total Seq. Length: 2048
head: Generation => Latency: 34.02s, Per-token Latency 33.23 ms, TFLOPs: 6.82, BW: -1.00 GB/sec, Answer Seq. Length: 1024
head: Training   => Latency: 95.50s, TFLOPs: 77.71
head: Actor Model Parameters => 6.738 B, Critic Model Parameters => 6.607 B
head: Average reward score: -0.454833984375 | EMA reward score: -0.454833984375
head: -------------------------------------------------------------------------------------
head: |E2E latency=142.83s |Gather latency=0.46s (0.32%) |Generate time=33.56s (23.50%) |Training time=96.43s (67.51%) |Others=12.84 (8.99%)|CurSamplesPerSec=0.90 |AvgSamplesPerSec=1.79
head: |E2E latency=48.72s |Gather latency=0.52s (1.07%) |Generate time=33.40s (68.56%) |Training time=0.00s (0.00%) |Others=15.32 (31.44%)|CurSamplesPerSec=2.63 |AvgSamplesPerSec=1.91
head: Invalidate trace cache @ step 454: expected module 453, but got module 1
head: Invalidate trace cache @ step 452: expected module 907, but got module 908
head: |E2E latency=48.41s |Gather latency=0.59s (1.21%) |Generate time=33.51s (69.22%) |Training time=0.00s (0.00%) |Others=14.90 (30.78%)|CurSamplesPerSec=2.64 |AvgSamplesPerSec=2.00
head: |E2E latency=49.09s |Gather latency=0.50s (1.02%) |Generate time=33.52s (68.28%) |Training time=0.00s (0.00%) |Others=15.57 (31.72%)|CurSamplesPerSec=2.61 |AvgSamplesPerSec=2.07
head: [2024-06-20 10:41:20,582] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
head: [2024-06-20 10:41:31,057] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
head: [step:3][stage:generate-experience][dt:135.98]
head: [step:3][stage:actor-inference][dt:5.34]
head: [step:3][stage:ref-inference][dt:18.93]
head: [step:3][stage:reward-inference][dt:12.46]
head: [step:3][stage:critic-inference][dt:15.31]
head: [step:3][stage:total-generate][dt:194.81]
head: [step:3][stage:actor-train][dt:42.71]
head: [step:3][stage:critic-train][dt:44.76]
head: [step:3][stage:total-train][dt:87.79]
head: ========== [step:3] end, E2E [dt:283.32] ==========
head: ========== [step:7] start ==========
head: Epoch: 0 | Step: 7 | PPO Epoch: 1 | Actor Loss: 0.0054687627125531435 | Critic Loss: 0.00943284248933196 | Unsupervised Loss: 0.0
head: End-to-End => Latency: 223.54s, TFLOPs: 37.35, Samples/sec: 2.29, Time/seq 0.44s, Batch Size: 512, Total Seq. Length: 2048
head: Generation => Latency: 33.94s, Per-token Latency 33.14 ms, TFLOPs: 6.83, BW: -1.00 GB/sec, Answer Seq. Length: 1024
head: Training   => Latency: 87.79s, TFLOPs: 84.54
head: Actor Model Parameters => 6.738 B, Critic Model Parameters => 6.607 B
head: Average reward score: -0.465087890625 | EMA reward score: -0.455859375
head: -------------------------------------------------------------------------------------
head: |E2E latency=137.10s |Gather latency=0.47s (0.34%) |Generate time=33.47s (24.41%) |Training time=90.95s (66.34%) |Others=12.68 (9.25%)|CurSamplesPerSec=0.93 |AvgSamplesPerSec=1.80
head: |E2E latency=48.05s |Gather latency=0.50s (1.03%) |Generate time=33.42s (69.55%) |Training time=0.00s (0.00%) |Others=14.63 (30.45%)|CurSamplesPerSec=2.66 |AvgSamplesPerSec=1.86
head: Invalidate trace cache @ step 454: expected module 453, but got module 1
head: Invalidate trace cache @ step 452: expected module 907, but got module 908
head: |E2E latency=48.81s |Gather latency=0.51s (1.05%) |Generate time=33.40s (68.42%) |Training time=0.00s (0.00%) |Others=15.42 (31.58%)|CurSamplesPerSec=2.62 |AvgSamplesPerSec=1.92
head: |E2E latency=47.73s |Gather latency=0.50s (1.05%) |Generate time=33.48s (70.14%) |Training time=0.00s (0.00%) |Others=14.26 (29.86%)|CurSamplesPerSec=2.68 |AvgSamplesPerSec=1.97
head: [2024-06-20 10:45:59,984] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384
head: [2024-06-20 10:46:10,631] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384
head: [step:7][stage:generate-experience][dt:135.72]
head: [step:7][stage:actor-inference][dt:5.45]
head: [step:7][stage:ref-inference][dt:17.17]
head: [step:7][stage:reward-inference][dt:12.5]
head: [step:7][stage:critic-inference][dt:14.77]
head: [step:7][stage:total-generate][dt:192.75]
head: [step:7][stage:actor-train][dt:45.09]
head: [step:7][stage:critic-train][dt:40.6]
head: [step:7][stage:total-train][dt:86.25]
head: ========== [step:7] end, E2E [dt:279.57] ==========
head: ========== [step:11] start ==========
head: Epoch: 0 | Step: 11 | PPO Epoch: 1 | Actor Loss: 0.0033912922954186797 | Critic Loss: 0.008829745696857572 | Unsupervised Loss: 0.0
head: End-to-End => Latency: 221.93s, TFLOPs: 37.62, Samples/sec: 2.31, Time/seq 0.43s, Batch Size: 512, Total Seq. Length: 2048
head: Generation => Latency: 33.92s, Per-token Latency 33.13 ms, TFLOPs: 6.84, BW: -1.00 GB/sec, Answer Seq. Length: 1024
head: Training   => Latency: 86.25s, TFLOPs: 86.05
head: Actor Model Parameters => 6.738 B, Critic Model Parameters => 6.607 B
head: Average reward score: -0.4833984375 | EMA reward score: -0.45861328125
head: -------------------------------------------------------------------------------------
head: |E2E latency=134.97s |Gather latency=0.47s (0.35%) |Generate time=33.45s (24.78%) |Training time=88.53s (65.59%) |Others=13.00 (9.63%)|CurSamplesPerSec=0.95 |AvgSamplesPerSec=1.81
head: |E2E latency=48.45s |Gather latency=0.54s (1.12%) |Generate time=33.43s (69.00%) |Training time=0.00s (0.00%) |Others=15.02 (31.00%)|CurSamplesPerSec=2.64 |AvgSamplesPerSec=1.85
head: Invalidate trace cache @ step 454: expected module 453, but got module 1
head: Invalidate trace cache @ step 452: expected module 907, but got module 908
head: |E2E latency=48.66s |Gather latency=0.54s (1.11%) |Generate time=33.48s (68.81%) |Training time=0.00s (0.00%) |Others=15.17 (31.19%)|CurSamplesPerSec=2.63 |AvgSamplesPerSec=1.89
head: |E2E latency=49.45s |Gather latency=0.50s (1.01%) |Generate time=33.42s (67.58%) |Training time=0.00s (0.00%) |Others=16.03 (32.42%)|CurSamplesPerSec=2.59 |AvgSamplesPerSec=1.93
head: [2024-06-20 10:50:47,293] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192
head: [2024-06-20 10:50:58,681] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192
head: [step:11][stage:generate-experience][dt:135.81]
head: [step:11][stage:actor-inference][dt:5.39]
head: [step:11][stage:ref-inference][dt:19.38]
head: [step:11][stage:reward-inference][dt:12.27]
head: [step:11][stage:critic-inference][dt:15.61]
head: [step:11][stage:total-generate][dt:195.43]
head: [step:11][stage:actor-train][dt:46.54]
head: [step:11][stage:critic-train][dt:45.02]
head: [step:11][stage:total-train][dt:91.97]
head: ========== [step:11] end, E2E [dt:288.05] ==========
head: ========== [step:15] start ==========
head: Epoch: 0 | Step: 15 | PPO Epoch: 1 | Actor Loss: 0.003395683132112026 | Critic Loss: 0.009314798982813954 | Unsupervised Loss: 0.0
head: End-to-End => Latency: 227.54s, TFLOPs: 36.69, Samples/sec: 2.25, Time/seq 0.44s, Batch Size: 512, Total Seq. Length: 2048
head: Generation => Latency: 33.89s, Per-token Latency 33.10 ms, TFLOPs: 6.84, BW: -1.00 GB/sec, Answer Seq. Length: 1024
head: Training   => Latency: 91.97s, TFLOPs: 80.69
head: Actor Model Parameters => 6.738 B, Critic Model Parameters => 6.607 B
head: Average reward score: -0.44287109375 | EMA reward score: -0.4570390625
head: -------------------------------------------------------------------------------------
head: |E2E latency=141.49s |Gather latency=0.48s (0.34%) |Generate time=33.41s (23.61%) |Training time=94.31s (66.65%) |Others=13.78 (9.74%)|CurSamplesPerSec=0.90 |AvgSamplesPerSec=1.80
head: |E2E latency=49.19s |Gather latency=0.47s (0.95%) |Generate time=33.42s (67.94%) |Training time=0.00s (0.00%) |Others=15.77 (32.06%)|CurSamplesPerSec=2.60 |AvgSamplesPerSec=1.83
head: Invalidate trace cache @ step 454: expected module 453, but got module 1
head: Invalidate trace cache @ step 452: expected module 907, but got module 908
head: |E2E latency=48.31s |Gather latency=0.48s (1.00%) |Generate time=33.49s (69.31%) |Training time=0.00s (0.00%) |Others=14.83 (30.69%)|CurSamplesPerSec=2.65 |AvgSamplesPerSec=1.87
head: |E2E latency=48.57s |Gather latency=0.51s (1.05%) |Generate time=33.43s (68.83%) |Training time=0.00s (0.00%) |Others=15.14 (31.17%)|CurSamplesPerSec=2.64 |AvgSamplesPerSec=1.89
head: [2024-06-20 10:55:34,733] [WARNING] [stage3.py:1991:step] 101 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
head: [2024-06-20 10:55:45,172] [WARNING] [stage3.py:1991:step] 102 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
head: [step:15][stage:generate-experience][dt:135.72]
head: [step:15][stage:actor-inference][dt:6.9]
head: [step:15][stage:ref-inference][dt:16.28]
head: [step:15][stage:reward-inference][dt:13.05]
head: [step:15][stage:critic-inference][dt:15.63]
head: [step:15][stage:total-generate][dt:194.82]
head: [step:15][stage:actor-train][dt:45.68]
head: [step:15][stage:critic-train][dt:44.65]
head: [step:15][stage:total-train][dt:90.78]
head: ========== [step:15] end, E2E [dt:286.49] ==========
head: ========== [step:19] start ==========
head: Epoch: 0 | Step: 19 | PPO Epoch: 1 | Actor Loss: -0.0026010568253695965 | Critic Loss: 0.007887827581726015 | Unsupervised Loss: 0.0
head: End-to-End => Latency: 226.48s, TFLOPs: 36.86, Samples/sec: 2.26, Time/seq 0.44s, Batch Size: 512, Total Seq. Length: 2048
head: Generation => Latency: 33.93s, Per-token Latency 33.13 ms, TFLOPs: 6.84, BW: -1.00 GB/sec, Answer Seq. Length: 1024
head: Training   => Latency: 90.78s, TFLOPs: 81.75
head: Actor Model Parameters => 6.738 B, Critic Model Parameters => 6.607 B
head: Average reward score: -0.46044921875 | EMA reward score: -0.457380078125
head: -------------------------------------------------------------------------------------
head: |E2E latency=140.42s |Gather latency=0.46s (0.33%) |Generate time=33.46s (23.83%) |Training time=94.16s (67.06%) |Others=12.80 (9.11%)|CurSamplesPerSec=0.91 |AvgSamplesPerSec=1.80
head: |E2E latency=49.14s |Gather latency=0.48s (0.98%) |Generate time=33.40s (67.97%) |Training time=0.00s (0.00%) |Others=15.74 (32.03%)|CurSamplesPerSec=2.60 |AvgSamplesPerSec=1.82
head: Invalidate trace cache @ step 454: expected module 453, but got module 1
head: Invalidate trace cache @ step 452: expected module 907, but got module 908
head: |E2E latency=48.83s |Gather latency=0.49s (1.00%) |Generate time=33.54s (68.69%) |Training time=0.00s (0.00%) |Others=15.29 (31.31%)|CurSamplesPerSec=2.62 |AvgSamplesPerSec=1.85
head: |E2E latency=47.14s |Gather latency=0.54s (1.14%) |Generate time=33.48s (71.02%) |Training time=0.00s (0.00%) |Others=13.66 (28.98%)|CurSamplesPerSec=2.72 |AvgSamplesPerSec=1.88
head: [2024-06-20 11:00:19,516] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096
head: [2024-06-20 11:00:30,400] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096
head: [step:19][stage:generate-experience][dt:136.01]
head: [step:19][stage:actor-inference][dt:5.45]
head: [step:19][stage:ref-inference][dt:19.75]
head: [step:19][stage:reward-inference][dt:11.57]
head: [step:19][stage:critic-inference][dt:15.15]
head: [step:19][stage:total-generate][dt:195.22]
head: [step:19][stage:actor-train][dt:45.46]
head: [step:19][stage:critic-train][dt:43.39]
head: [step:19][stage:total-train][dt:89.2]
head: ========== [step:19] end, E2E [dt:285.23] ==========
head: ========== [step:23] start ==========
head: Epoch: 0 | Step: 23 | PPO Epoch: 1 | Actor Loss: -3.760681629180908 | Critic Loss: 34.99307155609131 | Unsupervised Loss: 0.0
head: End-to-End => Latency: 225.56s, TFLOPs: 37.01, Samples/sec: 2.27, Time/seq 0.44s, Batch Size: 512, Total Seq. Length: 2048
head: Generation => Latency: 34.09s, Per-token Latency 33.29 ms, TFLOPs: 6.80, BW: -1.00 GB/sec, Answer Seq. Length: 1024
head: Training   => Latency: 89.20s, TFLOPs: 83.20
head: Actor Model Parameters => 6.738 B, Critic Model Parameters => 6.607 B
head: Average reward score: -0.463623046875 | EMA reward score: -0.45800437499999996
head: -------------------------------------------------------------------------------------
head: |E2E latency=140.12s |Gather latency=0.50s (0.36%) |Generate time=33.59s (23.97%) |Training time=93.37s (66.63%) |Others=13.17 (9.40%)|CurSamplesPerSec=0.91 |AvgSamplesPerSec=1.80
head: |E2E latency=48.51s |Gather latency=0.53s (1.10%) |Generate time=33.42s (68.90%) |Training time=0.00s (0.00%) |Others=15.09 (31.10%)|CurSamplesPerSec=2.64 |AvgSamplesPerSec=1.82
head: Invalidate trace cache @ step 454: expected module 453, but got module 1
head: Invalidate trace cache @ step 452: expected module 907, but got module 908
head: |E2E latency=48.78s |Gather latency=0.65s (1.33%) |Generate time=33.37s (68.42%) |Training time=0.00s (0.00%) |Others=15.41 (31.58%)|CurSamplesPerSec=2.62 |AvgSamplesPerSec=1.84
head: |E2E latency=48.28s |Gather latency=0.47s (0.97%) |Generate time=33.41s (69.20%) |Training time=0.00s (0.00%) |Others=14.87 (30.80%)|CurSamplesPerSec=2.65 |AvgSamplesPerSec=1.86
head: [2024-06-20 11:05:04,516] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096, reducing to 2048
head: [2024-06-20 11:05:14,640] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096, reducing to 2048
head: [step:23][stage:generate-experience][dt:135.74]
head: [step:23][stage:actor-inference][dt:5.44]
head: [step:23][stage:ref-inference][dt:18.09]
head: [step:23][stage:reward-inference][dt:12.2]
head: [step:23][stage:critic-inference][dt:15.05]
head: [step:23][stage:total-generate][dt:193.3]
head: [step:23][stage:actor-train][dt:47.77]
head: [step:23][stage:critic-train][dt:41.94]
head: [step:23][stage:total-train][dt:90.28]
head: ========== [step:23] end, E2E [dt:284.24] ==========
head: ========== [step:27] start ==========
head: Epoch: 0 | Step: 27 | PPO Epoch: 1 | Actor Loss: -5.270769238471985 | Critic Loss: 41.6575403213501 | Unsupervised Loss: 0.0
head: End-to-End => Latency: 225.87s, TFLOPs: 36.96, Samples/sec: 2.27, Time/seq 0.44s, Batch Size: 512, Total Seq. Length: 2048
head: Generation => Latency: 33.90s, Per-token Latency 33.10 ms, TFLOPs: 6.84, BW: -1.00 GB/sec, Answer Seq. Length: 1024
head: Training   => Latency: 90.28s, TFLOPs: 82.20
head: Actor Model Parameters => 6.738 B, Critic Model Parameters => 6.607 B
head: Average reward score: -0.5048828125 | EMA reward score: -0.46269221874999994
head: -------------------------------------------------------------------------------------
head: |E2E latency=138.68s |Gather latency=0.50s (0.36%) |Generate time=33.40s (24.08%) |Training time=92.88s (66.97%) |Others=12.41 (8.95%)|CurSamplesPerSec=0.92 |AvgSamplesPerSec=1.80
head: |E2E latency=48.45s |Gather latency=0.60s (1.24%) |Generate time=33.43s (68.99%) |Training time=0.00s (0.00%) |Others=15.02 (31.01%)|CurSamplesPerSec=2.64 |AvgSamplesPerSec=1.82
head: Invalidate trace cache @ step 454: expected module 453, but got module 1
head: Invalidate trace cache @ step 452: expected module 907, but got module 908
head: |E2E latency=50.04s |Gather latency=0.49s (0.98%) |Generate time=33.37s (66.69%) |Training time=0.00s (0.00%) |Others=16.67 (33.31%)|CurSamplesPerSec=2.56 |AvgSamplesPerSec=1.84
head: |E2E latency=49.72s |Gather latency=0.51s (1.02%) |Generate time=33.43s (67.23%) |Training time=0.00s (0.00%) |Others=16.30 (32.77%)|CurSamplesPerSec=2.57 |AvgSamplesPerSec=1.85
head: [2024-06-20 11:09:51,577] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2048, reducing to 1024
head: [2024-06-20 11:10:02,932] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2048, reducing to 1024
head: [step:27][stage:generate-experience][dt:135.73]
head: [step:27][stage:actor-inference][dt:5.31]
head: [step:27][stage:ref-inference][dt:20.56]
head: [step:27][stage:reward-inference][dt:11.86]
head: [step:27][stage:critic-inference][dt:16.03]
head: [step:27][stage:total-generate][dt:196.5]
head: [step:27][stage:actor-train][dt:46.78]
head: [step:27][stage:critic-train][dt:43.98]
head: [step:27][stage:total-train][dt:91.21]
head: ========== [step:27] end, E2E [dt:288.29] ==========
head: ========== [step:31] start ==========
head: Epoch: 0 | Step: 31 | PPO Epoch: 1 | Actor Loss: -3.9078890308737755 | Critic Loss: 35.50621032714844 | Unsupervised Loss: 0.0
head: End-to-End => Latency: 226.85s, TFLOPs: 36.80, Samples/sec: 2.26, Time/seq 0.44s, Batch Size: 512, Total Seq. Length: 2048
head: Generation => Latency: 33.91s, Per-token Latency 33.12 ms, TFLOPs: 6.84, BW: -1.00 GB/sec, Answer Seq. Length: 1024
head: Training   => Latency: 91.21s, TFLOPs: 81.37
head: Actor Model Parameters => 6.738 B, Critic Model Parameters => 6.607 B
head: Average reward score: -0.483154296875 | EMA reward score: -0.4647384265624999
head: -------------------------------------------------------------------------------------
head: |E2E latency=140.08s |Gather latency=0.52s (0.37%) |Generate time=33.39s (23.84%) |Training time=93.05s (66.42%) |Others=13.64 (9.74%)|CurSamplesPerSec=0.91 |AvgSamplesPerSec=1.79
head: |E2E latency=48.72s |Gather latency=0.51s (1.04%) |Generate time=33.42s (68.60%) |Training time=0.00s (0.00%) |Others=15.30 (31.40%)|CurSamplesPerSec=2.63 |AvgSamplesPerSec=1.81
head: Invalidate trace cache @ step 454: expected module 453, but got module 1
head: Invalidate trace cache @ step 452: expected module 907, but got module 908
head: |E2E latency=50.01s |Gather latency=0.52s (1.04%) |Generate time=33.46s (66.89%) |Training time=0.00s (0.00%) |Others=16.56 (33.11%)|CurSamplesPerSec=2.56 |AvgSamplesPerSec=1.83
head: |E2E latency=49.78s |Gather latency=0.53s (1.06%) |Generate time=33.42s (67.14%) |Training time=0.00s (0.00%) |Others=16.36 (32.86%)|CurSamplesPerSec=2.57 |AvgSamplesPerSec=1.84
head: [2024-06-20 11:14:40,672] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1024, reducing to 512
head: [2024-06-20 11:14:50,540] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1024, reducing to 512
head: [step:31][stage:generate-experience][dt:135.74]
head: [step:31][stage:actor-inference][dt:5.44]
head: [step:31][stage:ref-inference][dt:20.12]
head: [step:31][stage:reward-inference][dt:12.26]
head: [step:31][stage:critic-inference][dt:15.72]
head: [step:31][stage:total-generate][dt:196.32]
head: [step:31][stage:actor-train][dt:46.99]
head: [step:31][stage:critic-train][dt:43.05]
head: [step:31][stage:total-train][dt:90.6]
head: ========== [step:31] end, E2E [dt:287.61] ==========
head: ========== [step:35] start ==========
head: Epoch: 0 | Step: 35 | PPO Epoch: 1 | Actor Loss: -4.819996237754822 | Critic Loss: 39.27101707458496 | Unsupervised Loss: 0.0
head: End-to-End => Latency: 226.15s, TFLOPs: 36.92, Samples/sec: 2.26, Time/seq 0.44s, Batch Size: 512, Total Seq. Length: 2048
head: Generation => Latency: 33.89s, Per-token Latency 33.09 ms, TFLOPs: 6.84, BW: -1.00 GB/sec, Answer Seq. Length: 1024
head: Training   => Latency: 90.61s, TFLOPs: 81.91
head: Actor Model Parameters => 6.738 B, Critic Model Parameters => 6.607 B
head: Average reward score: -0.4833984375 | EMA reward score: -0.46660442765624993
head: -------------------------------------------------------------------------------------
head: |E2E latency=139.10s |Gather latency=0.47s (0.34%) |Generate time=33.41s (24.02%) |Training time=93.65s (67.33%) |Others=12.03 (8.65%)|CurSamplesPerSec=0.92 |AvgSamplesPerSec=1.79
head: |E2E latency=48.65s |Gather latency=0.46s (0.95%) |Generate time=33.38s (68.63%) |Training time=0.00s (0.00%) |Others=15.26 (31.37%)|CurSamplesPerSec=2.63 |AvgSamplesPerSec=1.81
head: Invalidate trace cache @ step 454: expected module 453, but got module 1
head: Invalidate trace cache @ step 452: expected module 907, but got module 908
head: |E2E latency=49.80s |Gather latency=0.50s (1.00%) |Generate time=33.44s (67.15%) |Training time=0.00s (0.00%) |Others=16.36 (32.85%)|CurSamplesPerSec=2.57 |AvgSamplesPerSec=1.82
head: |E2E latency=49.26s |Gather latency=0.50s (1.01%) |Generate time=33.43s (67.87%) |Training time=0.00s (0.00%) |Others=15.83 (32.13%)|CurSamplesPerSec=2.60 |AvgSamplesPerSec=1.84
head: [2024-06-20 11:19:26,511] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 512, reducing to 256
head: [2024-06-20 11:19:26,512] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=9, lr=[9.646854560255193e-06, 9.646854560255193e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
head: [2024-06-20 11:19:26,512] [INFO] [timer.py:260:stop] epoch=0/micro_step=40/global_step=10, RunningAvgSamplesPerSec=10.976188537834002, CurrSamplesPerSec=10.46917183237901, MemAllocated=18.59GB, MaxMemAllocated=45.55GB
head: [2024-06-20 11:19:37,923] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 512, reducing to 256
head: [2024-06-20 11:19:37,924] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=9, lr=[4.998370238474193e-06, 4.998370238474193e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
head: [step:35][stage:generate-experience][dt:135.58]
head: [step:35][stage:actor-inference][dt:5.47]
head: [step:35][stage:ref-inference][dt:17.88]
head: [step:35][stage:reward-inference][dt:12.5]
head: [step:35][stage:critic-inference][dt:15.66]
head: [step:35][stage:total-generate][dt:194.37]
head: [step:35][stage:actor-train][dt:48.91]
head: [step:35][stage:critic-train][dt:42.87]
head: [step:35][stage:total-train][dt:92.14]
head: ========== [step:35] end, E2E [dt:287.38] ==========
head: ========== [step:39] start ==========
head: Epoch: 0 | Step: 39 | PPO Epoch: 1 | Actor Loss: -1.7220012359321117 | Critic Loss: 29.44951343536377 | Unsupervised Loss: 0.0
head: End-to-End => Latency: 227.62s, TFLOPs: 36.68, Samples/sec: 2.25, Time/seq 0.44s, Batch Size: 512, Total Seq. Length: 2048
head: Generation => Latency: 33.87s, Per-token Latency 33.08 ms, TFLOPs: 6.85, BW: -1.00 GB/sec, Answer Seq. Length: 1024
head: Training   => Latency: 92.14s, TFLOPs: 80.55
head: Actor Model Parameters => 6.738 B, Critic Model Parameters => 6.607 B
head: Average reward score: -0.46044921875 | EMA reward score: -0.46598890676562493
head: -------------------------------------------------------------------------------------
head: |E2E latency=139.68s |Gather latency=0.48s (0.34%) |Generate time=33.39s (23.90%) |Training time=92.59s (66.29%) |Others=13.70 (9.81%)|CurSamplesPerSec=0.92 |AvgSamplesPerSec=1.79
head: |E2E latency=48.08s |Gather latency=0.48s (1.00%) |Generate time=33.38s (69.42%) |Training time=0.00s (0.00%) |Others=14.70 (30.58%)|CurSamplesPerSec=2.66 |AvgSamplesPerSec=1.81
head: Invalidate trace cache @ step 454: expected module 453, but got module 1
head: Invalidate trace cache @ step 452: expected module 907, but got module 908
head: |E2E latency=50.98s |Gather latency=0.53s (1.05%) |Generate time=33.64s (65.99%) |Training time=0.00s (0.00%) |Others=17.34 (34.01%)|CurSamplesPerSec=2.51 |AvgSamplesPerSec=1.82
head: |E2E latency=48.25s |Gather latency=0.52s (1.07%) |Generate time=33.42s (69.26%) |Training time=0.00s (0.00%) |Others=14.83 (30.74%)|CurSamplesPerSec=2.65 |AvgSamplesPerSec=1.83
head: [2024-06-20 11:24:14,287] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 256, reducing to 128
head: [2024-06-20 11:24:25,984] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 256, reducing to 128
head: [step:39][stage:generate-experience][dt:135.87]
head: [step:39][stage:actor-inference][dt:5.49]
head: [step:39][stage:ref-inference][dt:19.4]
head: [step:39][stage:reward-inference][dt:11.44]
head: [step:39][stage:critic-inference][dt:15.73]
head: [step:39][stage:total-generate][dt:194.88]
head: [step:39][stage:actor-train][dt:47.29]
head: [step:39][stage:critic-train][dt:45.06]
head: [step:39][stage:total-train][dt:92.77]
head: ========== [step:39] end, E2E [dt:288.06] ==========
head: ========== [step:43] start ==========
head: Epoch: 0 | Step: 43 | PPO Epoch: 1 | Actor Loss: -4.009018272161484 | Critic Loss: 35.56053352355957 | Unsupervised Loss: 0.0
head: End-to-End => Latency: 228.37s, TFLOPs: 36.56, Samples/sec: 2.24, Time/seq 0.45s, Batch Size: 512, Total Seq. Length: 2048
head: Generation => Latency: 33.90s, Per-token Latency 33.10 ms, TFLOPs: 6.84, BW: -1.00 GB/sec, Answer Seq. Length: 1024
head: Training   => Latency: 92.78s, TFLOPs: 79.99
head: Actor Model Parameters => 6.738 B, Critic Model Parameters => 6.607 B
head: Average reward score: -0.4638671875 | EMA reward score: -0.46577673483906246
head: -------------------------------------------------------------------------------------
head: |E2E latency=140.75s |Gather latency=0.48s (0.34%) |Generate time=33.42s (23.74%) |Training time=93.31s (66.30%) |Others=14.02 (9.96%)|CurSamplesPerSec=0.91 |AvgSamplesPerSec=1.79
head: |E2E latency=47.63s |Gather latency=0.53s (1.10%) |Generate time=33.45s (70.24%) |Training time=0.00s (0.00%) |Others=14.17 (29.76%)|CurSamplesPerSec=2.69 |AvgSamplesPerSec=1.80
head: Invalidate trace cache @ step 454: expected module 453, but got module 1
head: Invalidate trace cache @ step 452: expected module 907, but got module 908
head: |E2E latency=50.38s |Gather latency=0.48s (0.96%) |Generate time=33.64s (66.78%) |Training time=0.00s (0.00%) |Others=16.74 (33.22%)|CurSamplesPerSec=2.54 |AvgSamplesPerSec=1.82
head: |E2E latency=48.83s |Gather latency=0.54s (1.10%) |Generate time=33.43s (68.45%) |Training time=0.00s (0.00%) |Others=15.41 (31.55%)|CurSamplesPerSec=2.62 |AvgSamplesPerSec=1.83
head: [2024-06-20 11:28:59,488] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 128, reducing to 64
head: [2024-06-20 11:29:10,739] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 128, reducing to 64
head: [step:43][stage:generate-experience][dt:136.04]
head: [step:43][stage:actor-inference][dt:5.4]
head: [step:43][stage:ref-inference][dt:19.51]
head: [step:43][stage:reward-inference][dt:12.32]
head: [step:43][stage:critic-inference][dt:15.51]
head: [step:43][stage:total-generate][dt:195.79]
head: [step:43][stage:actor-train][dt:45.43]
head: [step:43][stage:critic-train][dt:42.66]
head: [step:43][stage:total-train][dt:88.59]
head: ========== [step:43] end, E2E [dt:284.76] ==========
head: ========== [step:47] start ==========
head: Epoch: 0 | Step: 47 | PPO Epoch: 1 | Actor Loss: -4.599581956863403 | Critic Loss: 39.759366035461426 | Unsupervised Loss: 0.0
head: End-to-End => Latency: 224.50s, TFLOPs: 37.19, Samples/sec: 2.28, Time/seq 0.44s, Batch Size: 512, Total Seq. Length: 2048
head: Generation => Latency: 33.98s, Per-token Latency 33.18 ms, TFLOPs: 6.83, BW: -1.00 GB/sec, Answer Seq. Length: 1024
head: Training   => Latency: 88.59s, TFLOPs: 83.77
head: Actor Model Parameters => 6.738 B, Critic Model Parameters => 6.607 B
head: Average reward score: -0.49267578125 | EMA reward score: -0.4684666394801562
head: -------------------------------------------------------------------------------------
head: |E2E latency=137.91s |Gather latency=0.52s (0.38%) |Generate time=33.45s (24.26%) |Training time=90.88s (65.90%) |Others=13.58 (9.85%)|CurSamplesPerSec=0.93 |AvgSamplesPerSec=1.79
head: |E2E latency=48.25s |Gather latency=0.49s (1.02%) |Generate time=33.44s (69.31%) |Training time=0.00s (0.00%) |Others=14.81 (30.69%)|CurSamplesPerSec=2.65 |AvgSamplesPerSec=1.80
head: Invalidate trace cache @ step 454: expected module 453, but got module 1
head: Invalidate trace cache @ step 452: expected module 907, but got module 908
head: |E2E latency=49.08s |Gather latency=0.49s (1.00%) |Generate time=33.36s (67.96%) |Training time=0.00s (0.00%) |Others=15.72 (32.04%)|CurSamplesPerSec=2.61 |AvgSamplesPerSec=1.81
head: |E2E latency=48.12s |Gather latency=0.52s (1.08%) |Generate time=33.42s (69.46%) |Training time=0.00s (0.00%) |Others=14.70 (30.54%)|CurSamplesPerSec=2.66 |AvgSamplesPerSec=1.83
head: [2024-06-20 11:33:46,571] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 64, reducing to 32
head: [2024-06-20 11:33:57,425] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 64, reducing to 32
head: [step:47][stage:generate-experience][dt:135.67]
head: [step:47][stage:actor-inference][dt:5.45]
head: [step:47][stage:ref-inference][dt:19.38]
head: [step:47][stage:reward-inference][dt:11.52]
head: [step:47][stage:critic-inference][dt:15.28]
head: [step:47][stage:total-generate][dt:194.61]
head: [step:47][stage:actor-train][dt:45.28]
head: [step:47][stage:critic-train][dt:45.56]
head: [step:47][stage:total-train][dt:91.31]
head: ========== [step:47] end, E2E [dt:286.69] ==========
head: ========== [step:51] start ==========
head: Epoch: 0 | Step: 51 | PPO Epoch: 1 | Actor Loss: -2.920380651950836 | Critic Loss: 31.215816497802734 | Unsupervised Loss: 0.0
head: End-to-End => Latency: 227.09s, TFLOPs: 36.77, Samples/sec: 2.25, Time/seq 0.44s, Batch Size: 512, Total Seq. Length: 2048
head: Generation => Latency: 33.95s, Per-token Latency 33.15 ms, TFLOPs: 6.83, BW: -1.00 GB/sec, Answer Seq. Length: 1024
head: Training   => Latency: 91.31s, TFLOPs: 81.28
head: Actor Model Parameters => 6.738 B, Critic Model Parameters => 6.607 B
head: Average reward score: -0.460205078125 | EMA reward score: -0.4676404833446406
head: -------------------------------------------------------------------------------------
head: |E2E latency=141.24s |Gather latency=0.49s (0.35%) |Generate time=33.45s (23.68%) |Training time=94.56s (66.95%) |Others=13.23 (9.37%)|CurSamplesPerSec=0.91 |AvgSamplesPerSec=1.79
head: |E2E latency=48.12s |Gather latency=0.59s (1.22%) |Generate time=33.39s (69.39%) |Training time=0.00s (0.00%) |Others=14.73 (30.61%)|CurSamplesPerSec=2.66 |AvgSamplesPerSec=1.80
head: Invalidate trace cache @ step 454: expected module 453, but got module 1
head: Invalidate trace cache @ step 452: expected module 907, but got module 908
head: |E2E latency=48.69s |Gather latency=0.47s (0.97%) |Generate time=33.35s (68.48%) |Training time=0.00s (0.00%) |Others=15.35 (31.52%)|CurSamplesPerSec=2.63 |AvgSamplesPerSec=1.81
head: |E2E latency=48.90s |Gather latency=0.50s (1.02%) |Generate time=33.44s (68.38%) |Training time=0.00s (0.00%) |Others=15.46 (31.62%)|CurSamplesPerSec=2.62 |AvgSamplesPerSec=1.82
head: [2024-06-20 11:38:30,921] [WARNING] [stage3.py:1991:step] 189 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
head: [2024-06-20 11:38:42,125] [WARNING] [stage3.py:1991:step] 189 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
head: [step:51][stage:generate-experience][dt:135.63]
head: [step:51][stage:actor-inference][dt:5.42]
head: [step:51][stage:ref-inference][dt:18.42]
head: [step:51][stage:reward-inference][dt:11.83]
head: [step:51][stage:critic-inference][dt:15.93]
head: [step:51][stage:total-generate][dt:194.17]
head: [step:51][stage:actor-train][dt:47.39]
head: [step:51][stage:critic-train][dt:42.58]
head: [step:51][stage:total-train][dt:90.23]
head: ========== [step:51] end, E2E [dt:284.7] ==========
head: ========== [step:55] start ==========
head: Epoch: 0 | Step: 55 | PPO Epoch: 1 | Actor Loss: -3.734640806913376 | Critic Loss: 35.08912372589111 | Unsupervised Loss: 0.0
head: End-to-End => Latency: 225.85s, TFLOPs: 36.97, Samples/sec: 2.27, Time/seq 0.44s, Batch Size: 512, Total Seq. Length: 2048
head: Generation => Latency: 33.90s, Per-token Latency 33.11 ms, TFLOPs: 6.84, BW: -1.00 GB/sec, Answer Seq. Length: 1024
head: Training   => Latency: 90.23s, TFLOPs: 82.25
head: Actor Model Parameters => 6.738 B, Critic Model Parameters => 6.607 B
head: Average reward score: -0.44775390625 | EMA reward score: -0.4656518256351765
head: -------------------------------------------------------------------------------------
head: |E2E latency=138.99s |Gather latency=0.48s (0.35%) |Generate time=33.42s (24.05%) |Training time=92.06s (66.23%) |Others=13.51 (9.72%)|CurSamplesPerSec=0.92 |AvgSamplesPerSec=1.79
head: |E2E latency=48.95s |Gather latency=0.47s (0.96%) |Generate time=33.40s (68.22%) |Training time=0.00s (0.00%) |Others=15.55 (31.78%)|CurSamplesPerSec=2.61 |AvgSamplesPerSec=1.80
head: Invalidate trace cache @ step 454: expected module 453, but got module 1
head: Invalidate trace cache @ step 452: expected module 907, but got module 908
head: |E2E latency=49.16s |Gather latency=0.53s (1.08%) |Generate time=33.37s (67.88%) |Training time=0.00s (0.00%) |Others=15.79 (32.12%)|CurSamplesPerSec=2.60 |AvgSamplesPerSec=1.81
head: |E2E latency=48.96s |Gather latency=0.53s (1.09%) |Generate time=33.46s (68.34%) |Training time=0.00s (0.00%) |Others=15.50 (31.66%)|CurSamplesPerSec=2.61 |AvgSamplesPerSec=1.82
head: [2024-06-20 11:43:13,844] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32, reducing to 16
head: [2024-06-20 11:43:24,385] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32, reducing to 16
head: [step:55][stage:generate-experience][dt:135.66]
head: [step:55][stage:actor-inference][dt:5.42]
head: [step:55][stage:ref-inference][dt:18.29]
head: [step:55][stage:reward-inference][dt:11.65]
head: [step:55][stage:critic-inference][dt:15.51]
head: [step:55][stage:total-generate][dt:193.73]
head: [step:55][stage:actor-train][dt:44.46]
head: [step:55][stage:critic-train][dt:42.86]
head: [step:55][stage:total-train][dt:87.75]
head: ========== [step:55] end, E2E [dt:282.26] ==========
head: ========== [step:59] start ==========
head: Epoch: 0 | Step: 59 | PPO Epoch: 1 | Actor Loss: nan | Critic Loss: nan | Unsupervised Loss: 0.0
head: End-to-End => Latency: 223.31s, TFLOPs: 37.39, Samples/sec: 2.29, Time/seq 0.44s, Batch Size: 512, Total Seq. Length: 2048
head: Generation => Latency: 33.89s, Per-token Latency 33.10 ms, TFLOPs: 6.84, BW: -1.00 GB/sec, Answer Seq. Length: 1024
head: Training   => Latency: 87.75s, TFLOPs: 84.57
head: Actor Model Parameters => 6.738 B, Critic Model Parameters => 6.607 B
head: Average reward score: -0.49169921875 | EMA reward score: -0.4682565649466588
head: -------------------------------------------------------------------------------------
head: |E2E latency=135.18s |Gather latency=0.49s (0.36%) |Generate time=33.40s (24.71%) |Training time=88.89s (65.76%) |Others=12.89 (9.54%)|CurSamplesPerSec=0.95 |AvgSamplesPerSec=1.79
head: |E2E latency=48.63s |Gather latency=0.51s (1.06%) |Generate time=33.44s (68.77%) |Training time=0.00s (0.00%) |Others=15.19 (31.23%)|CurSamplesPerSec=2.63 |AvgSamplesPerSec=1.80
head: Invalidate trace cache @ step 454: expected module 453, but got module 1
head: Invalidate trace cache @ step 452: expected module 907, but got module 908
head: |E2E latency=48.45s |Gather latency=0.49s (1.02%) |Generate time=33.46s (69.06%) |Training time=0.00s (0.00%) |Others=14.99 (30.94%)|CurSamplesPerSec=2.64 |AvgSamplesPerSec=1.81
head: |E2E latency=49.68s |Gather latency=0.47s (0.95%) |Generate time=33.41s (67.25%) |Training time=0.00s (0.00%) |Others=16.27 (32.75%)|CurSamplesPerSec=2.58 |AvgSamplesPerSec=1.82
head: [2024-06-20 11:47:57,611] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16, reducing to 8
head: [2024-06-20 11:48:08,436] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16, reducing to 8
head: [step:59][stage:generate-experience][dt:135.72]
head: [step:59][stage:actor-inference][dt:5.29]
head: [step:59][stage:ref-inference][dt:20.21]
head: [step:59][stage:reward-inference][dt:12.44]
head: [step:59][stage:critic-inference][dt:15.27]
head: [step:59][stage:total-generate][dt:196.06]
head: [step:59][stage:actor-train][dt:45.48]
head: [step:59][stage:critic-train][dt:41.63]
head: [step:59][stage:total-train][dt:87.58]
head: ========== [step:59] end, E2E [dt:284.05] ==========
head: ========== [step:63] start ==========
head: Epoch: 0 | Step: 63 | PPO Epoch: 1 | Actor Loss: nan | Critic Loss: nan | Unsupervised Loss: 0.0
head: End-to-End => Latency: 223.33s, TFLOPs: 37.38, Samples/sec: 2.29, Time/seq 0.44s, Batch Size: 512, Total Seq. Length: 2048
head: Generation => Latency: 33.94s, Per-token Latency 33.14 ms, TFLOPs: 6.83, BW: -1.00 GB/sec, Answer Seq. Length: 1024
head: Training   => Latency: 87.59s, TFLOPs: 84.73
head: Actor Model Parameters => 6.738 B, Critic Model Parameters => 6.607 B
head: Average reward score: -0.46240234375 | EMA reward score: -0.4676711428269929
head: -------------------------------------------------------------------------------------
head: |E2E latency=137.30s |Gather latency=0.52s (0.38%) |Generate time=33.42s (24.34%) |Training time=90.83s (66.16%) |Others=13.05 (9.50%)|CurSamplesPerSec=0.93 |AvgSamplesPerSec=1.79
head: |E2E latency=49.01s |Gather latency=0.53s (1.07%) |Generate time=33.39s (68.14%) |Training time=0.00s (0.00%) |Others=15.61 (31.86%)|CurSamplesPerSec=2.61 |AvgSamplesPerSec=1.80
head: Invalidate trace cache @ step 454: expected module 453, but got module 1
head: Invalidate trace cache @ step 452: expected module 907, but got module 908
head: |E2E latency=49.25s |Gather latency=0.50s (1.01%) |Generate time=33.43s (67.87%) |Training time=0.00s (0.00%) |Others=15.82 (32.13%)|CurSamplesPerSec=2.60 |AvgSamplesPerSec=1.81
head: |E2E latency=49.55s |Gather latency=0.55s (1.10%) |Generate time=33.42s (67.44%) |Training time=0.00s (0.00%) |Others=16.13 (32.56%)|CurSamplesPerSec=2.58 |AvgSamplesPerSec=1.82
head: [2024-06-20 11:52:45,144] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8, reducing to 4
head: [2024-06-20 11:52:55,258] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8, reducing to 4
head: [step:63][stage:generate-experience][dt:135.7]
head: [step:63][stage:actor-inference][dt:5.55]
head: [step:63][stage:ref-inference][dt:18.2]
head: [step:63][stage:reward-inference][dt:13.03]
head: [step:63][stage:critic-inference][dt:14.79]
head: [step:63][stage:total-generate][dt:194.28]
head: [step:63][stage:actor-train][dt:47.23]
head: [step:63][stage:critic-train][dt:43.99]
head: [step:63][stage:total-train][dt:91.51]
head: ========== [step:63] end, E2E [dt:286.82] ==========
head: ========== [step:67] start ==========
head: Epoch: 0 | Step: 67 | PPO Epoch: 1 | Actor Loss: nan | Critic Loss: nan | Unsupervised Loss: 0.0
head: End-to-End => Latency: 227.08s, TFLOPs: 36.77, Samples/sec: 2.25, Time/seq 0.44s, Batch Size: 512, Total Seq. Length: 2048
head: Generation => Latency: 33.89s, Per-token Latency 33.10 ms, TFLOPs: 6.84, BW: -1.00 GB/sec, Answer Seq. Length: 1024
head: Training   => Latency: 91.51s, TFLOPs: 81.10
head: Actor Model Parameters => 6.738 B, Critic Model Parameters => 6.607 B
head: Average reward score: -0.460205078125 | EMA reward score: -0.4669245363567936
head: -------------------------------------------------------------------------------------
head: |E2E latency=139.01s |Gather latency=0.46s (0.33%) |Generate time=33.43s (24.05%) |Training time=93.19s (67.03%) |Others=12.40 (8.92%)|CurSamplesPerSec=0.92 |AvgSamplesPerSec=1.79
head: |E2E latency=48.15s |Gather latency=0.50s (1.03%) |Generate time=33.40s (69.37%) |Training time=0.00s (0.00%) |Others=14.75 (30.63%)|CurSamplesPerSec=2.66 |AvgSamplesPerSec=1.80
head: Invalidate trace cache @ step 454: expected module 453, but got module 1
head: Invalidate trace cache @ step 452: expected module 907, but got module 908
head: |E2E latency=48.86s |Gather latency=0.52s (1.07%) |Generate time=33.31s (68.17%) |Training time=0.00s (0.00%) |Others=15.55 (31.83%)|CurSamplesPerSec=2.62 |AvgSamplesPerSec=1.81
head: |E2E latency=49.90s |Gather latency=0.51s (1.03%) |Generate time=33.41s (66.95%) |Training time=0.00s (0.00%) |Others=16.49 (33.05%)|CurSamplesPerSec=2.57 |AvgSamplesPerSec=1.82
head: [2024-06-20 11:57:28,317] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4, reducing to 2
head: [2024-06-20 11:57:37,903] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4, reducing to 2
head: [step:67][stage:generate-experience][dt:135.64]
head: [step:67][stage:actor-inference][dt:5.54]
head: [step:67][stage:ref-inference][dt:19.26]
head: [step:67][stage:reward-inference][dt:11.54]
head: [step:67][stage:critic-inference][dt:15.34]
head: [step:67][stage:total-generate][dt:194.56]
head: [step:67][stage:actor-train][dt:43.78]
head: [step:67][stage:critic-train][dt:43.44]
head: [step:67][stage:total-train][dt:87.49]
head: ========== [step:67] end, E2E [dt:282.64] ==========
head: ========== [step:71] start ==========
head: Epoch: 0 | Step: 71 | PPO Epoch: 1 | Actor Loss: nan | Critic Loss: nan | Unsupervised Loss: 0.0
head: End-to-End => Latency: 223.46s, TFLOPs: 37.36, Samples/sec: 2.29, Time/seq 0.44s, Batch Size: 512, Total Seq. Length: 2048
head: Generation => Latency: 33.99s, Per-token Latency 33.20 ms, TFLOPs: 6.82, BW: -1.00 GB/sec, Answer Seq. Length: 1024
head: Training   => Latency: 87.49s, TFLOPs: 84.83
head: Actor Model Parameters => 6.738 B, Critic Model Parameters => 6.607 B
head: Average reward score: -0.47802734375 | EMA reward score: -0.4680348170961142
head: -------------------------------------------------------------------------------------
head: |E2E latency=135.73s |Gather latency=0.54s (0.40%) |Generate time=33.45s (24.64%) |Training time=90.30s (66.53%) |Others=11.98 (8.82%)|CurSamplesPerSec=0.94 |AvgSamplesPerSec=1.79
head: |E2E latency=48.61s |Gather latency=0.48s (0.99%) |Generate time=33.38s (68.66%) |Training time=0.00s (0.00%) |Others=15.23 (31.34%)|CurSamplesPerSec=2.63 |AvgSamplesPerSec=1.80
head: Invalidate trace cache @ step 454: expected module 453, but got module 1
head: Invalidate trace cache @ step 452: expected module 907, but got module 908
head: |E2E latency=48.00s |Gather latency=0.51s (1.06%) |Generate time=33.44s (69.67%) |Training time=0.00s (0.00%) |Others=14.56 (30.33%)|CurSamplesPerSec=2.67 |AvgSamplesPerSec=1.81
head: |E2E latency=50.46s |Gather latency=0.52s (1.03%) |Generate time=33.44s (66.26%) |Training time=0.00s (0.00%) |Others=17.02 (33.74%)|CurSamplesPerSec=2.54 |AvgSamplesPerSec=1.82
head: [2024-06-20 12:02:11,590] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2, reducing to 1
head: [2024-06-20 12:02:22,501] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2, reducing to 1
head: [step:71][stage:generate-experience][dt:135.73]
head: [step:71][stage:actor-inference][dt:5.42]
head: [step:71][stage:ref-inference][dt:20.88]
head: [step:71][stage:reward-inference][dt:11.74]
head: [step:71][stage:critic-inference][dt:15.79]
head: [step:71][stage:total-generate][dt:196.64]
head: [step:71][stage:actor-train][dt:43.37]
head: [step:71][stage:critic-train][dt:43.67]
head: [step:71][stage:total-train][dt:87.42]
head: ========== [step:71] end, E2E [dt:284.6] ==========
head: ========== [step:75] start ==========
head: Epoch: 0 | Step: 75 | PPO Epoch: 1 | Actor Loss: nan | Critic Loss: nan | Unsupervised Loss: 0.0
head: End-to-End => Latency: 223.28s, TFLOPs: 37.39, Samples/sec: 2.29, Time/seq 0.44s, Batch Size: 512, Total Seq. Length: 2048
head: Generation => Latency: 33.96s, Per-token Latency 33.17 ms, TFLOPs: 6.83, BW: -1.00 GB/sec, Answer Seq. Length: 1024
head: Training   => Latency: 87.42s, TFLOPs: 84.89
head: Actor Model Parameters => 6.738 B, Critic Model Parameters => 6.607 B
head: Average reward score: -0.4521484375 | EMA reward score: -0.4664461791365028
head: -------------------------------------------------------------------------------------
head: |E2E latency=137.53s |Gather latency=0.53s (0.38%) |Generate time=33.43s (24.31%) |Training time=90.88s (66.08%) |Others=13.21 (9.61%)|CurSamplesPerSec=0.93 |AvgSamplesPerSec=1.79
head: |E2E latency=48.61s |Gather latency=0.55s (1.12%) |Generate time=33.39s (68.69%) |Training time=0.00s (0.00%) |Others=15.22 (31.31%)|CurSamplesPerSec=2.63 |AvgSamplesPerSec=1.80
head: Invalidate trace cache @ step 454: expected module 453, but got module 1
head: Invalidate trace cache @ step 452: expected module 907, but got module 908
head: |E2E latency=49.32s |Gather latency=0.51s (1.04%) |Generate time=33.46s (67.85%) |Training time=0.00s (0.00%) |Others=15.86 (32.15%)|CurSamplesPerSec=2.60 |AvgSamplesPerSec=1.81
head: |E2E latency=48.40s |Gather latency=0.52s (1.08%) |Generate time=33.42s (69.05%) |Training time=0.00s (0.00%) |Others=14.98 (30.95%)|CurSamplesPerSec=2.64 |AvgSamplesPerSec=1.82
head: Traceback (most recent call last):
head:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 697, in <module>
head:     main()
head:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 571, in main
head:     actor_loss, critic_loss = trainer.train_rlhf(exp_data)
head:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py", line 267, in train_rlhf
head:     self.actor_model.step()
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/hybrid_engine.py", line 452, in step
head:     super().step(lr_kwargs=lr_kwargs)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2148, in step
head:     self._take_model_step(lr_kwargs)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2054, in _take_model_step
head:     self.optimizer.step()
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
head:     ret_val = func(*args, **kwargs)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1946, in step
head:     if self._overflow_check_and_loss_scale_update():
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
head:     ret_val = func(*args, **kwargs)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1894, in _overflow_check_and_loss_scale_update
head:     self._update_scale(self.overflow)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 2281, in _update_scale
head:     self.loss_scaler.update_scale(has_overflow)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 175, in update_scale
head:     raise Exception(
head: Exception: Current loss scale already at minimum - cannot decrease scale anymore. Exiting run.
head: Traceback (most recent call last):
head:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 697, in <module>
head:     main()
head:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 571, in main
head:     actor_loss, critic_loss = trainer.train_rlhf(exp_data)
head:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py", line 267, in train_rlhf
head:     self.actor_model.step()
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/hybrid_engine.py", line 452, in step
head:     super().step(lr_kwargs=lr_kwargs)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2148, in step
head:     self._take_model_step(lr_kwargs)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2054, in _take_model_step
head:     self.optimizer.step()
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
head:     ret_val = func(*args, **kwargs)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1946, in step
head:     if self._overflow_check_and_loss_scale_update():
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
head:     ret_val = func(*args, **kwargs)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1894, in _overflow_check_and_loss_scale_update
head:     self._update_scale(self.overflow)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 2281, in _update_scale
head:     self.loss_scaler.update_scale(has_overflow)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 175, in update_scale
head:     raise Exception(
head: Exception: Current loss scale already at minimum - cannot decrease scale anymore. Exiting run.
head: Traceback (most recent call last):
head:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 697, in <module>
head:     main()
head:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 571, in main
head:     actor_loss, critic_loss = trainer.train_rlhf(exp_data)
head:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py", line 267, in train_rlhf
head:     self.actor_model.step()
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/hybrid_engine.py", line 452, in step
head:     super().step(lr_kwargs=lr_kwargs)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2148, in step
head:     self._take_model_step(lr_kwargs)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2054, in _take_model_step
head:     self.optimizer.step()
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
head:     ret_val = func(*args, **kwargs)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1946, in step
head:     if self._overflow_check_and_loss_scale_update():
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
head:     ret_val = func(*args, **kwargs)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1894, in _overflow_check_and_loss_scale_update
head:     self._update_scale(self.overflow)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 2281, in _update_scale
head:     self.loss_scaler.update_scale(has_overflow)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 175, in update_scale
head:     raise Exception(
head: Exception: Current loss scale already at minimum - cannot decrease scale anymore. Exiting run.
head: Traceback (most recent call last):
head:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 697, in <module>
head:     main()
head:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 571, in main
head:     actor_loss, critic_loss = trainer.train_rlhf(exp_data)
head:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py", line 267, in train_rlhf
head:     self.actor_model.step()
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/hybrid_engine.py", line 452, in step
head:     super().step(lr_kwargs=lr_kwargs)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2148, in step
head:     self._take_model_step(lr_kwargs)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2054, in _take_model_step
head:     self.optimizer.step()
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
head:     ret_val = func(*args, **kwargs)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1946, in step
head:     if self._overflow_check_and_loss_scale_update():
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
head:     ret_val = func(*args, **kwargs)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1894, in _overflow_check_and_loss_scale_update
head:     self._update_scale(self.overflow)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 2281, in _update_scale
head:     self.loss_scaler.update_scale(has_overflow)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 175, in update_scale
head:     raise Exception(
head: Exception: Current loss scale already at minimum - cannot decrease scale anymore. Exiting run.
head: Traceback (most recent call last):
head:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 697, in <module>
head:     main()
head:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 571, in main
head:     actor_loss, critic_loss = trainer.train_rlhf(exp_data)
head:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py", line 267, in train_rlhf
head:     self.actor_model.step()
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/hybrid_engine.py", line 452, in step
head:     super().step(lr_kwargs=lr_kwargs)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2148, in step
head:     self._take_model_step(lr_kwargs)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2054, in _take_model_step
head:     self.optimizer.step()
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
head:     ret_val = func(*args, **kwargs)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1946, in step
head:     if self._overflow_check_and_loss_scale_update():
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
head:     ret_val = func(*args, **kwargs)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1894, in _overflow_check_and_loss_scale_update
head:     self._update_scale(self.overflow)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 2281, in _update_scale
head:     self.loss_scaler.update_scale(has_overflow)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 175, in update_scale
head:     raise Exception(
head: Exception: Current loss scale already at minimum - cannot decrease scale anymore. Exiting run.
head: Traceback (most recent call last):
head:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 697, in <module>
head:     main()
head:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 571, in main
head:     actor_loss, critic_loss = trainer.train_rlhf(exp_data)
head:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py", line 267, in train_rlhf
head:     self.actor_model.step()
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/hybrid_engine.py", line 452, in step
head:     super().step(lr_kwargs=lr_kwargs)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2148, in step
head:     self._take_model_step(lr_kwargs)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2054, in _take_model_step
head:     self.optimizer.step()
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
head:     ret_val = func(*args, **kwargs)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1946, in step
head:     if self._overflow_check_and_loss_scale_update():
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
head:     ret_val = func(*args, **kwargs)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1894, in _overflow_check_and_loss_scale_update
head:     self._update_scale(self.overflow)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 2281, in _update_scale
head:     self.loss_scaler.update_scale(has_overflow)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 175, in update_scale
head:     raise Exception(
head: Exception: Current loss scale already at minimum - cannot decrease scale anymore. Exiting run.
head: Traceback (most recent call last):
head:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 697, in <module>
head:     main()
head:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 571, in main
head:     actor_loss, critic_loss = trainer.train_rlhf(exp_data)
head:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py", line 267, in train_rlhf
head:     self.actor_model.step()
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/hybrid_engine.py", line 452, in step
head:     super().step(lr_kwargs=lr_kwargs)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2148, in step
head:     self._take_model_step(lr_kwargs)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2054, in _take_model_step
head:     self.optimizer.step()
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
head:     ret_val = func(*args, **kwargs)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1946, in step
head:     if self._overflow_check_and_loss_scale_update():
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
head:     ret_val = func(*args, **kwargs)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1894, in _overflow_check_and_loss_scale_update
head:     self._update_scale(self.overflow)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 2281, in _update_scale
head:     self.loss_scaler.update_scale(has_overflow)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 175, in update_scale
head:     raise Exception(
head: Exception: Current loss scale already at minimum - cannot decrease scale anymore. Exiting run.
head: Traceback (most recent call last):
head:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 697, in <module>
head:     main()
head:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 571, in main
head:     actor_loss, critic_loss = trainer.train_rlhf(exp_data)
head:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py", line 267, in train_rlhf
head:     self.actor_model.step()
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/hybrid_engine.py", line 452, in step
head:     super().step(lr_kwargs=lr_kwargs)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2148, in step
head:     self._take_model_step(lr_kwargs)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2054, in _take_model_step
head:     self.optimizer.step()
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
head:     ret_val = func(*args, **kwargs)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1946, in step
head:     if self._overflow_check_and_loss_scale_update():
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
head:     ret_val = func(*args, **kwargs)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1894, in _overflow_check_and_loss_scale_update
head:     self._update_scale(self.overflow)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 2281, in _update_scale
head:     self.loss_scaler.update_scale(has_overflow)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 175, in update_scale
head:     raise Exception(
head: Exception: Current loss scale already at minimum - cannot decrease scale anymore. Exiting run.
worker1: Traceback (most recent call last):
worker1:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 697, in <module>
worker1:     main()
worker1:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 571, in main
worker1:     actor_loss, critic_loss = trainer.train_rlhf(exp_data)
worker1:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py", line 267, in train_rlhf
worker1:     self.actor_model.step()
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/hybrid_engine.py", line 452, in step
worker1:     super().step(lr_kwargs=lr_kwargs)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2148, in step
worker1:     self._take_model_step(lr_kwargs)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2054, in _take_model_step
worker1:     self.optimizer.step()
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
worker1:     ret_val = func(*args, **kwargs)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1946, in step
worker1:     if self._overflow_check_and_loss_scale_update():
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
worker1:     ret_val = func(*args, **kwargs)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1894, in _overflow_check_and_loss_scale_update
worker1:     self._update_scale(self.overflow)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 2281, in _update_scale
worker1:     self.loss_scaler.update_scale(has_overflow)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 175, in update_scale
worker1:     raise Exception(
worker1: Exception: Current loss scale already at minimum - cannot decrease scale anymore. Exiting run.
worker1: Traceback (most recent call last):
worker1:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 697, in <module>
worker1:     main()
worker1:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 571, in main
worker1:     actor_loss, critic_loss = trainer.train_rlhf(exp_data)
worker1:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py", line 267, in train_rlhf
worker1:     self.actor_model.step()
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/hybrid_engine.py", line 452, in step
worker1:     super().step(lr_kwargs=lr_kwargs)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2148, in step
worker1:     self._take_model_step(lr_kwargs)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2054, in _take_model_step
worker1:     self.optimizer.step()
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
worker1:     ret_val = func(*args, **kwargs)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1946, in step
worker1:     if self._overflow_check_and_loss_scale_update():
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
worker1:     ret_val = func(*args, **kwargs)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1894, in _overflow_check_and_loss_scale_update
worker1:     self._update_scale(self.overflow)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 2281, in _update_scale
worker1:     self.loss_scaler.update_scale(has_overflow)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 175, in update_scale
worker1:     raise Exception(
worker1: Exception: Current loss scale already at minimum - cannot decrease scale anymore. Exiting run.
worker1: Traceback (most recent call last):
worker1:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 697, in <module>
worker1:     main()
worker1:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 571, in main
worker1:     actor_loss, critic_loss = trainer.train_rlhf(exp_data)
worker1:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py", line 267, in train_rlhf
worker1:     self.actor_model.step()
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/hybrid_engine.py", line 452, in step
worker1:     super().step(lr_kwargs=lr_kwargs)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2148, in step
worker1:     self._take_model_step(lr_kwargs)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2054, in _take_model_step
worker1:     self.optimizer.step()
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
worker1:     ret_val = func(*args, **kwargs)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1946, in step
worker1:     if self._overflow_check_and_loss_scale_update():
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
worker1:     ret_val = func(*args, **kwargs)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1894, in _overflow_check_and_loss_scale_update
worker1:     self._update_scale(self.overflow)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 2281, in _update_scale
worker1:     self.loss_scaler.update_scale(has_overflow)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 175, in update_scale
worker1:     raise Exception(
worker1: Exception: Current loss scale already at minimum - cannot decrease scale anymore. Exiting run.
worker1: Traceback (most recent call last):
worker1:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 697, in <module>
worker1:     main()
worker1:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 571, in main
worker1:     actor_loss, critic_loss = trainer.train_rlhf(exp_data)
worker1:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py", line 267, in train_rlhf
worker1:     self.actor_model.step()
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/hybrid_engine.py", line 452, in step
worker1:     super().step(lr_kwargs=lr_kwargs)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2148, in step
worker1:     self._take_model_step(lr_kwargs)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2054, in _take_model_step
worker1:     self.optimizer.step()
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
worker1:     ret_val = func(*args, **kwargs)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1946, in step
worker1:     if self._overflow_check_and_loss_scale_update():
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
worker1:     ret_val = func(*args, **kwargs)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1894, in _overflow_check_and_loss_scale_update
worker1:     self._update_scale(self.overflow)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 2281, in _update_scale
worker1:     self.loss_scaler.update_scale(has_overflow)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 175, in update_scale
worker1:     raise Exception(
worker1: Exception: Current loss scale already at minimum - cannot decrease scale anymore. Exiting run.
worker1: Traceback (most recent call last):
worker1:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 697, in <module>
worker1:     main()
worker1:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 571, in main
worker1:     actor_loss, critic_loss = trainer.train_rlhf(exp_data)
worker1:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py", line 267, in train_rlhf
worker1:     self.actor_model.step()
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/hybrid_engine.py", line 452, in step
worker1:     super().step(lr_kwargs=lr_kwargs)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2148, in step
worker1:     self._take_model_step(lr_kwargs)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2054, in _take_model_step
worker1:     self.optimizer.step()
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
worker1:     ret_val = func(*args, **kwargs)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1946, in step
worker1:     if self._overflow_check_and_loss_scale_update():
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
worker1:     ret_val = func(*args, **kwargs)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1894, in _overflow_check_and_loss_scale_update
worker1:     self._update_scale(self.overflow)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 2281, in _update_scale
worker1:     self.loss_scaler.update_scale(has_overflow)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 175, in update_scale
worker1:     raise Exception(
worker1: Exception: Current loss scale already at minimum - cannot decrease scale anymore. Exiting run.
worker1: Traceback (most recent call last):
worker1:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 697, in <module>
worker1:     main()
worker1:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 571, in main
worker1:     actor_loss, critic_loss = trainer.train_rlhf(exp_data)
worker1:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py", line 267, in train_rlhf
worker1:     self.actor_model.step()
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/hybrid_engine.py", line 452, in step
worker1:     super().step(lr_kwargs=lr_kwargs)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2148, in step
worker1:     self._take_model_step(lr_kwargs)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2054, in _take_model_step
worker1:     self.optimizer.step()
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
worker1:     ret_val = func(*args, **kwargs)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1946, in step
worker1:     if self._overflow_check_and_loss_scale_update():
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
worker1:     ret_val = func(*args, **kwargs)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1894, in _overflow_check_and_loss_scale_update
worker1:     self._update_scale(self.overflow)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 2281, in _update_scale
worker1:     self.loss_scaler.update_scale(has_overflow)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 175, in update_scale
worker1:     raise Exception(
worker1: Exception: Current loss scale already at minimum - cannot decrease scale anymore. Exiting run.
worker1: Traceback (most recent call last):
worker1:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 697, in <module>
worker1:     main()
worker1:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 571, in main
worker1:     actor_loss, critic_loss = trainer.train_rlhf(exp_data)
worker1:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py", line 267, in train_rlhf
worker1:     self.actor_model.step()
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/hybrid_engine.py", line 452, in step
worker1:     super().step(lr_kwargs=lr_kwargs)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2148, in step
worker1:     self._take_model_step(lr_kwargs)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2054, in _take_model_step
worker1:     self.optimizer.step()
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
worker1:     ret_val = func(*args, **kwargs)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1946, in step
worker1:     if self._overflow_check_and_loss_scale_update():
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
worker1:     ret_val = func(*args, **kwargs)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1894, in _overflow_check_and_loss_scale_update
worker1:     self._update_scale(self.overflow)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 2281, in _update_scale
worker1:     self.loss_scaler.update_scale(has_overflow)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 175, in update_scale
worker1:     raise Exception(
worker1: Exception: Current loss scale already at minimum - cannot decrease scale anymore. Exiting run.
worker1: Traceback (most recent call last):
worker1:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 697, in <module>
worker1:     main()
worker1:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 571, in main
worker1:     actor_loss, critic_loss = trainer.train_rlhf(exp_data)
worker1:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py", line 267, in train_rlhf
worker1:     self.actor_model.step()
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/hybrid_engine.py", line 452, in step
worker1:     super().step(lr_kwargs=lr_kwargs)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2148, in step
worker1:     self._take_model_step(lr_kwargs)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2054, in _take_model_step
worker1:     self.optimizer.step()
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
worker1:     ret_val = func(*args, **kwargs)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1946, in step
worker1:     if self._overflow_check_and_loss_scale_update():
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
worker1:     ret_val = func(*args, **kwargs)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1894, in _overflow_check_and_loss_scale_update
worker1:     self._update_scale(self.overflow)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 2281, in _update_scale
worker1:     self.loss_scaler.update_scale(has_overflow)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 175, in update_scale
worker1:     raise Exception(
worker1: Exception: Current loss scale already at minimum - cannot decrease scale anymore. Exiting run.
worker1: [2024-06-20 12:06:57,930] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 192638
head: [2024-06-20 12:06:58,462] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 388573
head: [2024-06-20 12:06:58,676] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 388574
head: [2024-06-20 12:06:58,929] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 388575
head: [2024-06-20 12:07:00,155] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 388576
head: [2024-06-20 12:07:00,156] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 388577
head: [2024-06-20 12:07:00,409] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 388578
head: [2024-06-20 12:07:00,411] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 388579
head: [2024-06-20 12:07:00,411] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 388580
head: [2024-06-20 12:07:00,412] [ERROR] [launch.py:321:sigkill_handler] ['/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/bin/python', '-u', 'main.py', '--local_rank=7', '--data_path', 'Dahoas/full-hh-rlhf', '--data_split', '2,4,4', '--actor_model_name_or_path', '/fs-computility/llm/chenyang2/OpenLLMAI/Llama-2-7b-sft-model-ocra-500k', '--critic_model_name_or_path', '/fs-computility/llm/chenyang2/hf_model/OpenLLMAI/Llama-2-7b-rm-anthropic_hh-lmsys-oasst-webgpt/models--OpenLLMAI--Llama-2-7b-rm-anthropic_hh-lmsys-oasst-webgpt/snapshots/a982afeed00fac9767d53aecde5b88947b1be194', '--num_padding_at_beginning', '1', '--per_device_generation_batch_size', '8', '--per_device_training_batch_size', '8', '--generation_batches', '4', '--gradient_accumulation_steps', '4', '--ppo_epochs', '1', '--max_answer_seq_len', '1024', '--max_prompt_seq_len', '1024', '--actor_learning_rate', '9.65e-6', '--critic_learning_rate', '5e-6', '--actor_weight_decay', '0.1', '--critic_weight_decay', '0.1', '--num_train_epochs', '1', '--lr_scheduler_type', 'cosine', '--actor_gradient_checkpointing', '--critic_gradient_checkpointing', '--actor_dropout', '0.0', '--num_warmup_steps', '0', '--deepspeed', '--seed', '1234', '--actor_zero_stage', '3', '--critic_zero_stage', '3', '--enable_hybrid_engine', '--output_dir', './output_step3_llama'] exits with return code = 1
worker1: [2024-06-20 12:07:00,771] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 192639
worker1: [2024-06-20 12:07:00,771] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 192640
worker1: [2024-06-20 12:07:00,772] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 192641
worker1: [2024-06-20 12:07:00,773] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 192642
worker1: [2024-06-20 12:07:00,774] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 192643
worker1: [2024-06-20 12:07:00,774] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 192644
worker1: [2024-06-20 12:07:00,775] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 192645
worker1: [2024-06-20 12:07:00,776] [ERROR] [launch.py:321:sigkill_handler] ['/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/bin/python', '-u', 'main.py', '--local_rank=7', '--data_path', 'Dahoas/full-hh-rlhf', '--data_split', '2,4,4', '--actor_model_name_or_path', '/fs-computility/llm/chenyang2/OpenLLMAI/Llama-2-7b-sft-model-ocra-500k', '--critic_model_name_or_path', '/fs-computility/llm/chenyang2/hf_model/OpenLLMAI/Llama-2-7b-rm-anthropic_hh-lmsys-oasst-webgpt/models--OpenLLMAI--Llama-2-7b-rm-anthropic_hh-lmsys-oasst-webgpt/snapshots/a982afeed00fac9767d53aecde5b88947b1be194', '--num_padding_at_beginning', '1', '--per_device_generation_batch_size', '8', '--per_device_training_batch_size', '8', '--generation_batches', '4', '--gradient_accumulation_steps', '4', '--ppo_epochs', '1', '--max_answer_seq_len', '1024', '--max_prompt_seq_len', '1024', '--actor_learning_rate', '9.65e-6', '--critic_learning_rate', '5e-6', '--actor_weight_decay', '0.1', '--critic_weight_decay', '0.1', '--num_train_epochs', '1', '--lr_scheduler_type', 'cosine', '--actor_gradient_checkpointing', '--critic_gradient_checkpointing', '--actor_dropout', '0.0', '--num_warmup_steps', '0', '--deepspeed', '--seed', '1234', '--actor_zero_stage', '3', '--critic_zero_stage', '3', '--enable_hybrid_engine', '--output_dir', './output_step3_llama'] exits with return code = 1
pdsh@t-20240620111711-c5m49-ray-job-head-zkrsj: head: ssh exited with exit code 1
pdsh@t-20240620111711-c5m49-ray-job-head-zkrsj: worker1: ssh exited with exit code 1
