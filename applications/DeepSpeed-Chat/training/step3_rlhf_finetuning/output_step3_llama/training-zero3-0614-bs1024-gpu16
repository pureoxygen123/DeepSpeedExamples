[2024-06-14 07:24:29,362] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: Permanently added '[172.28.32.135]:12222' (ECDSA) to the list of known hosts.
[2024-06-14 07:24:36,708] [INFO] [runner.py:463:main] Using IP address of 10.22.192.107 for node worker1
[2024-06-14 07:24:36,711] [INFO] [multinode_runner.py:80:get_cmd] Running on the following workers: worker1,worker2,worker3,head
[2024-06-14 07:24:36,711] [INFO] [runner.py:571:main] cmd = pdsh -S -f 1024 -w worker1,worker2,worker3,head export PYTHONUNBUFFERED=0; export NCCL_IB_PCI_RELAXED_ORDERING=1; export NCCL_VERSION=2.16.2-1; export NCCL_SOCKET_IFNAME=eth1; export NCCL_DEBUG=INFO; export NCCL_IB_HCA=mlx5_1:1,mlx5_2:1,mlx5_3:1,mlx5_4:1; export NCCL_IB_GID_INDEX=7; export NCCL_IB_TIMEOUT=23; export NCCL_IB_DISABLE=0; export NCCL_IB_RETRY_CNT=7; export PYTHONPATH=/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning;  cd /fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning; /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/bin/python -u -m deepspeed.launcher.launch --world_info=eyJ3b3JrZXIxIjogWzAsIDEsIDIsIDNdLCAid29ya2VyMiI6IFswLCAxLCAyLCAzXSwgIndvcmtlcjMiOiBbMCwgMSwgMiwgM10sICJoZWFkIjogWzAsIDEsIDIsIDNdfQ== --node_rank=%n --master_addr=10.22.192.107 --master_port=29500 main.py --data_path Dahoas/full-hh-rlhf --data_split 2,4,4 --actor_model_name_or_path /fs-computility/llm/chenyang2/OpenLLMAI/Llama-2-7b-sft-model-ocra-500k --critic_model_name_or_path /fs-computility/llm/chenyang2/hf_model/OpenLLMAI/Llama-2-7b-rm-anthropic_hh-lmsys-oasst-webgpt/models--OpenLLMAI--Llama-2-7b-rm-anthropic_hh-lmsys-oasst-webgpt/snapshots/a982afeed00fac9767d53aecde5b88947b1be194 --num_padding_at_beginning 1 --per_device_generation_batch_size 8 --per_device_training_batch_size 8 --generation_batches 8 --ppo_epochs 1 --max_answer_seq_len 1024 --max_prompt_seq_len 1024 --actor_learning_rate 9.65e-6 --critic_learning_rate 5e-6 --actor_weight_decay 0.1 --critic_weight_decay 0.1 --num_train_epochs 1 --lr_scheduler_type cosine --offload --gradient_accumulation_steps 8 --actor_gradient_checkpointing --critic_gradient_checkpointing --actor_dropout 0.0 --num_warmup_steps 0 --deepspeed --seed 1234 --actor_zero_stage 3 --critic_zero_stage 3 --enable_hybrid_engine --output_dir ./output_step3_llama
head: Warning: Permanently added '[172.28.32.131]:12222' (ECDSA) to the list of known hosts.
worker2: Warning: Permanently added '[172.28.16.13]:12222' (ECDSA) to the list of known hosts.
worker3: Warning: Permanently added '[172.28.16.12]:12222' (ECDSA) to the list of known hosts.
worker1: Warning: Permanently added '[172.28.32.135]:12222' (ECDSA) to the list of known hosts.
worker3: [2024-06-14 07:24:41,264] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
head: [2024-06-14 07:24:41,274] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
worker1: [2024-06-14 07:24:41,297] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
worker2: [2024-06-14 07:24:41,355] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
head: [2024-06-14 07:24:43,400] [INFO] [launch.py:138:main] 3 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.16.2-1+cuda11.8
head: [2024-06-14 07:24:43,400] [INFO] [launch.py:138:main] 3 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.16.2-1
head: [2024-06-14 07:24:43,400] [INFO] [launch.py:138:main] 3 NCCL_IB_PCI_RELAXED_ORDERING=1
head: [2024-06-14 07:24:43,400] [INFO] [launch.py:138:main] 3 NCCL_VERSION=2.16.2-1
head: [2024-06-14 07:24:43,400] [INFO] [launch.py:138:main] 3 NCCL_SOCKET_IFNAME=eth1
head: [2024-06-14 07:24:43,400] [INFO] [launch.py:138:main] 3 NCCL_DEBUG=INFO
head: [2024-06-14 07:24:43,400] [INFO] [launch.py:138:main] 3 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev
head: [2024-06-14 07:24:43,400] [INFO] [launch.py:138:main] 3 NCCL_IB_HCA=mlx5_1:1,mlx5_2:1,mlx5_3:1,mlx5_4:1
head: [2024-06-14 07:24:43,400] [INFO] [launch.py:138:main] 3 NCCL_IB_GID_INDEX=7
head: [2024-06-14 07:24:43,400] [INFO] [launch.py:138:main] 3 NCCL_IB_TIMEOUT=23
head: [2024-06-14 07:24:43,400] [INFO] [launch.py:138:main] 3 NV_LIBNCCL_PACKAGE=libnccl2=2.16.2-1+cuda11.8
head: [2024-06-14 07:24:43,400] [INFO] [launch.py:138:main] 3 NCCL_IB_DISABLE=0
head: [2024-06-14 07:24:43,400] [INFO] [launch.py:138:main] 3 NCCL_IB_RETRY_CNT=7
head: [2024-06-14 07:24:43,400] [INFO] [launch.py:138:main] 3 NV_LIBNCCL_PACKAGE_NAME=libnccl2
head: [2024-06-14 07:24:43,400] [INFO] [launch.py:138:main] 3 NV_LIBNCCL_PACKAGE_VERSION=2.16.2-1
head: [2024-06-14 07:24:43,400] [INFO] [launch.py:145:main] WORLD INFO DICT: {'worker1': [0, 1, 2, 3], 'worker2': [0, 1, 2, 3], 'worker3': [0, 1, 2, 3], 'head': [0, 1, 2, 3]}
head: [2024-06-14 07:24:43,400] [INFO] [launch.py:151:main] nnodes=4, num_local_procs=4, node_rank=3
head: [2024-06-14 07:24:43,401] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'worker1': [0, 1, 2, 3], 'worker2': [4, 5, 6, 7], 'worker3': [8, 9, 10, 11], 'head': [12, 13, 14, 15]})
head: [2024-06-14 07:24:43,401] [INFO] [launch.py:163:main] dist_world_size=16
head: [2024-06-14 07:24:43,401] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
worker3: [2024-06-14 07:24:43,749] [INFO] [launch.py:138:main] 2 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.16.2-1+cuda11.8
worker3: [2024-06-14 07:24:43,749] [INFO] [launch.py:138:main] 2 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.16.2-1
worker3: [2024-06-14 07:24:43,749] [INFO] [launch.py:138:main] 2 NCCL_IB_PCI_RELAXED_ORDERING=1
worker3: [2024-06-14 07:24:43,749] [INFO] [launch.py:138:main] 2 NCCL_VERSION=2.16.2-1
worker3: [2024-06-14 07:24:43,749] [INFO] [launch.py:138:main] 2 NCCL_SOCKET_IFNAME=eth1
worker3: [2024-06-14 07:24:43,749] [INFO] [launch.py:138:main] 2 NCCL_DEBUG=INFO
worker3: [2024-06-14 07:24:43,749] [INFO] [launch.py:138:main] 2 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev
worker3: [2024-06-14 07:24:43,749] [INFO] [launch.py:138:main] 2 NCCL_IB_HCA=mlx5_1:1,mlx5_2:1,mlx5_3:1,mlx5_4:1
worker3: [2024-06-14 07:24:43,749] [INFO] [launch.py:138:main] 2 NCCL_IB_GID_INDEX=7
worker3: [2024-06-14 07:24:43,749] [INFO] [launch.py:138:main] 2 NCCL_IB_TIMEOUT=23
worker3: [2024-06-14 07:24:43,749] [INFO] [launch.py:138:main] 2 NV_LIBNCCL_PACKAGE=libnccl2=2.16.2-1+cuda11.8
worker3: [2024-06-14 07:24:43,749] [INFO] [launch.py:138:main] 2 NCCL_IB_DISABLE=0
worker3: [2024-06-14 07:24:43,749] [INFO] [launch.py:138:main] 2 NCCL_IB_RETRY_CNT=7
worker3: [2024-06-14 07:24:43,749] [INFO] [launch.py:138:main] 2 NV_LIBNCCL_PACKAGE_NAME=libnccl2
worker3: [2024-06-14 07:24:43,749] [INFO] [launch.py:138:main] 2 NV_LIBNCCL_PACKAGE_VERSION=2.16.2-1
worker3: [2024-06-14 07:24:43,749] [INFO] [launch.py:145:main] WORLD INFO DICT: {'worker1': [0, 1, 2, 3], 'worker2': [0, 1, 2, 3], 'worker3': [0, 1, 2, 3], 'head': [0, 1, 2, 3]}
worker3: [2024-06-14 07:24:43,749] [INFO] [launch.py:151:main] nnodes=4, num_local_procs=4, node_rank=2
worker3: [2024-06-14 07:24:43,749] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'worker1': [0, 1, 2, 3], 'worker2': [4, 5, 6, 7], 'worker3': [8, 9, 10, 11], 'head': [12, 13, 14, 15]})
worker3: [2024-06-14 07:24:43,749] [INFO] [launch.py:163:main] dist_world_size=16
worker3: [2024-06-14 07:24:43,749] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
worker1: [2024-06-14 07:24:43,871] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.16.2-1+cuda11.8
worker1: [2024-06-14 07:24:43,871] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.16.2-1
worker1: [2024-06-14 07:24:43,871] [INFO] [launch.py:138:main] 0 NCCL_IB_PCI_RELAXED_ORDERING=1
worker1: [2024-06-14 07:24:43,871] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.16.2-1
worker1: [2024-06-14 07:24:43,871] [INFO] [launch.py:138:main] 0 NCCL_SOCKET_IFNAME=eth1
worker1: [2024-06-14 07:24:43,871] [INFO] [launch.py:138:main] 0 NCCL_DEBUG=INFO
worker1: [2024-06-14 07:24:43,871] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev
worker1: [2024-06-14 07:24:43,871] [INFO] [launch.py:138:main] 0 NCCL_IB_HCA=mlx5_1:1,mlx5_2:1,mlx5_3:1,mlx5_4:1
worker1: [2024-06-14 07:24:43,871] [INFO] [launch.py:138:main] 0 NCCL_IB_GID_INDEX=7
worker1: [2024-06-14 07:24:43,871] [INFO] [launch.py:138:main] 0 NCCL_IB_TIMEOUT=23
worker1: [2024-06-14 07:24:43,871] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.16.2-1+cuda11.8
worker1: [2024-06-14 07:24:43,871] [INFO] [launch.py:138:main] 0 NCCL_IB_DISABLE=0
worker1: [2024-06-14 07:24:43,871] [INFO] [launch.py:138:main] 0 NCCL_IB_RETRY_CNT=7
worker1: [2024-06-14 07:24:43,871] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2
worker1: [2024-06-14 07:24:43,871] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.16.2-1
worker1: [2024-06-14 07:24:43,871] [INFO] [launch.py:138:main] 0 NCCL_TOPO_FILE=/var/run/nvidia-topologyd/virtualTopology.xml
worker1: [2024-06-14 07:24:43,871] [INFO] [launch.py:145:main] WORLD INFO DICT: {'worker1': [0, 1, 2, 3], 'worker2': [0, 1, 2, 3], 'worker3': [0, 1, 2, 3], 'head': [0, 1, 2, 3]}
worker1: [2024-06-14 07:24:43,872] [INFO] [launch.py:151:main] nnodes=4, num_local_procs=4, node_rank=0
worker1: [2024-06-14 07:24:43,872] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'worker1': [0, 1, 2, 3], 'worker2': [4, 5, 6, 7], 'worker3': [8, 9, 10, 11], 'head': [12, 13, 14, 15]})
worker1: [2024-06-14 07:24:43,872] [INFO] [launch.py:163:main] dist_world_size=16
worker1: [2024-06-14 07:24:43,872] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
worker2: [2024-06-14 07:24:43,882] [INFO] [launch.py:138:main] 1 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.16.2-1+cuda11.8
worker2: [2024-06-14 07:24:43,883] [INFO] [launch.py:138:main] 1 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.16.2-1
worker2: [2024-06-14 07:24:43,883] [INFO] [launch.py:138:main] 1 NCCL_IB_PCI_RELAXED_ORDERING=1
worker2: [2024-06-14 07:24:43,883] [INFO] [launch.py:138:main] 1 NCCL_VERSION=2.16.2-1
worker2: [2024-06-14 07:24:43,883] [INFO] [launch.py:138:main] 1 NCCL_SOCKET_IFNAME=eth1
worker2: [2024-06-14 07:24:43,883] [INFO] [launch.py:138:main] 1 NCCL_DEBUG=INFO
worker2: [2024-06-14 07:24:43,883] [INFO] [launch.py:138:main] 1 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev
worker2: [2024-06-14 07:24:43,883] [INFO] [launch.py:138:main] 1 NCCL_IB_HCA=mlx5_1:1,mlx5_2:1,mlx5_3:1,mlx5_4:1
worker2: [2024-06-14 07:24:43,883] [INFO] [launch.py:138:main] 1 NCCL_IB_GID_INDEX=7
worker2: [2024-06-14 07:24:43,883] [INFO] [launch.py:138:main] 1 NCCL_IB_TIMEOUT=23
worker2: [2024-06-14 07:24:43,883] [INFO] [launch.py:138:main] 1 NV_LIBNCCL_PACKAGE=libnccl2=2.16.2-1+cuda11.8
worker2: [2024-06-14 07:24:43,883] [INFO] [launch.py:138:main] 1 NCCL_IB_DISABLE=0
worker2: [2024-06-14 07:24:43,883] [INFO] [launch.py:138:main] 1 NCCL_IB_RETRY_CNT=7
worker2: [2024-06-14 07:24:43,883] [INFO] [launch.py:138:main] 1 NV_LIBNCCL_PACKAGE_NAME=libnccl2
worker2: [2024-06-14 07:24:43,883] [INFO] [launch.py:138:main] 1 NV_LIBNCCL_PACKAGE_VERSION=2.16.2-1
worker2: [2024-06-14 07:24:43,883] [INFO] [launch.py:145:main] WORLD INFO DICT: {'worker1': [0, 1, 2, 3], 'worker2': [0, 1, 2, 3], 'worker3': [0, 1, 2, 3], 'head': [0, 1, 2, 3]}
worker2: [2024-06-14 07:24:43,883] [INFO] [launch.py:151:main] nnodes=4, num_local_procs=4, node_rank=1
worker2: [2024-06-14 07:24:43,883] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'worker1': [0, 1, 2, 3], 'worker2': [4, 5, 6, 7], 'worker3': [8, 9, 10, 11], 'head': [12, 13, 14, 15]})
worker2: [2024-06-14 07:24:43,883] [INFO] [launch.py:163:main] dist_world_size=16
worker2: [2024-06-14 07:24:43,883] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
head: [2024-06-14 07:24:46,299] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
head: [2024-06-14 07:24:46,322] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
head: [2024-06-14 07:24:46,328] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
head: [2024-06-14 07:24:46,347] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
worker3: [2024-06-14 07:24:46,625] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
worker3: [2024-06-14 07:24:46,638] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
worker3: [2024-06-14 07:24:46,639] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
worker3: [2024-06-14 07:24:46,705] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
worker1: [2024-06-14 07:24:46,756] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
worker2: [2024-06-14 07:24:46,800] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
worker2: [2024-06-14 07:24:46,802] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
worker2: [2024-06-14 07:24:46,810] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
worker2: [2024-06-14 07:24:46,825] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
worker1: [2024-06-14 07:24:46,831] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
worker1: [2024-06-14 07:24:46,862] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
worker1: [2024-06-14 07:24:46,881] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
head:   warnings.warn(
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
head:   warnings.warn(
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
head:   warnings.warn(
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
head:   warnings.warn(
head: [2024-06-14 07:24:48,458] [INFO] [comm.py:637:init_distributed] cdb=None
head: [2024-06-14 07:24:48,497] [INFO] [comm.py:637:init_distributed] cdb=None
head: [2024-06-14 07:24:48,531] [INFO] [comm.py:637:init_distributed] cdb=None
head: [2024-06-14 07:24:48,537] [INFO] [comm.py:637:init_distributed] cdb=None
worker3: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
worker3:   warnings.warn(
worker3: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
worker3:   warnings.warn(
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
worker1:   warnings.warn(
worker3: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
worker3:   warnings.warn(
worker3: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
worker3:   warnings.warn(
worker2: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
worker2:   warnings.warn(
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
worker1:   warnings.warn(
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
worker1:   warnings.warn(
worker2: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
worker2:   warnings.warn(
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
worker1:   warnings.warn(
worker3: [2024-06-14 07:24:48,814] [INFO] [comm.py:637:init_distributed] cdb=None
worker2: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
worker2:   warnings.warn(
worker3: [2024-06-14 07:24:48,879] [INFO] [comm.py:637:init_distributed] cdb=None
worker1: [2024-06-14 07:24:48,886] [INFO] [comm.py:637:init_distributed] cdb=None
worker2: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
worker2:   warnings.warn(
worker3: [2024-06-14 07:24:48,940] [INFO] [comm.py:637:init_distributed] cdb=None
worker3: [2024-06-14 07:24:49,003] [INFO] [comm.py:637:init_distributed] cdb=None
worker1: [2024-06-14 07:24:49,050] [INFO] [comm.py:637:init_distributed] cdb=None
worker2: [2024-06-14 07:24:49,070] [INFO] [comm.py:637:init_distributed] cdb=None
worker1: [2024-06-14 07:24:49,071] [INFO] [comm.py:637:init_distributed] cdb=None
worker2: [2024-06-14 07:24:49,088] [INFO] [comm.py:637:init_distributed] cdb=None
worker1: [2024-06-14 07:24:49,104] [INFO] [comm.py:637:init_distributed] cdb=None
worker1: [2024-06-14 07:24:49,104] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
worker2: [2024-06-14 07:24:49,142] [INFO] [comm.py:637:init_distributed] cdb=None
worker2: [2024-06-14 07:24:49,236] [INFO] [comm.py:637:init_distributed] cdb=None
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:283112 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:283112 [0] NCCL INFO Bootstrap : Using eth1:172.28.32.135<0>
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:283112 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:283112 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:283112 [0] NCCL INFO cudaDriverVersion 11080
worker1: NCCL version 2.18.6+cuda11.8
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283113:283113 [1] NCCL INFO cudaDriverVersion 11080
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190805:190805 [1] NCCL INFO cudaDriverVersion 11080
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:163002 [2] NCCL INFO cudaDriverVersion 11080
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:170620 [0] NCCL INFO cudaDriverVersion 11080
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283113:283113 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190807:190807 [3] NCCL INFO cudaDriverVersion 11080
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:190806 [2] NCCL INFO cudaDriverVersion 11080
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:190804 [0] NCCL INFO cudaDriverVersion 11080
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163003:163003 [3] NCCL INFO cudaDriverVersion 11080
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163001:163001 [1] NCCL INFO cudaDriverVersion 11080
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:163000 [0] NCCL INFO cudaDriverVersion 11080
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170623:170623 [3] NCCL INFO cudaDriverVersion 11080
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170621:170621 [1] NCCL INFO cudaDriverVersion 11080
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:170622 [2] NCCL INFO cudaDriverVersion 11080
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:163002 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163003:163003 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:163000 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190807:190807 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163001:163001 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283113:283113 [1] NCCL INFO Bootstrap : Using eth1:172.28.32.135<0>
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170623:170623 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:170620 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170621:170621 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:190804 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190805:190805 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:190806 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:170622 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283113:283113 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283113:283113 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:163002 [2] NCCL INFO Bootstrap : Using eth1:172.28.16.13<0>
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163003:163003 [3] NCCL INFO Bootstrap : Using eth1:172.28.16.13<0>
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:163000 [0] NCCL INFO Bootstrap : Using eth1:172.28.16.13<0>
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163001:163001 [1] NCCL INFO Bootstrap : Using eth1:172.28.16.13<0>
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190805:190805 [1] NCCL INFO Bootstrap : Using eth1:172.28.32.131<0>
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:190804 [0] NCCL INFO Bootstrap : Using eth1:172.28.32.131<0>
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190807:190807 [3] NCCL INFO Bootstrap : Using eth1:172.28.32.131<0>
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:190806 [2] NCCL INFO Bootstrap : Using eth1:172.28.32.131<0>
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170623:170623 [3] NCCL INFO Bootstrap : Using eth1:172.28.16.12<0>
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:170620 [0] NCCL INFO Bootstrap : Using eth1:172.28.16.12<0>
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170621:170621 [1] NCCL INFO Bootstrap : Using eth1:172.28.16.12<0>
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:170622 [2] NCCL INFO Bootstrap : Using eth1:172.28.16.12<0>
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163003:163003 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:163000 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:163002 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163001:163001 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163003:163003 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:163000 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:163002 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163001:163001 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190805:190805 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:190806 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190807:190807 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:190804 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190805:190805 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190807:190807 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:190806 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:190804 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:170620 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170623:170623 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:170620 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170623:170623 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170621:170621 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:170622 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170621:170621 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:170622 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283115:283115 [3] NCCL INFO cudaDriverVersion 11080
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283115:283115 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283115:283115 [3] NCCL INFO Bootstrap : Using eth1:172.28.32.135<0>
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283115:283115 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283115:283115 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:283891 [0] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:283891 [0] NCCL INFO P2P plugin IBext
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:283891 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283113:283892 [1] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283113:283892 [1] NCCL INFO P2P plugin IBext
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283113:283892 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283115:283893 [3] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283115:283893 [3] NCCL INFO P2P plugin IBext
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283115:283893 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163003:163768 [3] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163003:163768 [3] NCCL INFO P2P plugin IBext
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163003:163768 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:163769 [2] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:163769 [2] NCCL INFO P2P plugin IBext
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:163769 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:163770 [0] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:163770 [0] NCCL INFO P2P plugin IBext
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:163770 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163001:163771 [1] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163001:163771 [1] NCCL INFO P2P plugin IBext
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170623:171399 [3] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170623:171399 [3] NCCL INFO P2P plugin IBext
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163001:163771 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170623:171399 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:171400 [0] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:171400 [0] NCCL INFO P2P plugin IBext
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:171400 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:171402 [2] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:171402 [2] NCCL INFO P2P plugin IBext
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:171402 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170621:171401 [1] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170621:171401 [1] NCCL INFO P2P plugin IBext
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170621:171401 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:283114 [2] NCCL INFO cudaDriverVersion 11080
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:283114 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:283114 [2] NCCL INFO Bootstrap : Using eth1:172.28.32.135<0>
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:283114 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:283114 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190807:191612 [3] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190807:191612 [3] NCCL INFO P2P plugin IBext
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190807:191612 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:191613 [2] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:191613 [2] NCCL INFO P2P plugin IBext
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:191613 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:191614 [0] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:191614 [0] NCCL INFO P2P plugin IBext
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:191614 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190805:191615 [1] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190805:191615 [1] NCCL INFO P2P plugin IBext
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190805:191615 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:283896 [2] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:283896 [2] NCCL INFO P2P plugin IBext
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:283896 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283115:283893 [3] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283115:283893 [3] NCCL INFO NET/IB : Using [0]mlx5_1:1/RoCE [1]mlx5_2:1/RoCE [2]mlx5_3:1/RoCE [3]mlx5_4:1/RoCE [RO]; OOB eth1:172.28.32.135<0>
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283115:283893 [3] NCCL INFO Using network IBext
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283113:283892 [1] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283113:283892 [1] NCCL INFO NET/IB : Using [0]mlx5_1:1/RoCE [1]mlx5_2:1/RoCE [2]mlx5_3:1/RoCE [3]mlx5_4:1/RoCE [RO]; OOB eth1:172.28.32.135<0>
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283113:283892 [1] NCCL INFO Using network IBext
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:171402 [2] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:171402 [2] NCCL INFO NET/IB : Using [0]mlx5_1:1/RoCE [1]mlx5_2:1/RoCE [2]mlx5_3:1/RoCE [3]mlx5_4:1/RoCE [RO]; OOB eth1:172.28.16.12<0>
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:171402 [2] NCCL INFO Using network IBext
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170621:171401 [1] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170621:171401 [1] NCCL INFO NET/IB : Using [0]mlx5_1:1/RoCE [1]mlx5_2:1/RoCE [2]mlx5_3:1/RoCE [3]mlx5_4:1/RoCE [RO]; OOB eth1:172.28.16.12<0>
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170621:171401 [1] NCCL INFO Using network IBext
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:171400 [0] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:171400 [0] NCCL INFO NET/IB : Using [0]mlx5_1:1/RoCE [1]mlx5_2:1/RoCE [2]mlx5_3:1/RoCE [3]mlx5_4:1/RoCE [RO]; OOB eth1:172.28.16.12<0>
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:171400 [0] NCCL INFO Using network IBext
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:283896 [2] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:283891 [0] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:283891 [0] NCCL INFO NET/IB : Using [0]mlx5_1:1/RoCE [1]mlx5_2:1/RoCE [2]mlx5_3:1/RoCE [3]mlx5_4:1/RoCE [RO]; OOB eth1:172.28.32.135<0>
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:283896 [2] NCCL INFO NET/IB : Using [0]mlx5_1:1/RoCE [1]mlx5_2:1/RoCE [2]mlx5_3:1/RoCE [3]mlx5_4:1/RoCE [RO]; OOB eth1:172.28.32.135<0>
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:283891 [0] NCCL INFO Using network IBext
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:283896 [2] NCCL INFO Using network IBext
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:163769 [2] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:163769 [2] NCCL INFO NET/IB : Using [0]mlx5_1:1/RoCE [1]mlx5_2:1/RoCE [2]mlx5_3:1/RoCE [3]mlx5_4:1/RoCE [RO]; OOB eth1:172.28.16.13<0>
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:163769 [2] NCCL INFO Using network IBext
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163003:163768 [3] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163001:163771 [1] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163003:163768 [3] NCCL INFO NET/IB : Using [0]mlx5_1:1/RoCE [1]mlx5_2:1/RoCE [2]mlx5_3:1/RoCE [3]mlx5_4:1/RoCE [RO]; OOB eth1:172.28.16.13<0>
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163003:163768 [3] NCCL INFO Using network IBext
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:163770 [0] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163001:163771 [1] NCCL INFO NET/IB : Using [0]mlx5_1:1/RoCE [1]mlx5_2:1/RoCE [2]mlx5_3:1/RoCE [3]mlx5_4:1/RoCE [RO]; OOB eth1:172.28.16.13<0>
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163001:163771 [1] NCCL INFO Using network IBext
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:163770 [0] NCCL INFO NET/IB : Using [0]mlx5_1:1/RoCE [1]mlx5_2:1/RoCE [2]mlx5_3:1/RoCE [3]mlx5_4:1/RoCE [RO]; OOB eth1:172.28.16.13<0>
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:163770 [0] NCCL INFO Using network IBext
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:191613 [2] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190805:191615 [1] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:191613 [2] NCCL INFO NET/IB : Using [0]mlx5_1:1/RoCE [1]mlx5_2:1/RoCE [2]mlx5_3:1/RoCE [3]mlx5_4:1/RoCE [RO]; OOB eth1:172.28.32.131<0>
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:191613 [2] NCCL INFO Using network IBext
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190805:191615 [1] NCCL INFO NET/IB : Using [0]mlx5_1:1/RoCE [1]mlx5_2:1/RoCE [2]mlx5_3:1/RoCE [3]mlx5_4:1/RoCE [RO]; OOB eth1:172.28.32.131<0>
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190805:191615 [1] NCCL INFO Using network IBext
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190807:191612 [3] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190807:191612 [3] NCCL INFO NET/IB : Using [0]mlx5_1:1/RoCE [1]mlx5_2:1/RoCE [2]mlx5_3:1/RoCE [3]mlx5_4:1/RoCE [RO]; OOB eth1:172.28.32.131<0>
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190807:191612 [3] NCCL INFO Using network IBext
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170623:171399 [3] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170623:171399 [3] NCCL INFO NET/IB : Using [0]mlx5_1:1/RoCE [1]mlx5_2:1/RoCE [2]mlx5_3:1/RoCE [3]mlx5_4:1/RoCE [RO]; OOB eth1:172.28.16.12<0>
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170623:171399 [3] NCCL INFO Using network IBext
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:191614 [0] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:191614 [0] NCCL INFO NET/IB : Using [0]mlx5_1:1/RoCE [1]mlx5_2:1/RoCE [2]mlx5_3:1/RoCE [3]mlx5_4:1/RoCE [RO]; OOB eth1:172.28.32.131<0>
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:191614 [0] NCCL INFO Using network IBext
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190807:191612 [3] NCCL INFO comm 0x556b38dfa900 rank 15 nranks 16 cudaDev 3 nvmlDev 3 busId 6a000 commId 0x5336c5f13c05193c - Init START
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:191613 [2] NCCL INFO comm 0x55eb7fe44ce0 rank 14 nranks 16 cudaDev 2 nvmlDev 2 busId 65000 commId 0x5336c5f13c05193c - Init START
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190805:191615 [1] NCCL INFO comm 0x5605dee365c0 rank 13 nranks 16 cudaDev 1 nvmlDev 1 busId 46000 commId 0x5336c5f13c05193c - Init START
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:191614 [0] NCCL INFO comm 0x5559e45ba630 rank 12 nranks 16 cudaDev 0 nvmlDev 0 busId 40000 commId 0x5336c5f13c05193c - Init START
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:283891 [0] NCCL INFO comm 0x557126fed430 rank 0 nranks 16 cudaDev 0 nvmlDev 0 busId 40000 commId 0x5336c5f13c05193c - Init START
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283115:283893 [3] NCCL INFO comm 0x55d30f58df10 rank 3 nranks 16 cudaDev 3 nvmlDev 3 busId 6a000 commId 0x5336c5f13c05193c - Init START
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:283896 [2] NCCL INFO comm 0x55c4b4d057b0 rank 2 nranks 16 cudaDev 2 nvmlDev 2 busId 65000 commId 0x5336c5f13c05193c - Init START
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283113:283892 [1] NCCL INFO comm 0x560292766de0 rank 1 nranks 16 cudaDev 1 nvmlDev 1 busId 46000 commId 0x5336c5f13c05193c - Init START
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170623:171399 [3] NCCL INFO comm 0x562f3fdbf3c0 rank 11 nranks 16 cudaDev 3 nvmlDev 3 busId 6f000 commId 0x5336c5f13c05193c - Init START
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:163769 [2] NCCL INFO comm 0x564ea6e2b2c0 rank 6 nranks 16 cudaDev 2 nvmlDev 2 busId 69000 commId 0x5336c5f13c05193c - Init START
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163001:163771 [1] NCCL INFO comm 0x564bd6987960 rank 5 nranks 16 cudaDev 1 nvmlDev 1 busId 52000 commId 0x5336c5f13c05193c - Init START
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163003:163768 [3] NCCL INFO comm 0x55ea84953ad0 rank 7 nranks 16 cudaDev 3 nvmlDev 3 busId 6f000 commId 0x5336c5f13c05193c - Init START
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:163770 [0] NCCL INFO comm 0x5564391fad90 rank 4 nranks 16 cudaDev 0 nvmlDev 0 busId 4d000 commId 0x5336c5f13c05193c - Init START
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:171400 [0] NCCL INFO comm 0x55e6b1c71c90 rank 8 nranks 16 cudaDev 0 nvmlDev 0 busId 4d000 commId 0x5336c5f13c05193c - Init START
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:171402 [2] NCCL INFO comm 0x55638cf2a4b0 rank 10 nranks 16 cudaDev 2 nvmlDev 2 busId 69000 commId 0x5336c5f13c05193c - Init START
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170621:171401 [1] NCCL INFO comm 0x55f543b2c590 rank 9 nranks 16 cudaDev 1 nvmlDev 1 busId 52000 commId 0x5336c5f13c05193c - Init START
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:283891 [0] NCCL INFO NCCL_TOPO_FILE set by environment to /var/run/nvidia-topologyd/virtualTopology.xml
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283115:283893 [3] NCCL INFO NCCL_TOPO_FILE set by environment to /var/run/nvidia-topologyd/virtualTopology.xml
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283113:283892 [1] NCCL INFO NCCL_TOPO_FILE set by environment to /var/run/nvidia-topologyd/virtualTopology.xml
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:283896 [2] NCCL INFO NCCL_TOPO_FILE set by environment to /var/run/nvidia-topologyd/virtualTopology.xml
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170623:171399 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,00000000,ffffffff
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:171400 [0] NCCL INFO Setting affinity for GPU 0 to ffffffff,00000000,ffffffff
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:171402 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff,00000000,ffffffff
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170621:171401 [1] NCCL INFO Setting affinity for GPU 1 to ffffffff,00000000,ffffffff
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:163769 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff,00000000,ffffffff
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:163770 [0] NCCL INFO Setting affinity for GPU 0 to ffffffff,00000000,ffffffff
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163001:163771 [1] NCCL INFO Setting affinity for GPU 1 to ffffffff,00000000,ffffffff
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163003:163768 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,00000000,ffffffff
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190805:191615 [1] NCCL INFO Setting affinity for GPU 1 to ffffffff,00000000,ffffffff
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:191613 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff,00000000,ffffffff
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190807:191612 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,00000000,ffffffff
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:191614 [0] NCCL INFO Setting affinity for GPU 0 to ffffffff,00000000,ffffffff
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283115:283893 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,00000000,ffffffff
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:283891 [0] NCCL INFO Setting affinity for GPU 0 to ffffffff,00000000,ffffffff
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283113:283892 [1] NCCL INFO Setting affinity for GPU 1 to ffffffff,00000000,ffffffff
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:283896 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff,00000000,ffffffff
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:283896 [2] NCCL INFO Trees [0] 3/8/-1->2->-1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->4 [3] 3/-1/-1->2->1
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283115:283893 [3] NCCL INFO Trees [0] 0/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] 0/-1/-1->3->2 [3] -1/-1/-1->3->2
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:283896 [2] NCCL INFO P2P Chunksize set to 131072
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283115:283893 [3] NCCL INFO P2P Chunksize set to 131072
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283113:283892 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] -1/-1/-1->1->0 [3] 2/-1/-1->1->0
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283113:283892 [1] NCCL INFO P2P Chunksize set to 131072
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:163770 [0] NCCL INFO Trees [0] 5/-1/-1->4->9 [1] 5/-1/-1->4->7 [2] 5/2/-1->4->12 [3] 5/-1/-1->4->7
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163001:163771 [1] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] -1/-1/-1->5->4 [2] 6/8/-1->5->4 [3] -1/-1/-1->5->4
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170623:171399 [3] NCCL INFO Trees [0] -1/-1/-1->11->10 [1] 8/6/-1->11->10 [2] -1/-1/-1->11->10 [3] 8/-1/-1->11->10
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:283891 [0] NCCL INFO Channel 00/04 :    0   3   4   7   6   5   8  11  10   9  12  15  14  13   2   1
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190807:191612 [3] NCCL INFO Trees [0] -1/-1/-1->15->14 [1] 12/-1/-1->15->14 [2] -1/-1/-1->15->14 [3] 12/-1/-1->15->14
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:163770 [0] NCCL INFO P2P Chunksize set to 131072
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:283891 [0] NCCL INFO Channel 01/04 :    0   3   2   1   6   5   4   7  10   9   8  11  14  13  12  15
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163001:163771 [1] NCCL INFO P2P Chunksize set to 131072
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:163769 [2] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->11 [2] 7/-1/-1->6->5 [3] 7/0/-1->6->14
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163003:163768 [3] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] 4/-1/-1->7->6 [2] -1/-1/-1->7->6 [3] 4/10/-1->7->6
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:163769 [2] NCCL INFO P2P Chunksize set to 131072
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163003:163768 [3] NCCL INFO P2P Chunksize set to 131072
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:171400 [0] NCCL INFO Trees [0] 9/12/-1->8->2 [1] 9/-1/-1->8->11 [2] 9/-1/-1->8->5 [3] 9/-1/-1->8->11
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:283891 [0] NCCL INFO Channel 02/04 :    0   3   4   7   6   5   8  11  10   9  12  15  14  13   2   1
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:191613 [2] NCCL INFO Trees [0] 15/-1/-1->14->13 [1] 15/-1/-1->14->10 [2] 15/-1/-1->14->13 [3] 15/6/-1->14->-1
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:283891 [0] NCCL INFO Channel 03/04 :    0   3   2   1   6   5   4   7  10   9   8  11  14  13  12  15
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:191613 [2] NCCL INFO P2P Chunksize set to 131072
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:283891 [0] NCCL INFO Trees [0] 1/-1/-1->0->3 [1] 1/10/-1->0->-1 [2] 1/-1/-1->0->3 [3] 1/-1/-1->0->6
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190807:191612 [3] NCCL INFO P2P Chunksize set to 131072
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:283891 [0] NCCL INFO P2P Chunksize set to 131072
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:191614 [0] NCCL INFO Trees [0] 13/-1/-1->12->8 [1] 13/-1/-1->12->15 [2] 13/4/-1->12->-1 [3] 13/-1/-1->12->15
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:191614 [0] NCCL INFO P2P Chunksize set to 131072
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190805:191615 [1] NCCL INFO Trees [0] 14/-1/-1->13->12 [1] -1/-1/-1->13->12 [2] 14/-1/-1->13->12 [3] -1/-1/-1->13->12
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190805:191615 [1] NCCL INFO P2P Chunksize set to 131072
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170621:171401 [1] NCCL INFO Trees [0] 10/4/-1->9->8 [1] -1/-1/-1->9->8 [2] 10/-1/-1->9->8 [3] -1/-1/-1->9->8
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170623:171399 [3] NCCL INFO P2P Chunksize set to 131072
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:171400 [0] NCCL INFO P2P Chunksize set to 131072
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170621:171401 [1] NCCL INFO P2P Chunksize set to 131072
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:171402 [2] NCCL INFO Trees [0] 11/-1/-1->10->9 [1] 11/14/-1->10->0 [2] 11/-1/-1->10->9 [3] 11/-1/-1->10->7
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:171402 [2] NCCL INFO P2P Chunksize set to 131072
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283113:283892 [1] NCCL INFO Channel 01/0 : 1[1] -> 6[2] [send] via NET/IBext/0/GDRDMA
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:191613 [2] NCCL INFO Channel 01/0 : 11[3] -> 14[2] [receive] via NET/IBext/1/GDRDMA
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:191614 [0] NCCL INFO Channel 00/0 : 9[1] -> 12[0] [receive] via NET/IBext/0/GDRDMA
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283113:283892 [1] NCCL INFO Channel 03/0 : 1[1] -> 6[2] [send] via NET/IBext/0/GDRDMA
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:171400 [0] NCCL INFO Channel 00/0 : 5[1] -> 8[0] [receive] via NET/IBext/0/GDRDMA
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:191613 [2] NCCL INFO Channel 03/0 : 11[3] -> 14[2] [receive] via NET/IBext/1/GDRDMA
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163001:163771 [1] NCCL INFO Channel 00/0 : 5[1] -> 8[0] [send] via NET/IBext/0/GDRDMA
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:191614 [0] NCCL INFO Channel 02/0 : 9[1] -> 12[0] [receive] via NET/IBext/0/GDRDMA
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:191614 [0] NCCL INFO Channel 00/0 : 12[0] -> 15[3] via P2P/IPC/read
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:171400 [0] NCCL INFO Channel 02/0 : 5[1] -> 8[0] [receive] via NET/IBext/0/GDRDMA
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:163770 [0] NCCL INFO Channel 00/0 : 3[3] -> 4[0] [receive] via NET/IBext/0/GDRDMA
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:171400 [0] NCCL INFO Channel 00/0 : 8[0] -> 11[3] via P2P/IPC/read
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170621:171401 [1] NCCL INFO Channel 00/0 : 9[1] -> 12[0] [send] via NET/IBext/0/GDRDMA
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163001:163771 [1] NCCL INFO Channel 02/0 : 5[1] -> 8[0] [send] via NET/IBext/0/GDRDMA
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:163770 [0] NCCL INFO Channel 02/0 : 3[3] -> 4[0] [receive] via NET/IBext/0/GDRDMA
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:283891 [0] NCCL INFO Channel 01/0 : 15[3] -> 0[0] [receive] via NET/IBext/0/GDRDMA
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170621:171401 [1] NCCL INFO Channel 02/0 : 9[1] -> 12[0] [send] via NET/IBext/0/GDRDMA
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190805:191615 [1] NCCL INFO Channel 00/0 : 13[1] -> 2[2] [send] via NET/IBext/0/GDRDMA
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:171402 [2] NCCL INFO Channel 01/0 : 7[3] -> 10[2] [receive] via NET/IBext/1/GDRDMA
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190807:191612 [3] NCCL INFO Channel 01/0 : 15[3] -> 0[0] [send] via NET/IBext/1/GDRDMA
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:283891 [0] NCCL INFO Channel 03/0 : 15[3] -> 0[0] [receive] via NET/IBext/0/GDRDMA
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190805:191615 [1] NCCL INFO Channel 02/0 : 13[1] -> 2[2] [send] via NET/IBext/0/GDRDMA
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:171402 [2] NCCL INFO Channel 03/0 : 7[3] -> 10[2] [receive] via NET/IBext/1/GDRDMA
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190807:191612 [3] NCCL INFO Channel 03/0 : 15[3] -> 0[0] [send] via NET/IBext/1/GDRDMA
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:191614 [0] NCCL INFO Channel 01/0 : 12[0] -> 15[3] via P2P/IPC/read
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:283891 [0] NCCL INFO Channel 00/0 : 0[0] -> 3[3] via P2P/IPC/read
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:171400 [0] NCCL INFO Channel 01/0 : 8[0] -> 11[3] via P2P/IPC/read
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283115:283893 [3] NCCL INFO Channel 00/0 : 3[3] -> 4[0] [send] via NET/IBext/1/GDRDMA
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:283896 [2] NCCL INFO Channel 00/0 : 13[1] -> 2[2] [receive] via NET/IBext/1/GDRDMA
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283115:283893 [3] NCCL INFO Channel 02/0 : 3[3] -> 4[0] [send] via NET/IBext/1/GDRDMA
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:283896 [2] NCCL INFO Channel 02/0 : 13[1] -> 2[2] [receive] via NET/IBext/1/GDRDMA
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:171400 [0] NCCL INFO Channel 02/0 : 8[0] -> 11[3] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:163770 [0] NCCL INFO Channel 00/0 : 4[0] -> 7[3] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:163769 [2] NCCL INFO Channel 01/0 : 1[1] -> 6[2] [receive] via NET/IBext/1/GDRDMA
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:191614 [0] NCCL INFO Channel 02/0 : 12[0] -> 15[3] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:163769 [2] NCCL INFO Channel 03/0 : 1[1] -> 6[2] [receive] via NET/IBext/1/GDRDMA
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:171400 [0] NCCL INFO Channel 03/0 : 8[0] -> 11[3] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:163770 [0] NCCL INFO Channel 01/0 : 4[0] -> 7[3] via P2P/IPC/read
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:191614 [0] NCCL INFO Channel 03/0 : 12[0] -> 15[3] via P2P/IPC/read
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170623:171399 [3] NCCL INFO Channel 01/0 : 11[3] -> 14[2] [send] via NET/IBext/1/GDRDMA
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170623:171399 [3] NCCL INFO Channel 03/0 : 11[3] -> 14[2] [send] via NET/IBext/1/GDRDMA
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:163770 [0] NCCL INFO Channel 02/0 : 4[0] -> 7[3] via P2P/IPC/read
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170623:171399 [3] NCCL INFO Channel 00/0 : 11[3] -> 10[2] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163003:163768 [3] NCCL INFO Channel 01/0 : 7[3] -> 10[2] [send] via NET/IBext/1/GDRDMA
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170623:171399 [3] NCCL INFO Channel 02/0 : 11[3] -> 10[2] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163003:163768 [3] NCCL INFO Channel 03/0 : 7[3] -> 10[2] [send] via NET/IBext/1/GDRDMA
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:163770 [0] NCCL INFO Channel 03/0 : 4[0] -> 7[3] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:163769 [2] NCCL INFO Channel 00/0 : 6[2] -> 5[1] via P2P/IPC/read
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:283891 [0] NCCL INFO Channel 01/0 : 0[0] -> 3[3] via P2P/IPC/read
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190807:191612 [3] NCCL INFO Channel 00/0 : 15[3] -> 14[2] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:163769 [2] NCCL INFO Channel 01/0 : 6[2] -> 5[1] via P2P/IPC/read
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:191613 [2] NCCL INFO Channel 00/0 : 14[2] -> 13[1] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163003:163768 [3] NCCL INFO Channel 00/0 : 7[3] -> 6[2] via P2P/IPC/read
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:283891 [0] NCCL INFO Channel 02/0 : 0[0] -> 3[3] via P2P/IPC/read
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190805:191615 [1] NCCL INFO Channel 01/0 : 13[1] -> 12[0] via P2P/IPC/read
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:171402 [2] NCCL INFO Channel 00/0 : 10[2] -> 9[1] via P2P/IPC/read
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190807:191612 [3] NCCL INFO Channel 02/0 : 15[3] -> 14[2] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:163769 [2] NCCL INFO Channel 02/0 : 6[2] -> 5[1] via P2P/IPC/read
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:191613 [2] NCCL INFO Channel 01/0 : 14[2] -> 13[1] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163003:163768 [3] NCCL INFO Channel 02/0 : 7[3] -> 6[2] via P2P/IPC/read
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:171402 [2] NCCL INFO Channel 01/0 : 10[2] -> 9[1] via P2P/IPC/read
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:283896 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163001:163771 [1] NCCL INFO Channel 01/0 : 5[1] -> 4[0] via P2P/IPC/read
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:283891 [0] NCCL INFO Channel 03/0 : 0[0] -> 3[3] via P2P/IPC/read
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170621:171401 [1] NCCL INFO Channel 01/0 : 9[1] -> 8[0] via P2P/IPC/read
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190805:191615 [1] NCCL INFO Channel 03/0 : 13[1] -> 12[0] via P2P/IPC/read
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:171402 [2] NCCL INFO Channel 02/0 : 10[2] -> 9[1] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:163769 [2] NCCL INFO Channel 03/0 : 6[2] -> 5[1] via P2P/IPC/read
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170621:171401 [1] NCCL INFO Channel 03/0 : 9[1] -> 8[0] via P2P/IPC/read
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:191613 [2] NCCL INFO Channel 02/0 : 14[2] -> 13[1] via P2P/IPC/read
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:171402 [2] NCCL INFO Channel 03/0 : 10[2] -> 9[1] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163001:163771 [1] NCCL INFO Channel 03/0 : 5[1] -> 4[0] via P2P/IPC/read
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:283896 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/IPC/read
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:191613 [2] NCCL INFO Channel 03/0 : 14[2] -> 13[1] via P2P/IPC/read
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283113:283892 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/IPC/read
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:283896 [2] NCCL INFO Channel 02/0 : 2[2] -> 1[1] via P2P/IPC/read
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283115:283893 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/IPC/read
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283113:283892 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/IPC/read
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170621:171422 [1] NCCL INFO NCCL_IB_GID_INDEX set by environment to 7.
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163001:163789 [1] NCCL INFO NCCL_IB_GID_INDEX set by environment to 7.
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:191634 [0] NCCL INFO NCCL_IB_GID_INDEX set by environment to 7.
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:283896 [2] NCCL INFO Channel 03/0 : 2[2] -> 1[1] via P2P/IPC/read
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:171419 [0] NCCL INFO NCCL_IB_GID_INDEX set by environment to 7.
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283115:283893 [3] NCCL INFO Channel 03/0 : 3[3] -> 2[2] via P2P/IPC/read
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190807:191635 [3] NCCL INFO NCCL_IB_GID_INDEX set by environment to 7.
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190805:191636 [1] NCCL INFO NCCL_IB_GID_INDEX set by environment to 7.
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163003:163791 [3] NCCL INFO NCCL_IB_GID_INDEX set by environment to 7.
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:171421 [2] NCCL INFO NCCL_IB_GID_INDEX set by environment to 7.
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:283914 [0] NCCL INFO NCCL_IB_GID_INDEX set by environment to 7.
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:283911 [2] NCCL INFO NCCL_IB_GID_INDEX set by environment to 7.
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170623:171420 [3] NCCL INFO NCCL_IB_GID_INDEX set by environment to 7.
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:191633 [2] NCCL INFO NCCL_IB_GID_INDEX set by environment to 7.
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:171419 [0] NCCL INFO NCCL_IB_TIMEOUT set by environment to 23.
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:191634 [0] NCCL INFO NCCL_IB_TIMEOUT set by environment to 23.
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:191634 [0] NCCL INFO NCCL_IB_RETRY_CNT set by environment to 7.
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:171419 [0] NCCL INFO NCCL_IB_RETRY_CNT set by environment to 7.
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283113:283912 [1] NCCL INFO NCCL_IB_GID_INDEX set by environment to 7.
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:163792 [2] NCCL INFO NCCL_IB_GID_INDEX set by environment to 7.
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283115:283913 [3] NCCL INFO NCCL_IB_GID_INDEX set by environment to 7.
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:171421 [2] NCCL INFO NCCL_IB_TIMEOUT set by environment to 23.
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:171421 [2] NCCL INFO NCCL_IB_RETRY_CNT set by environment to 7.
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:163790 [0] NCCL INFO NCCL_IB_GID_INDEX set by environment to 7.
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:283914 [0] NCCL INFO NCCL_IB_TIMEOUT set by environment to 23.
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:283914 [0] NCCL INFO NCCL_IB_RETRY_CNT set by environment to 7.
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:191633 [2] NCCL INFO NCCL_IB_TIMEOUT set by environment to 23.
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:191633 [2] NCCL INFO NCCL_IB_RETRY_CNT set by environment to 7.
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:283911 [2] NCCL INFO NCCL_IB_TIMEOUT set by environment to 23.
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:283911 [2] NCCL INFO NCCL_IB_RETRY_CNT set by environment to 7.
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:163792 [2] NCCL INFO NCCL_IB_TIMEOUT set by environment to 23.
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:163792 [2] NCCL INFO NCCL_IB_RETRY_CNT set by environment to 7.
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:163790 [0] NCCL INFO NCCL_IB_TIMEOUT set by environment to 23.
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:163790 [0] NCCL INFO NCCL_IB_RETRY_CNT set by environment to 7.
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163003:163768 [3] NCCL INFO Connected all rings
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170621:171401 [1] NCCL INFO Connected all rings
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163001:163771 [1] NCCL INFO Connected all rings
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190807:191612 [3] NCCL INFO Connected all rings
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170623:171399 [3] NCCL INFO Connected all rings
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190805:191615 [1] NCCL INFO Connected all rings
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163001:163771 [1] NCCL INFO Channel 00/0 : 5[1] -> 6[2] via P2P/IPC/read
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170621:171401 [1] NCCL INFO Channel 00/0 : 9[1] -> 10[2] via P2P/IPC/read
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283115:283893 [3] NCCL INFO Connected all rings
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163001:163771 [1] NCCL INFO Channel 02/0 : 5[1] -> 6[2] via P2P/IPC/read
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283113:283892 [1] NCCL INFO Connected all rings
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170621:171401 [1] NCCL INFO Channel 02/0 : 9[1] -> 10[2] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:163769 [2] NCCL INFO Connected all rings
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:171400 [0] NCCL INFO Connected all rings
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:171400 [0] NCCL INFO Channel 00/0 : 8[0] -> 9[1] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:163770 [0] NCCL INFO Connected all rings
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:163770 [0] NCCL INFO Channel 00/0 : 4[0] -> 5[1] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:163769 [2] NCCL INFO Channel 00/0 : 6[2] -> 7[3] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:163770 [0] NCCL INFO Channel 01/0 : 4[0] -> 5[1] via P2P/IPC/read
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:171400 [0] NCCL INFO Channel 01/0 : 8[0] -> 9[1] via P2P/IPC/read
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:171402 [2] NCCL INFO Connected all rings
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:191614 [0] NCCL INFO Connected all rings
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:191614 [0] NCCL INFO Channel 00/0 : 12[0] -> 13[1] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:163769 [2] NCCL INFO Channel 01/0 : 6[2] -> 7[3] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:163770 [0] NCCL INFO Channel 02/0 : 4[0] -> 5[1] via P2P/IPC/read
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:171400 [0] NCCL INFO Channel 02/0 : 8[0] -> 9[1] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:163769 [2] NCCL INFO Channel 02/0 : 6[2] -> 7[3] via P2P/IPC/read
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:171400 [0] NCCL INFO Channel 03/0 : 8[0] -> 9[1] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:163770 [0] NCCL INFO Channel 03/0 : 4[0] -> 5[1] via P2P/IPC/read
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:171402 [2] NCCL INFO Channel 00/0 : 10[2] -> 11[3] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:163769 [2] NCCL INFO Channel 03/0 : 6[2] -> 7[3] via P2P/IPC/read
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190805:191615 [1] NCCL INFO Channel 00/0 : 13[1] -> 14[2] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:163770 [0] NCCL INFO Channel 02/0 : 2[2] -> 4[0] [receive] via NET/IBext/0/GDRDMA
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:171400 [0] NCCL INFO Channel 00/0 : 8[0] -> 12[0] [send] via NET/IBext/0/GDRDMA
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:171402 [2] NCCL INFO Channel 01/0 : 10[2] -> 11[3] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163001:163771 [1] NCCL INFO Channel 02/0 : 8[0] -> 5[1] [receive] via NET/IBext/0/GDRDMA
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:171402 [2] NCCL INFO Channel 02/0 : 10[2] -> 11[3] via P2P/IPC/read
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:191614 [0] NCCL INFO Channel 01/0 : 12[0] -> 13[1] via P2P/IPC/read
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:171402 [2] NCCL INFO Channel 03/0 : 10[2] -> 11[3] via P2P/IPC/read
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170621:171401 [1] NCCL INFO Channel 00/0 : 4[0] -> 9[1] [receive] via NET/IBext/0/GDRDMA
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170623:171399 [3] NCCL INFO Channel 01/0 : 6[2] -> 11[3] [receive] via NET/IBext/1/GDRDMA
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:171402 [2] NCCL INFO Channel 01/0 : 10[2] -> 14[2] [send] via NET/IBext/1/GDRDMA
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190805:191615 [1] NCCL INFO Channel 02/0 : 13[1] -> 14[2] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:163769 [2] NCCL INFO Channel 01/0 : 6[2] -> 11[3] [send] via NET/IBext/1/GDRDMA
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163003:163768 [3] NCCL INFO Channel 03/0 : 10[2] -> 7[3] [receive] via NET/IBext/1/GDRDMA
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163003:163768 [3] NCCL INFO Channel 01/0 : 7[3] -> 4[0] via P2P/IPC/read
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:283896 [2] NCCL INFO Connected all rings
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:163769 [2] NCCL INFO Channel 03/0 : 0[0] -> 6[2] [receive] via NET/IBext/1/GDRDMA
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170623:171399 [3] NCCL INFO Channel 01/0 : 11[3] -> 6[2] [send] via NET/IBext/1/GDRDMA
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283113:283892 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163003:163768 [3] NCCL INFO Channel 03/0 : 7[3] -> 4[0] via P2P/IPC/read
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:283891 [0] NCCL INFO Connected all rings
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:283891 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/IPC/read
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:191614 [0] NCCL INFO Channel 02/0 : 12[0] -> 13[1] via P2P/IPC/read
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283113:283892 [1] NCCL INFO Channel 03/0 : 1[1] -> 2[2] via P2P/IPC/read
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:191613 [2] NCCL INFO Connected all rings
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:283891 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/IPC/read
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:283896 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/IPC/read
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:191614 [0] NCCL INFO Channel 03/0 : 12[0] -> 13[1] via P2P/IPC/read
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:283891 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/IPC/read
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:283896 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/IPC/read
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:283891 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/IPC/read
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:283896 [2] NCCL INFO Channel 02/0 : 2[2] -> 3[3] via P2P/IPC/read
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:191614 [0] NCCL INFO Channel 00/0 : 8[0] -> 12[0] [receive] via NET/IBext/0/GDRDMA
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:191613 [2] NCCL INFO Channel 00/0 : 14[2] -> 15[3] via P2P/IPC/read
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:191614 [0] NCCL INFO Channel 02/0 : 4[0] -> 12[0] [receive] via NET/IBext/0/GDRDMA
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:283891 [0] NCCL INFO Channel 01/0 : 10[2] -> 0[0] [receive] via NET/IBext/0/GDRDMA
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:191614 [0] NCCL INFO Channel 02/0 : 12[0] -> 4[0] [send] via NET/IBext/0/GDRDMA
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:283891 [0] NCCL INFO Channel 03/0 : 0[0] -> 6[2] [send] via NET/IBext/0/GDRDMA
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:171400 [0] NCCL INFO Channel 00/0 : 2[2] -> 8[0] [receive] via NET/IBext/0/GDRDMA
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:283896 [2] NCCL INFO Channel 03/0 : 2[2] -> 3[3] via P2P/IPC/read
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:191613 [2] NCCL INFO Channel 01/0 : 14[2] -> 15[3] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:163769 [2] NCCL INFO Channel 03/0 : 14[2] -> 6[2] [receive] via NET/IBext/1/GDRDMA
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:191613 [2] NCCL INFO Channel 02/0 : 14[2] -> 15[3] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:163769 [2] NCCL INFO Channel 03/0 : 6[2] -> 14[2] [send] via NET/IBext/1/GDRDMA
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283113:283892 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/IPC/read
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283115:283893 [3] NCCL INFO Channel 00/0 : 3[3] -> 0[0] via P2P/IPC/read
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:283896 [2] NCCL INFO Channel 02/0 : 2[2] -> 4[0] [send] via NET/IBext/1/GDRDMA
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:191613 [2] NCCL INFO Channel 03/0 : 14[2] -> 15[3] via P2P/IPC/read
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:283896 [2] NCCL INFO Channel 00/0 : 2[2] -> 8[0] [send] via NET/IBext/1/GDRDMA
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:163770 [0] NCCL INFO Channel 00/0 : 4[0] -> 9[1] [send] via NET/IBext/0/GDRDMA
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283113:283892 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/IPC/read
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190805:191615 [1] NCCL INFO Channel 00/0 : 13[1] -> 12[0] via P2P/IPC/read
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190807:191612 [3] NCCL INFO Channel 01/0 : 15[3] -> 12[0] via P2P/IPC/read
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:171400 [0] NCCL INFO Channel 00/0 : 8[0] -> 2[2] [send] via NET/IBext/0/GDRDMA
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:163770 [0] NCCL INFO Channel 02/0 : 12[0] -> 4[0] [receive] via NET/IBext/0/GDRDMA
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:283896 [2] NCCL INFO Channel 00/0 : 8[0] -> 2[2] [receive] via NET/IBext/1/GDRDMA
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170621:171401 [1] NCCL INFO Channel 00/0 : 9[1] -> 4[0] [send] via NET/IBext/0/GDRDMA
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:163770 [0] NCCL INFO Channel 02/0 : 4[0] -> 12[0] [send] via NET/IBext/0/GDRDMA
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283115:283893 [3] NCCL INFO Channel 02/0 : 3[3] -> 0[0] via P2P/IPC/read
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:191613 [2] NCCL INFO Channel 01/0 : 10[2] -> 14[2] [receive] via NET/IBext/1/GDRDMA
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:163770 [0] NCCL INFO Channel 00/0 : 9[1] -> 4[0] [receive] via NET/IBext/0/GDRDMA
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:191614 [0] NCCL INFO Channel 00/0 : 12[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:191613 [2] NCCL INFO Channel 03/0 : 6[2] -> 14[2] [receive] via NET/IBext/1/GDRDMA
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:283896 [2] NCCL INFO Channel 02/0 : 4[0] -> 2[2] [receive] via NET/IBext/1/GDRDMA
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:171400 [0] NCCL INFO Channel 00/0 : 12[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:191613 [2] NCCL INFO Channel 03/0 : 14[2] -> 6[2] [send] via NET/IBext/1/GDRDMA
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:171402 [2] NCCL INFO Channel 01/0 : 10[2] -> 0[0] [send] via NET/IBext/1/GDRDMA
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170621:171401 [1] NCCL INFO Channel 00/0 : 9[1] -> 8[0] via P2P/IPC/read
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:191613 [2] NCCL INFO Channel 01/0 : 14[2] -> 10[2] [send] via NET/IBext/1/GDRDMA
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:171402 [2] NCCL INFO Channel 01/0 : 0[0] -> 10[2] [receive] via NET/IBext/1/GDRDMA
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:283891 [0] NCCL INFO Channel 03/0 : 6[2] -> 0[0] [receive] via NET/IBext/0/GDRDMA
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:163769 [2] NCCL INFO Channel 03/0 : 6[2] -> 0[0] [send] via NET/IBext/1/GDRDMA
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190805:191615 [1] NCCL INFO Channel 02/0 : 13[1] -> 12[0] via P2P/IPC/read
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:283891 [0] NCCL INFO Channel 01/0 : 0[0] -> 10[2] [send] via NET/IBext/0/GDRDMA
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190807:191612 [3] NCCL INFO Channel 03/0 : 15[3] -> 12[0] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:163769 [2] NCCL INFO Channel 01/0 : 11[3] -> 6[2] [receive] via NET/IBext/1/GDRDMA
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170621:171401 [1] NCCL INFO Channel 02/0 : 9[1] -> 8[0] via P2P/IPC/read
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:171402 [2] NCCL INFO Channel 01/0 : 14[2] -> 10[2] [receive] via NET/IBext/1/GDRDMA
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170623:171399 [3] NCCL INFO Channel 01/0 : 11[3] -> 8[0] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:163770 [0] NCCL INFO Channel 02/0 : 4[0] -> 2[2] [send] via NET/IBext/0/GDRDMA
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:171402 [2] NCCL INFO Channel 03/0 : 10[2] -> 7[3] [send] via NET/IBext/1/GDRDMA
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163003:163768 [3] NCCL INFO Channel 01/0 : 7[3] -> 6[2] via P2P/IPC/read
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283115:283893 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/IPC/read
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190807:191612 [3] NCCL INFO Channel 01/0 : 15[3] -> 14[2] via P2P/IPC/read
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170623:171399 [3] NCCL INFO Channel 03/0 : 11[3] -> 8[0] via P2P/IPC/read
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:171400 [0] NCCL INFO Channel 02/0 : 8[0] -> 5[1] [send] via NET/IBext/0/GDRDMA
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163003:163768 [3] NCCL INFO Channel 03/0 : 7[3] -> 6[2] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163001:163771 [1] NCCL INFO Channel 00/0 : 5[1] -> 4[0] via P2P/IPC/read
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170623:171399 [3] NCCL INFO Channel 01/0 : 11[3] -> 10[2] via P2P/IPC/read
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190807:191612 [3] NCCL INFO Channel 03/0 : 15[3] -> 14[2] via P2P/IPC/read
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283115:283893 [3] NCCL INFO Channel 02/0 : 3[3] -> 2[2] via P2P/IPC/read
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170623:171399 [3] NCCL INFO Channel 03/0 : 11[3] -> 10[2] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163001:163771 [1] NCCL INFO Channel 02/0 : 5[1] -> 4[0] via P2P/IPC/read
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170621:171422 [1] NCCL INFO NCCL_IB_TIMEOUT set by environment to 23.
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170621:171422 [1] NCCL INFO NCCL_IB_RETRY_CNT set by environment to 7.
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283113:283892 [1] NCCL INFO Connected all trees
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283113:283892 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283113:283892 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190805:191615 [1] NCCL INFO Connected all trees
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190805:191615 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190805:191615 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283115:283893 [3] NCCL INFO Connected all trees
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283115:283893 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283115:283893 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190807:191612 [3] NCCL INFO Connected all trees
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190807:191612 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190807:191612 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170623:171420 [3] NCCL INFO NCCL_IB_TIMEOUT set by environment to 23.
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170623:171420 [3] NCCL INFO NCCL_IB_RETRY_CNT set by environment to 7.
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:191614 [0] NCCL INFO Connected all trees
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:191614 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:191614 [0] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:191613 [2] NCCL INFO Connected all trees
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:191613 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:191613 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170621:171401 [1] NCCL INFO Connected all trees
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170621:171401 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170621:171401 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170623:171399 [3] NCCL INFO Connected all trees
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170623:171399 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170623:171399 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:283891 [0] NCCL INFO Connected all trees
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:283891 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:283891 [0] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:163769 [2] NCCL INFO Connected all trees
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:163769 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:163769 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:163770 [0] NCCL INFO Connected all trees
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:163770 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:163770 [0] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:283896 [2] NCCL INFO Connected all trees
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:283896 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:283896 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163001:163789 [1] NCCL INFO NCCL_IB_TIMEOUT set by environment to 23.
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163001:163789 [1] NCCL INFO NCCL_IB_RETRY_CNT set by environment to 7.
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163003:163791 [3] NCCL INFO NCCL_IB_TIMEOUT set by environment to 23.
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163003:163791 [3] NCCL INFO NCCL_IB_RETRY_CNT set by environment to 7.
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:191613 [2] NCCL INFO comm 0x55eb7fe44ce0 rank 14 nranks 16 cudaDev 2 nvmlDev 2 busId 65000 commId 0x5336c5f13c05193c - Init COMPLETE
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:191614 [0] NCCL INFO comm 0x5559e45ba630 rank 12 nranks 16 cudaDev 0 nvmlDev 0 busId 40000 commId 0x5336c5f13c05193c - Init COMPLETE
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190805:191615 [1] NCCL INFO comm 0x5605dee365c0 rank 13 nranks 16 cudaDev 1 nvmlDev 1 busId 46000 commId 0x5336c5f13c05193c - Init COMPLETE
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190807:191612 [3] NCCL INFO comm 0x556b38dfa900 rank 15 nranks 16 cudaDev 3 nvmlDev 3 busId 6a000 commId 0x5336c5f13c05193c - Init COMPLETE
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:171400 [0] NCCL INFO Connected all trees
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:171400 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:171400 [0] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:171402 [2] NCCL INFO Connected all trees
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:171402 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:171402 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163003:163768 [3] NCCL INFO Connected all trees
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163003:163768 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163003:163768 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163001:163771 [1] NCCL INFO Connected all trees
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163001:163771 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163001:163771 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:283891 [0] NCCL INFO comm 0x557126fed430 rank 0 nranks 16 cudaDev 0 nvmlDev 0 busId 40000 commId 0x5336c5f13c05193c - Init COMPLETE
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283113:283892 [1] NCCL INFO comm 0x560292766de0 rank 1 nranks 16 cudaDev 1 nvmlDev 1 busId 46000 commId 0x5336c5f13c05193c - Init COMPLETE
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:283896 [2] NCCL INFO comm 0x55c4b4d057b0 rank 2 nranks 16 cudaDev 2 nvmlDev 2 busId 65000 commId 0x5336c5f13c05193c - Init COMPLETE
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283115:283893 [3] NCCL INFO comm 0x55d30f58df10 rank 3 nranks 16 cudaDev 3 nvmlDev 3 busId 6a000 commId 0x5336c5f13c05193c - Init COMPLETE
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170621:171401 [1] NCCL INFO comm 0x55f543b2c590 rank 9 nranks 16 cudaDev 1 nvmlDev 1 busId 52000 commId 0x5336c5f13c05193c - Init COMPLETE
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170623:171399 [3] NCCL INFO comm 0x562f3fdbf3c0 rank 11 nranks 16 cudaDev 3 nvmlDev 3 busId 6f000 commId 0x5336c5f13c05193c - Init COMPLETE
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:171400 [0] NCCL INFO comm 0x55e6b1c71c90 rank 8 nranks 16 cudaDev 0 nvmlDev 0 busId 4d000 commId 0x5336c5f13c05193c - Init COMPLETE
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:171402 [2] NCCL INFO comm 0x55638cf2a4b0 rank 10 nranks 16 cudaDev 2 nvmlDev 2 busId 69000 commId 0x5336c5f13c05193c - Init COMPLETE
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163001:163771 [1] NCCL INFO comm 0x564bd6987960 rank 5 nranks 16 cudaDev 1 nvmlDev 1 busId 52000 commId 0x5336c5f13c05193c - Init COMPLETE
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:163770 [0] NCCL INFO comm 0x5564391fad90 rank 4 nranks 16 cudaDev 0 nvmlDev 0 busId 4d000 commId 0x5336c5f13c05193c - Init COMPLETE
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:163769 [2] NCCL INFO comm 0x564ea6e2b2c0 rank 6 nranks 16 cudaDev 2 nvmlDev 2 busId 69000 commId 0x5336c5f13c05193c - Init COMPLETE
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163003:163768 [3] NCCL INFO comm 0x55ea84953ad0 rank 7 nranks 16 cudaDev 3 nvmlDev 3 busId 6f000 commId 0x5336c5f13c05193c - Init COMPLETE
worker3: Setting model_config.attention_dropout to 0.0
worker1: Setting model_config.attention_dropout to 0.0
worker3: Setting model_config.attention_dropout to 0.0
head: Setting model_config.attention_dropout to 0.0
worker3: Setting model_config.attention_dropout to 0.0
worker2: Setting model_config.attention_dropout to 0.0
worker1: ************************[start] Initializing Actor Model [start] *************************
worker2: Setting model_config.attention_dropout to 0.0
worker1: Setting model_config.attention_dropout to 0.0
worker3: Setting model_config.attention_dropout to 0.0
worker1: Setting model_config.attention_dropout to 0.0
worker1: Setting model_config.attention_dropout to 0.0
head: Setting model_config.attention_dropout to 0.0
worker2: Setting model_config.attention_dropout to 0.0
head: Setting model_config.attention_dropout to 0.0
head: Setting model_config.attention_dropout to 0.0
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283113:283915 [1] NCCL INFO NCCL_IB_TIMEOUT set by environment to 23.
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283113:283915 [1] NCCL INFO NCCL_IB_RETRY_CNT set by environment to 7.
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283115:283917 [3] NCCL INFO NCCL_IB_TIMEOUT set by environment to 23.
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283115:283917 [3] NCCL INFO NCCL_IB_RETRY_CNT set by environment to 7.
worker2: Setting model_config.attention_dropout to 0.0
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190805:191639 [1] NCCL INFO NCCL_IB_TIMEOUT set by environment to 23.
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190805:191639 [1] NCCL INFO NCCL_IB_RETRY_CNT set by environment to 7.
worker1: [2024-06-14 07:25:01,931] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190807:191640 [3] NCCL INFO NCCL_IB_TIMEOUT set by environment to 23.
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190807:191640 [3] NCCL INFO NCCL_IB_RETRY_CNT set by environment to 7.
worker3: Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:03,  1.62s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:03,  1.62s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:03,  1.64s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:03,  1.67s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.73s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.73s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.74s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.75s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.58s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.61s/it]
worker3: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.59s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.61s/it]
head: Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:03,  1.67s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:03,  1.69s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:03,  1.71s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:03,  1.74s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.73s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.73s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.74s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.76s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.58s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.62s/it]
worker3: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.59s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.62s/it]
worker1: Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:03,  1.70s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:03,  1.71s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:03,  1.74s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:03,  1.82s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.72s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.73s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.75s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.78s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.59s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.62s/it]
worker2: Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:03,  1.66s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:03,  1.67s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:03,  1.72s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:03,  1.75s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.74s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.74s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.74s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.76s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.58s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.62s/it]
worker2: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.59s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.62s/it]
worker2: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.59s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.62s/it]
head: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.58s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.62s/it]
worker1: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.59s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.63s/it]
worker2: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.58s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.63s/it]
worker3: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.60s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.63s/it]
head: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.60s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.63s/it]
head: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.59s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.64s/it]
worker1: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.60s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.64s/it]
worker1: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.60s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.65s/it]
worker3: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
worker3: Detected CUDA files, patching ldflags
worker3: Emitting ninja build file /root/.cache/torch_extensions/py310_cu118/cpu_adam/build.ninja...
worker3: Building extension module cpu_adam...
worker3: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
worker3: ninja: no work to do.
worker3: Loading extension module cpu_adam...
worker3: Time to load cpu_adam op: 2.403235673904419 seconds
worker2: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
worker2: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
worker2: Detected CUDA files, patching ldflags
worker2: Emitting ninja build file /root/.cache/torch_extensions/py310_cu118/cpu_adam/build.ninja...
worker2: Building extension module cpu_adam...
worker2: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
worker2: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
worker2: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
worker2: ninja: no work to do.
worker2: Loading extension module cpu_adam...
worker2: Time to load cpu_adam op: 2.5671732425689697 seconds
worker3: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
head: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
head: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
head: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
head: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
worker3: Detected CUDA files, patching ldflags
worker3: Emitting ninja build file /root/.cache/torch_extensions/py310_cu118/cpu_adam/build.ninja...
worker3: Building extension module cpu_adam...
worker3: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
worker3: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
worker3: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
head: Detected CUDA files, patching ldflags
head: Emitting ninja build file /root/.cache/torch_extensions/py310_cu118/cpu_adam/build.ninja...
head: Building extension module cpu_adam...
head: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
worker2: Loading extension module cpu_adam...
worker2: Time to load cpu_adam op: 2.6323344707489014 seconds
worker3: ninja: no work to do.
head: ninja: no work to do.
worker3: Loading extension module cpu_adam...
worker3: Time to load cpu_adam op: 2.543346643447876 seconds
head: Loading extension module cpu_adam...
head: Time to load cpu_adam op: 2.6321709156036377 seconds
worker2: Loading extension module cpu_adam...
worker2: Loading extension module cpu_adam...
worker2: Time to load cpu_adam op: 2.645867109298706 seconds
worker2: Time to load cpu_adam op: 2.6449568271636963 seconds
worker1: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
worker1: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
worker1: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
worker1: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
head: Loading extension module cpu_adam...
head: Loading extension module cpu_adam...
head: Time to load cpu_adam op: 2.667585849761963 secondsTime to load cpu_adam op: 2.670011281967163 seconds
head: 
head: Loading extension module cpu_adam...
head: Time to load cpu_adam op: 2.6765859127044678 seconds
worker1: Detected CUDA files, patching ldflags
worker1: Emitting ninja build file /root/.cache/torch_extensions/py310_cu118/cpu_adam/build.ninja...
worker1: Building extension module cpu_adam...
worker1: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
worker3: Loading extension module cpu_adam...
worker3: Loading extension module cpu_adam...
worker3: Time to load cpu_adam op: 2.6090710163116455 seconds
worker3: Time to load cpu_adam op: 2.604081392288208 seconds
worker1: ninja: no work to do.
worker1: Loading extension module cpu_adam...
worker1: Time to load cpu_adam op: 2.7404141426086426 seconds
worker1: Loading extension module cpu_adam...Loading extension module cpu_adam...
worker1: 
worker1: Time to load cpu_adam op: 2.777251958847046 secondsTime to load cpu_adam op: 2.773465633392334 seconds
worker1: 
worker1: Loading extension module cpu_adam...
worker1: Time to load cpu_adam op: 2.7792534828186035 seconds
worker2: Adam Optimizer #0 is created with AVX512 arithmetic capability.
worker2: Config: alpha=0.000010, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
head: Adam Optimizer #0 is created with AVX512 arithmetic capability.
head: Config: alpha=0.000010, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
head: Adam Optimizer #0 is created with AVX512 arithmetic capability.
head: Config: alpha=0.000010, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
worker2: Adam Optimizer #0 is created with AVX512 arithmetic capability.
worker2: Config: alpha=0.000010, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
worker3: Adam Optimizer #0 is created with AVX512 arithmetic capability.
worker3: Config: alpha=0.000010, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
worker3: Adam Optimizer #0 is created with AVX512 arithmetic capability.
worker3: Config: alpha=0.000010, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
head: Adam Optimizer #0 is created with AVX512 arithmetic capability.
head: Config: alpha=0.000010, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
head: Adam Optimizer #0 is created with AVX512 arithmetic capability.
head: Config: alpha=0.000010, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
worker1: Adam Optimizer #0 is created with AVX512 arithmetic capability.
worker1: Config: alpha=0.000010, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
worker2: Adam Optimizer #0 is created with AVX512 arithmetic capability.
worker2: Config: alpha=0.000010, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
worker2: Adam Optimizer #0 is created with AVX512 arithmetic capability.
worker2: Config: alpha=0.000010, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
worker1: Adam Optimizer #0 is created with AVX512 arithmetic capability.
worker1: Config: alpha=0.000010, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
worker3: Adam Optimizer #0 is created with AVX512 arithmetic capability.
worker3: Config: alpha=0.000010, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
worker1: Adam Optimizer #0 is created with AVX512 arithmetic capability.
worker1: Config: alpha=0.000010, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
worker1: Adam Optimizer #0 is created with AVX512 arithmetic capability.
worker1: Config: alpha=0.000010, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
worker1: [2024-06-14 07:25:12,594] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.6+b1e9b0fa, git-hash=b1e9b0fa, git-branch=main
worker3: Adam Optimizer #0 is created with AVX512 arithmetic capability.
worker3: Config: alpha=0.000010, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
worker1: [2024-06-14 07:25:12,614] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
worker1: [2024-06-14 07:25:12,615] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
worker1: [2024-06-14 07:25:12,616] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
worker1: [2024-06-14 07:25:12,626] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
worker1: [2024-06-14 07:25:12,626] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
worker1: [2024-06-14 07:25:12,626] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
worker1: [2024-06-14 07:25:12,626] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 3 optimizer
worker1: [2024-06-14 07:25:12,947] [INFO] [utils.py:791:see_memory_usage] Stage 3 initialize beginning
worker1: [2024-06-14 07:25:12,948] [INFO] [utils.py:792:see_memory_usage] MA 0.55 GB         Max_MA 1.1 GB         CA 1.29 GB         Max_CA 1 GB 
worker1: [2024-06-14 07:25:12,949] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 19.93 GB, percent = 1.1%
worker1: [2024-06-14 07:25:12,950] [INFO] [stage3.py:127:__init__] Reduce bucket size 500,000,000
worker1: [2024-06-14 07:25:12,950] [INFO] [stage3.py:128:__init__] Prefetch bucket size 30000000
worker1: [2024-06-14 07:25:13,195] [INFO] [utils.py:791:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
worker1: [2024-06-14 07:25:13,196] [INFO] [utils.py:792:see_memory_usage] MA 0.55 GB         Max_MA 0.55 GB         CA 1.29 GB         Max_CA 1 GB 
worker1: [2024-06-14 07:25:13,196] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 19.93 GB, percent = 1.1%
worker1: Parameter Offload: Total persistent parameters: 266240 in 65 params
worker1: [2024-06-14 07:25:13,496] [INFO] [utils.py:791:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
worker1: [2024-06-14 07:25:13,496] [INFO] [utils.py:792:see_memory_usage] MA 0.06 GB         Max_MA 0.55 GB         CA 1.29 GB         Max_CA 1 GB 
worker1: [2024-06-14 07:25:13,497] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 20.05 GB, percent = 1.1%
worker1: [2024-06-14 07:25:13,789] [INFO] [utils.py:791:see_memory_usage] Before creating fp16 partitions
worker1: [2024-06-14 07:25:13,790] [INFO] [utils.py:792:see_memory_usage] MA 0.06 GB         Max_MA 0.06 GB         CA 1.29 GB         Max_CA 1 GB 
worker1: [2024-06-14 07:25:13,791] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 20.05 GB, percent = 1.1%
worker1: [2024-06-14 07:25:19,288] [INFO] [utils.py:791:see_memory_usage] After creating fp16 partitions: 2
worker1: [2024-06-14 07:25:19,290] [INFO] [utils.py:792:see_memory_usage] MA 0.06 GB         Max_MA 0.06 GB         CA 1.29 GB         Max_CA 1 GB 
worker1: [2024-06-14 07:25:19,291] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 23.44 GB, percent = 1.2%
worker1: [2024-06-14 07:25:19,601] [INFO] [utils.py:791:see_memory_usage] Before creating fp32 partitions
worker1: [2024-06-14 07:25:19,602] [INFO] [utils.py:792:see_memory_usage] MA 0.06 GB         Max_MA 0.06 GB         CA 1.29 GB         Max_CA 1 GB 
worker1: [2024-06-14 07:25:19,602] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 24.8 GB, percent = 1.3%
worker1: [2024-06-14 07:25:20,001] [INFO] [utils.py:791:see_memory_usage] After creating fp32 partitions
worker1: [2024-06-14 07:25:20,002] [INFO] [utils.py:792:see_memory_usage] MA 0.06 GB         Max_MA 0.06 GB         CA 1.29 GB         Max_CA 1 GB 
worker1: [2024-06-14 07:25:20,010] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 26.37 GB, percent = 1.4%
worker1: [2024-06-14 07:25:20,642] [INFO] [utils.py:791:see_memory_usage] Before initializing optimizer states
worker1: [2024-06-14 07:25:20,643] [INFO] [utils.py:792:see_memory_usage] MA 0.06 GB         Max_MA 0.06 GB         CA 1.29 GB         Max_CA 1 GB 
worker1: [2024-06-14 07:25:20,643] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 44.05 GB, percent = 2.3%
worker1: [2024-06-14 07:25:21,429] [INFO] [utils.py:791:see_memory_usage] After initializing optimizer states
worker1: [2024-06-14 07:25:21,429] [INFO] [utils.py:792:see_memory_usage] MA 0.06 GB         Max_MA 0.06 GB         CA 1.29 GB         Max_CA 1 GB 
worker1: [2024-06-14 07:25:21,430] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 45.23 GB, percent = 2.4%
worker1: [2024-06-14 07:25:21,430] [INFO] [stage3.py:479:_setup_for_real_optimizer] optimizer state initialized
worker1: [2024-06-14 07:25:22,299] [INFO] [utils.py:791:see_memory_usage] After initializing ZeRO optimizer
worker1: [2024-06-14 07:25:22,300] [INFO] [utils.py:792:see_memory_usage] MA 0.99 GB         Max_MA 1.48 GB         CA 2.22 GB         Max_CA 2 GB 
worker1: [2024-06-14 07:25:22,300] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 48.38 GB, percent = 2.6%
worker1: [2024-06-14 07:25:22,300] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
worker1: [2024-06-14 07:25:22,300] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
worker1: [2024-06-14 07:25:22,300] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x7f18f43f00d0>
worker1: [2024-06-14 07:25:22,301] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[9.65e-06, 9.65e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
worker1: [2024-06-14 07:25:22,301] [INFO] [config.py:984:print] DeepSpeedEngine configuration:
worker1: [2024-06-14 07:25:22,302] [INFO] [config.py:988:print]   activation_checkpointing_config  {
worker1:     "partition_activations": false, 
worker1:     "contiguous_memory_optimization": false, 
worker1:     "cpu_checkpointing": false, 
worker1:     "number_checkpoints": null, 
worker1:     "synchronize_checkpoint_boundary": false, 
worker1:     "profile": false
worker1: }
worker1: [2024-06-14 07:25:22,302] [INFO] [config.py:988:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
worker1: [2024-06-14 07:25:22,302] [INFO] [config.py:988:print]   amp_enabled .................. False
worker1: [2024-06-14 07:25:22,302] [INFO] [config.py:988:print]   amp_params ................... False
worker1: [2024-06-14 07:25:22,302] [INFO] [config.py:988:print]   autotuning_config ............ {
worker1:     "enabled": false, 
worker1:     "start_step": null, 
worker1:     "end_step": null, 
worker1:     "metric_path": null, 
worker1:     "arg_mappings": null, 
worker1:     "metric": "throughput", 
worker1:     "model_info": null, 
worker1:     "results_dir": "autotuning_results", 
worker1:     "exps_dir": "autotuning_exps", 
worker1:     "overwrite": true, 
worker1:     "fast": true, 
worker1:     "start_profile_step": 3, 
worker1:     "end_profile_step": 5, 
worker1:     "tuner_type": "gridsearch", 
worker1:     "tuner_early_stopping": 5, 
worker1:     "tuner_num_trials": 50, 
worker1:     "model_info_path": null, 
worker1:     "mp_size": 1, 
worker1:     "max_train_batch_size": null, 
worker1:     "min_train_batch_size": 1, 
worker1:     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
worker1:     "min_train_micro_batch_size_per_gpu": 1, 
worker1:     "num_tuning_micro_batch_sizes": 3
worker1: }
worker1: [2024-06-14 07:25:22,302] [INFO] [config.py:988:print]   bfloat16_enabled ............. False
worker1: [2024-06-14 07:25:22,302] [INFO] [config.py:988:print]   checkpoint_parallel_write_pipeline  False
worker1: [2024-06-14 07:25:22,302] [INFO] [config.py:988:print]   checkpoint_tag_validation_enabled  True
worker1: [2024-06-14 07:25:22,302] [INFO] [config.py:988:print]   checkpoint_tag_validation_fail  False
worker1: [2024-06-14 07:25:22,302] [INFO] [config.py:988:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f19784597e0>
worker1: [2024-06-14 07:25:22,302] [INFO] [config.py:988:print]   communication_data_type ...... None
worker1: [2024-06-14 07:25:22,302] [INFO] [config.py:988:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
worker1: [2024-06-14 07:25:22,302] [INFO] [config.py:988:print]   curriculum_enabled_legacy .... False
worker1: [2024-06-14 07:25:22,302] [INFO] [config.py:988:print]   curriculum_params_legacy ..... False
worker1: [2024-06-14 07:25:22,302] [INFO] [config.py:988:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
worker1: [2024-06-14 07:25:22,302] [INFO] [config.py:988:print]   data_efficiency_enabled ...... False
worker1: [2024-06-14 07:25:22,302] [INFO] [config.py:988:print]   dataloader_drop_last ......... False
worker1: [2024-06-14 07:25:22,302] [INFO] [config.py:988:print]   disable_allgather ............ False
worker1: [2024-06-14 07:25:22,302] [INFO] [config.py:988:print]   dump_state ................... False
worker1: [2024-06-14 07:25:22,302] [INFO] [config.py:988:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 100, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
worker1: [2024-06-14 07:25:22,302] [INFO] [config.py:988:print]   eigenvalue_enabled ........... False
worker1: [2024-06-14 07:25:22,302] [INFO] [config.py:988:print]   eigenvalue_gas_boundary_resolution  1
worker1: [2024-06-14 07:25:22,302] [INFO] [config.py:988:print]   eigenvalue_layer_name ........ bert.encoder.layer
worker1: [2024-06-14 07:25:22,302] [INFO] [config.py:988:print]   eigenvalue_layer_num ......... 0
worker1: [2024-06-14 07:25:22,302] [INFO] [config.py:988:print]   eigenvalue_max_iter .......... 100
worker1: [2024-06-14 07:25:22,302] [INFO] [config.py:988:print]   eigenvalue_stability ......... 1e-06
worker1: [2024-06-14 07:25:22,303] [INFO] [config.py:988:print]   eigenvalue_tol ............... 0.01
worker1: [2024-06-14 07:25:22,303] [INFO] [config.py:988:print]   eigenvalue_verbose ........... False
worker1: [2024-06-14 07:25:22,303] [INFO] [config.py:988:print]   elasticity_enabled ........... False
worker1: [2024-06-14 07:25:22,303] [INFO] [config.py:988:print]   flops_profiler_config ........ {
worker1:     "enabled": false, 
worker1:     "recompute_fwd_factor": 0.0, 
worker1:     "profile_step": 1, 
worker1:     "module_depth": -1, 
worker1:     "top_modules": 1, 
worker1:     "detailed": true, 
worker1:     "output_file": null
worker1: }
worker1: [2024-06-14 07:25:22,303] [INFO] [config.py:988:print]   fp16_auto_cast ............... False
worker1: [2024-06-14 07:25:22,303] [INFO] [config.py:988:print]   fp16_enabled ................. True
worker1: [2024-06-14 07:25:22,303] [INFO] [config.py:988:print]   fp16_master_weights_and_gradients  False
worker1: [2024-06-14 07:25:22,303] [INFO] [config.py:988:print]   global_rank .................. 0
worker1: [2024-06-14 07:25:22,303] [INFO] [config.py:988:print]   grad_accum_dtype ............. None
worker1: [2024-06-14 07:25:22,303] [INFO] [config.py:988:print]   gradient_accumulation_steps .. 8
worker1: [2024-06-14 07:25:22,303] [INFO] [config.py:988:print]   gradient_clipping ............ 1.0
worker1: [2024-06-14 07:25:22,303] [INFO] [config.py:988:print]   gradient_predivide_factor .... 1.0
worker1: [2024-06-14 07:25:22,303] [INFO] [config.py:988:print]   graph_harvesting ............. False
worker1: [2024-06-14 07:25:22,303] [INFO] [config.py:988:print]   hybrid_engine ................ enabled=True max_out_tokens=2048 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
worker1: [2024-06-14 07:25:22,303] [INFO] [config.py:988:print]   initial_dynamic_scale ........ 65536
worker1: [2024-06-14 07:25:22,303] [INFO] [config.py:988:print]   load_universal_checkpoint .... False
worker1: [2024-06-14 07:25:22,303] [INFO] [config.py:988:print]   loss_scale ................... 0
worker1: [2024-06-14 07:25:22,303] [INFO] [config.py:988:print]   memory_breakdown ............. False
worker1: [2024-06-14 07:25:22,303] [INFO] [config.py:988:print]   mics_hierarchial_params_gather  False
worker1: [2024-06-14 07:25:22,303] [INFO] [config.py:988:print]   mics_shard_size .............. -1
worker1: [2024-06-14 07:25:22,303] [INFO] [config.py:988:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='step3_tensorboard/ds_tensorboard_logs/', job_name='step3_actor_tensorboard') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
worker1: [2024-06-14 07:25:22,303] [INFO] [config.py:988:print]   nebula_config ................ {
worker1:     "enabled": false, 
worker1:     "persistent_storage_path": null, 
worker1:     "persistent_time_interval": 100, 
worker1:     "num_of_version_in_retention": 2, 
worker1:     "enable_nebula_load": true, 
worker1:     "load_path": null
worker1: }
worker1: [2024-06-14 07:25:22,303] [INFO] [config.py:988:print]   optimizer_legacy_fusion ...... False
worker1: [2024-06-14 07:25:22,303] [INFO] [config.py:988:print]   optimizer_name ............... None
worker1: [2024-06-14 07:25:22,303] [INFO] [config.py:988:print]   optimizer_params ............. None
worker1: [2024-06-14 07:25:22,303] [INFO] [config.py:988:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
worker1: [2024-06-14 07:25:22,303] [INFO] [config.py:988:print]   pld_enabled .................. False
worker1: [2024-06-14 07:25:22,303] [INFO] [config.py:988:print]   pld_params ................... False
worker1: [2024-06-14 07:25:22,303] [INFO] [config.py:988:print]   prescale_gradients ........... False
worker1: [2024-06-14 07:25:22,303] [INFO] [config.py:988:print]   scheduler_name ............... None
worker1: [2024-06-14 07:25:22,303] [INFO] [config.py:988:print]   scheduler_params ............. None
worker1: [2024-06-14 07:25:22,303] [INFO] [config.py:988:print]   seq_parallel_communication_data_type  torch.float32
worker1: [2024-06-14 07:25:22,303] [INFO] [config.py:988:print]   sparse_attention ............. None
worker1: [2024-06-14 07:25:22,303] [INFO] [config.py:988:print]   sparse_gradients_enabled ..... False
worker1: [2024-06-14 07:25:22,303] [INFO] [config.py:988:print]   steps_per_print .............. 10
worker1: [2024-06-14 07:25:22,303] [INFO] [config.py:988:print]   train_batch_size ............. 1024
worker1: [2024-06-14 07:25:22,303] [INFO] [config.py:988:print]   train_micro_batch_size_per_gpu  8
worker1: [2024-06-14 07:25:22,303] [INFO] [config.py:988:print]   use_data_before_expert_parallel_  False
worker1: [2024-06-14 07:25:22,303] [INFO] [config.py:988:print]   use_node_local_storage ....... False
worker1: [2024-06-14 07:25:22,303] [INFO] [config.py:988:print]   wall_clock_breakdown ......... False
worker1: [2024-06-14 07:25:22,303] [INFO] [config.py:988:print]   weight_quantization_config ... None
worker1: [2024-06-14 07:25:22,303] [INFO] [config.py:988:print]   world_size ................... 16
worker1: [2024-06-14 07:25:22,303] [INFO] [config.py:988:print]   zero_allow_untested_optimizer  False
worker1: [2024-06-14 07:25:22,304] [INFO] [config.py:988:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=30000000 param_persistence_threshold=10000 model_persistence_threshold=sys.maxsize max_live_parameters=30000000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=False pipeline_loading_checkpoint=False override_module_apply=True
worker1: [2024-06-14 07:25:22,304] [INFO] [config.py:988:print]   zero_enabled ................. True
worker1: [2024-06-14 07:25:22,304] [INFO] [config.py:988:print]   zero_force_ds_cpu_optimizer .. True
worker1: [2024-06-14 07:25:22,304] [INFO] [config.py:988:print]   zero_optimization_stage ...... 3
worker1: [2024-06-14 07:25:22,304] [INFO] [config.py:974:print_user_config]   json = {
worker1:     "train_batch_size": 1.024000e+03, 
worker1:     "train_micro_batch_size_per_gpu": 8, 
worker1:     "steps_per_print": 10, 
worker1:     "zero_optimization": {
worker1:         "stage": 3, 
worker1:         "offload_param": {
worker1:             "device": "cpu"
worker1:         }, 
worker1:         "offload_optimizer": {
worker1:             "device": "cpu"
worker1:         }, 
worker1:         "stage3_param_persistence_threshold": 1.000000e+04, 
worker1:         "stage3_max_live_parameters": 3.000000e+07, 
worker1:         "stage3_prefetch_bucket_size": 3.000000e+07, 
worker1:         "memory_efficient_linear": false
worker1:     }, 
worker1:     "fp16": {
worker1:         "enabled": true, 
worker1:         "loss_scale_window": 100
worker1:     }, 
worker1:     "gradient_clipping": 1.0, 
worker1:     "prescale_gradients": false, 
worker1:     "wall_clock_breakdown": false, 
worker1:     "hybrid_engine": {
worker1:         "enabled": true, 
worker1:         "max_out_tokens": 2.048000e+03, 
worker1:         "inference_tp_size": 1, 
worker1:         "release_inference_cache": false, 
worker1:         "pin_parameters": true, 
worker1:         "tp_gather_partition_size": 8
worker1:     }, 
worker1:     "tensorboard": {
worker1:         "enabled": false, 
worker1:         "output_path": "step3_tensorboard/ds_tensorboard_logs/", 
worker1:         "job_name": "step3_actor_tensorboard"
worker1:     }
worker1: }
worker3: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
worker3: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
worker1: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
worker3: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
worker3: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
head: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
head: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
head: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
head: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
worker1: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
worker1: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
worker1: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
worker2: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
worker2: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
worker2: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
worker2: Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
worker3: Detected CUDA files, patching ldflags
worker3: Emitting ninja build file /root/.cache/torch_extensions/py310_cu118/transformer_inference/build.ninja...
worker3: Building extension module transformer_inference...
worker3: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
worker1: Detected CUDA files, patching ldflags
worker1: Emitting ninja build file /root/.cache/torch_extensions/py310_cu118/transformer_inference/build.ninja...
worker1: Building extension module transformer_inference...
worker1: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
head: Detected CUDA files, patching ldflags
head: Emitting ninja build file /root/.cache/torch_extensions/py310_cu118/transformer_inference/build.ninja...
head: Building extension module transformer_inference...
head: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
worker2: Detected CUDA files, patching ldflags
worker2: Emitting ninja build file /root/.cache/torch_extensions/py310_cu118/transformer_inference/build.ninja...
worker2: Building extension module transformer_inference...
worker2: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
worker3: ninja: no work to do.
worker1: ninja: no work to do.
worker3: Loading extension module transformer_inference...
head: ninja: no work to do.
worker1: Loading extension module transformer_inference...
worker3: Time to load transformer_inference op: 0.06806540489196777 seconds
worker1: Time to load transformer_inference op: 0.0681314468383789 seconds
worker2: ninja: no work to do.
worker1: [2024-06-14 07:25:22,397] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed-Inference config: {'layer_id': 0, 'hidden_size': 4096, 'intermediate_size': 11008, 'heads': 32, 'num_hidden_layers': -1, 'dtype': torch.float16, 'pre_layer_norm': True, 'norm_type': <NormType.RMSNorm: 3>, 'local_rank': -1, 'stochastic_mode': False, 'epsilon': 1e-05, 'mp_size': 1, 'scale_attention': True, 'triangular_masking': True, 'local_attention': False, 'window_size': 1, 'rotary_dim': 128, 'rotate_half': True, 'rotate_every_two': False, 'return_tuple': True, 'mlp_after_attn': True, 'mlp_act_func_type': <ActivationFuncType.GATED_SILU: 4>, 'specialized_mode': False, 'training_mp_size': 1, 'bigscience_bloom': False, 'max_out_tokens': 2048, 'min_out_tokens': 2048, 'scale_attn_by_inverse_layer_idx': False, 'enable_qkv_quantization': False, 'use_mup': False, 'return_single_tuple': False, 'set_empty_params': True, 'transposed_mode': True, 'use_triton': False, 'triton_autotune': False, 'num_kv': 32, 'rope_theta': 10000.0}
head: Loading extension module transformer_inference...
head: Time to load transformer_inference op: 0.0681004524230957 seconds
worker2: Loading extension module transformer_inference...
worker2: Time to load transformer_inference op: 0.06993699073791504 seconds
worker3: Loading extension module transformer_inference...
worker3: Loading extension module transformer_inference...
worker3: Loading extension module transformer_inference...
worker3: Time to load transformer_inference op: 0.10403299331665039 seconds
worker3: Time to load transformer_inference op: 0.10390996932983398 seconds
worker3: Time to load transformer_inference op: 0.10381245613098145 seconds
head: Loading extension module transformer_inference...
head: Loading extension module transformer_inference...
head: Loading extension module transformer_inference...
worker1: Loading extension module transformer_inference...
worker1: Loading extension module transformer_inference...
worker1: Loading extension module transformer_inference...
worker2: Loading extension module transformer_inference...
worker2: Loading extension module transformer_inference...
head: Time to load transformer_inference op: 0.10459685325622559 seconds
head: Time to load transformer_inference op: 0.10457086563110352 seconds
head: Time to load transformer_inference op: 0.10453367233276367 seconds
worker1: Time to load transformer_inference op: 0.10465216636657715 seconds
worker1: Time to load transformer_inference op: 0.10458540916442871 seconds
worker1: Time to load transformer_inference op: 0.10443925857543945 seconds
worker2: Time to load transformer_inference op: 0.10469818115234375 seconds
worker2: Time to load transformer_inference op: 0.1047968864440918 seconds
worker1: ******************[end] Initialized Actor Model [end] (duration: 22.27s)******************
worker1: *************************[start] Initializing Ref Model [start] **************************
worker2: Loading extension module transformer_inference...
worker2: Time to load transformer_inference op: 0.10442185401916504 seconds
worker1: [2024-06-14 07:25:22,637] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 582, num_elems = 13.48B
worker3: Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.15s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.12s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.15s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.23s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.51s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.50s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.51s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.54s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.42s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.41s/it]
worker3: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.41s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.40s/it]
worker2: Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.24s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.28s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.25s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.24s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.46s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.47s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.48s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.49s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.40s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.39s/it]
worker2: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.40s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.39s/it]
head: Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.26s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.26s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.31s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.26s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.49s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.47s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.47s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.47s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.40s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.40s/it]
head: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.40s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.40s/it]
head: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.41s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.42s/it]
head: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.40s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.40s/it]
worker2: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.41s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.41s/it]
worker1: Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.27s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.29s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.35s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:02<00:04,  2.01s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.48s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.49s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.50s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.82s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.41s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.41s/it]
worker1: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.40s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.40s/it]
worker3: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.42s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.41s/it]
worker2: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.41s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.41s/it]
worker1: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.41s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.42s/it]
worker3: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.44s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.44s/it]
worker1: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.57s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.66s/it]
worker1: [2024-06-14 07:25:27,667] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.6+b1e9b0fa, git-hash=b1e9b0fa, git-branch=main
worker1: [2024-06-14 07:25:27,688] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
worker1: [2024-06-14 07:25:27,690] [INFO] [logging.py:96:log_dist] [Rank 0] Creating ZeRO Offload
worker1: [2024-06-14 07:25:27,958] [INFO] [utils.py:791:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
worker1: [2024-06-14 07:25:27,959] [INFO] [utils.py:792:see_memory_usage] MA 2.57 GB         Max_MA 3.15 GB         CA 12.55 GB         Max_CA 13 GB 
worker1: [2024-06-14 07:25:27,959] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 48.38 GB, percent = 2.6%
worker1: Parameter Offload: Total persistent parameters: 266240 in 65 params
worker1: [2024-06-14 07:25:28,248] [INFO] [utils.py:791:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
worker1: [2024-06-14 07:25:28,248] [INFO] [utils.py:792:see_memory_usage] MA 2.11 GB         Max_MA 2.58 GB         CA 12.55 GB         Max_CA 13 GB 
worker1: [2024-06-14 07:25:28,249] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 48.38 GB, percent = 2.6%
worker1: [2024-06-14 07:25:28,250] [INFO] [config.py:984:print] DeepSpeedEngine configuration:
worker1: [2024-06-14 07:25:28,250] [INFO] [config.py:988:print]   activation_checkpointing_config  {
worker1:     "partition_activations": false, 
worker1:     "contiguous_memory_optimization": false, 
worker1:     "cpu_checkpointing": false, 
worker1:     "number_checkpoints": null, 
worker1:     "synchronize_checkpoint_boundary": false, 
worker1:     "profile": false
worker1: }
worker1: [2024-06-14 07:25:28,250] [INFO] [config.py:988:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
worker1: [2024-06-14 07:25:28,250] [INFO] [config.py:988:print]   amp_enabled .................. False
worker1: [2024-06-14 07:25:28,250] [INFO] [config.py:988:print]   amp_params ................... False
worker1: [2024-06-14 07:25:28,250] [INFO] [config.py:988:print]   autotuning_config ............ {
worker1:     "enabled": false, 
worker1:     "start_step": null, 
worker1:     "end_step": null, 
worker1:     "metric_path": null, 
worker1:     "arg_mappings": null, 
worker1:     "metric": "throughput", 
worker1:     "model_info": null, 
worker1:     "results_dir": "autotuning_results", 
worker1:     "exps_dir": "autotuning_exps", 
worker1:     "overwrite": true, 
worker1:     "fast": true, 
worker1:     "start_profile_step": 3, 
worker1:     "end_profile_step": 5, 
worker1:     "tuner_type": "gridsearch", 
worker1:     "tuner_early_stopping": 5, 
worker1:     "tuner_num_trials": 50, 
worker1:     "model_info_path": null, 
worker1:     "mp_size": 1, 
worker1:     "max_train_batch_size": null, 
worker1:     "min_train_batch_size": 1, 
worker1:     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
worker1:     "min_train_micro_batch_size_per_gpu": 1, 
worker1:     "num_tuning_micro_batch_sizes": 3
worker1: }
worker1: [2024-06-14 07:25:28,250] [INFO] [config.py:988:print]   bfloat16_enabled ............. False
worker1: [2024-06-14 07:25:28,250] [INFO] [config.py:988:print]   checkpoint_parallel_write_pipeline  False
worker1: [2024-06-14 07:25:28,250] [INFO] [config.py:988:print]   checkpoint_tag_validation_enabled  True
worker1: [2024-06-14 07:25:28,250] [INFO] [config.py:988:print]   checkpoint_tag_validation_fail  False
worker1: [2024-06-14 07:25:28,250] [INFO] [config.py:988:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f195404bb80>
worker1: [2024-06-14 07:25:28,250] [INFO] [config.py:988:print]   communication_data_type ...... None
worker1: [2024-06-14 07:25:28,250] [INFO] [config.py:988:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
worker1: [2024-06-14 07:25:28,250] [INFO] [config.py:988:print]   curriculum_enabled_legacy .... False
worker1: [2024-06-14 07:25:28,250] [INFO] [config.py:988:print]   curriculum_params_legacy ..... False
worker1: [2024-06-14 07:25:28,250] [INFO] [config.py:988:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
worker1: [2024-06-14 07:25:28,250] [INFO] [config.py:988:print]   data_efficiency_enabled ...... False
worker1: [2024-06-14 07:25:28,250] [INFO] [config.py:988:print]   dataloader_drop_last ......... False
worker1: [2024-06-14 07:25:28,250] [INFO] [config.py:988:print]   disable_allgather ............ False
worker1: [2024-06-14 07:25:28,250] [INFO] [config.py:988:print]   dump_state ................... False
worker1: [2024-06-14 07:25:28,250] [INFO] [config.py:988:print]   dynamic_loss_scale_args ...... None
worker1: [2024-06-14 07:25:28,251] [INFO] [config.py:988:print]   eigenvalue_enabled ........... False
worker1: [2024-06-14 07:25:28,251] [INFO] [config.py:988:print]   eigenvalue_gas_boundary_resolution  1
worker1: [2024-06-14 07:25:28,251] [INFO] [config.py:988:print]   eigenvalue_layer_name ........ bert.encoder.layer
worker1: [2024-06-14 07:25:28,251] [INFO] [config.py:988:print]   eigenvalue_layer_num ......... 0
worker1: [2024-06-14 07:25:28,251] [INFO] [config.py:988:print]   eigenvalue_max_iter .......... 100
worker1: [2024-06-14 07:25:28,251] [INFO] [config.py:988:print]   eigenvalue_stability ......... 1e-06
worker1: [2024-06-14 07:25:28,251] [INFO] [config.py:988:print]   eigenvalue_tol ............... 0.01
worker1: [2024-06-14 07:25:28,251] [INFO] [config.py:988:print]   eigenvalue_verbose ........... False
worker1: [2024-06-14 07:25:28,251] [INFO] [config.py:988:print]   elasticity_enabled ........... False
worker1: [2024-06-14 07:25:28,251] [INFO] [config.py:988:print]   flops_profiler_config ........ {
worker1:     "enabled": false, 
worker1:     "recompute_fwd_factor": 0.0, 
worker1:     "profile_step": 1, 
worker1:     "module_depth": -1, 
worker1:     "top_modules": 1, 
worker1:     "detailed": true, 
worker1:     "output_file": null
worker1: }
worker1: [2024-06-14 07:25:28,251] [INFO] [config.py:988:print]   fp16_auto_cast ............... False
worker1: [2024-06-14 07:25:28,251] [INFO] [config.py:988:print]   fp16_enabled ................. True
worker1: [2024-06-14 07:25:28,251] [INFO] [config.py:988:print]   fp16_master_weights_and_gradients  False
worker1: [2024-06-14 07:25:28,251] [INFO] [config.py:988:print]   global_rank .................. 0
worker1: [2024-06-14 07:25:28,251] [INFO] [config.py:988:print]   grad_accum_dtype ............. None
worker1: [2024-06-14 07:25:28,251] [INFO] [config.py:988:print]   gradient_accumulation_steps .. 8
worker1: [2024-06-14 07:25:28,251] [INFO] [config.py:988:print]   gradient_clipping ............ 1.0
worker1: [2024-06-14 07:25:28,251] [INFO] [config.py:988:print]   gradient_predivide_factor .... 1.0
worker1: [2024-06-14 07:25:28,251] [INFO] [config.py:988:print]   graph_harvesting ............. False
worker1: [2024-06-14 07:25:28,251] [INFO] [config.py:988:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
worker1: [2024-06-14 07:25:28,251] [INFO] [config.py:988:print]   initial_dynamic_scale ........ 65536
worker1: [2024-06-14 07:25:28,251] [INFO] [config.py:988:print]   load_universal_checkpoint .... False
worker1: [2024-06-14 07:25:28,251] [INFO] [config.py:988:print]   loss_scale ................... 0
worker1: [2024-06-14 07:25:28,251] [INFO] [config.py:988:print]   memory_breakdown ............. False
worker1: [2024-06-14 07:25:28,251] [INFO] [config.py:988:print]   mics_hierarchial_params_gather  False
worker1: [2024-06-14 07:25:28,251] [INFO] [config.py:988:print]   mics_shard_size .............. -1
worker1: [2024-06-14 07:25:28,251] [INFO] [config.py:988:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
worker1: [2024-06-14 07:25:28,251] [INFO] [config.py:988:print]   nebula_config ................ {
worker1:     "enabled": false, 
worker1:     "persistent_storage_path": null, 
worker1:     "persistent_time_interval": 100, 
worker1:     "num_of_version_in_retention": 2, 
worker1:     "enable_nebula_load": true, 
worker1:     "load_path": null
worker1: }
worker1: [2024-06-14 07:25:28,251] [INFO] [config.py:988:print]   optimizer_legacy_fusion ...... False
worker1: [2024-06-14 07:25:28,251] [INFO] [config.py:988:print]   optimizer_name ............... None
worker1: [2024-06-14 07:25:28,251] [INFO] [config.py:988:print]   optimizer_params ............. None
worker1: [2024-06-14 07:25:28,251] [INFO] [config.py:988:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
worker1: [2024-06-14 07:25:28,251] [INFO] [config.py:988:print]   pld_enabled .................. False
worker1: [2024-06-14 07:25:28,251] [INFO] [config.py:988:print]   pld_params ................... False
worker1: [2024-06-14 07:25:28,251] [INFO] [config.py:988:print]   prescale_gradients ........... False
worker1: [2024-06-14 07:25:28,251] [INFO] [config.py:988:print]   scheduler_name ............... None
worker1: [2024-06-14 07:25:28,251] [INFO] [config.py:988:print]   scheduler_params ............. None
worker1: [2024-06-14 07:25:28,251] [INFO] [config.py:988:print]   seq_parallel_communication_data_type  torch.float32
worker1: [2024-06-14 07:25:28,251] [INFO] [config.py:988:print]   sparse_attention ............. None
worker1: [2024-06-14 07:25:28,251] [INFO] [config.py:988:print]   sparse_gradients_enabled ..... False
worker1: [2024-06-14 07:25:28,251] [INFO] [config.py:988:print]   steps_per_print .............. 10
worker1: [2024-06-14 07:25:28,251] [INFO] [config.py:988:print]   train_batch_size ............. 1024
worker1: [2024-06-14 07:25:28,251] [INFO] [config.py:988:print]   train_micro_batch_size_per_gpu  8
worker1: [2024-06-14 07:25:28,251] [INFO] [config.py:988:print]   use_data_before_expert_parallel_  False
worker1: [2024-06-14 07:25:28,251] [INFO] [config.py:988:print]   use_node_local_storage ....... False
worker1: [2024-06-14 07:25:28,251] [INFO] [config.py:988:print]   wall_clock_breakdown ......... False
worker1: [2024-06-14 07:25:28,252] [INFO] [config.py:988:print]   weight_quantization_config ... None
worker1: [2024-06-14 07:25:28,252] [INFO] [config.py:988:print]   world_size ................... 16
worker1: [2024-06-14 07:25:28,252] [INFO] [config.py:988:print]   zero_allow_untested_optimizer  False
worker1: [2024-06-14 07:25:28,252] [INFO] [config.py:988:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=10000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=False pipeline_loading_checkpoint=False override_module_apply=True
worker1: [2024-06-14 07:25:28,252] [INFO] [config.py:988:print]   zero_enabled ................. True
worker1: [2024-06-14 07:25:28,252] [INFO] [config.py:988:print]   zero_force_ds_cpu_optimizer .. True
worker1: [2024-06-14 07:25:28,252] [INFO] [config.py:988:print]   zero_optimization_stage ...... 3
worker1: [2024-06-14 07:25:28,252] [INFO] [config.py:974:print_user_config]   json = {
worker1:     "train_batch_size": 1.024000e+03, 
worker1:     "train_micro_batch_size_per_gpu": 8, 
worker1:     "steps_per_print": 10, 
worker1:     "zero_optimization": {
worker1:         "stage": 3, 
worker1:         "stage3_param_persistence_threshold": 1.000000e+04, 
worker1:         "offload_param": {
worker1:             "device": "none"
worker1:         }, 
worker1:         "memory_efficient_linear": false
worker1:     }, 
worker1:     "fp16": {
worker1:         "enabled": true
worker1:     }, 
worker1:     "gradient_clipping": 1.0, 
worker1:     "prescale_gradients": false, 
worker1:     "wall_clock_breakdown": false
worker1: }
worker1: *******************[end] Initialized Ref Model [end] (duration: 5.81s)********************
worker1: ************************[start] Initializing Critic Model [start] ************************
worker1: [2024-06-14 07:25:28,435] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 872, num_elems = 20.08B
worker3: Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.21s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.20s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.25s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.33s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.46s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.46s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.48s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.51s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.41s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.40s/it]
worker3: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.41s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.40s/it]
worker2: Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.22s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.22s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.26s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.27s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.46s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.46s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.47s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.49s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.41s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.40s/it]
head: Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.21s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.22s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.31s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.33s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.47s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.47s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.46s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.47s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.41s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.40s/it]
worker2: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.41s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.40s/it]
head: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.42s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.41s/it]
worker2: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.41s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.41s/it]
head: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.40s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.41s/it]
worker2: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.42s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.41s/it]
worker3: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.42s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.41s/it]
worker1: Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.28s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.30s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.31s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:03,  1.95s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.47s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.48s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.50s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.79s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.42s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.42s/it]
head: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.42s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.42s/it]
worker1: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.43s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.42s/it]
worker1: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.43s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.43s/it]
worker3: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.44s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.44s/it]
worker1: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.53s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.62s/it]
worker1: >Creating model from_config took 5.084728002548218 seconds
worker2: Adam Optimizer #1 is created with AVX512 arithmetic capability.
worker2: Config: alpha=0.000005, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
worker1: Adam Optimizer #1 is created with AVX512 arithmetic capability.
worker1: Config: alpha=0.000005, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
worker1: Adam Optimizer #1 is created with AVX512 arithmetic capability.
worker1: Config: alpha=0.000005, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
worker1: Adam Optimizer #1 is created with AVX512 arithmetic capability.
worker1: Config: alpha=0.000005, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
worker2: Adam Optimizer #1 is created with AVX512 arithmetic capability.
worker2: Config: alpha=0.000005, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
worker2: Adam Optimizer #1 is created with AVX512 arithmetic capability.
worker2: Config: alpha=0.000005, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
worker2: Adam Optimizer #1 is created with AVX512 arithmetic capability.
worker2: Config: alpha=0.000005, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
worker3: Adam Optimizer #1 is created with AVX512 arithmetic capability.
worker3: Config: alpha=0.000005, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
head: Adam Optimizer #1 is created with AVX512 arithmetic capability.
head: Config: alpha=0.000005, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
head: Adam Optimizer #1 is created with AVX512 arithmetic capability.
head: Config: alpha=0.000005, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
head: Adam Optimizer #1 is created with AVX512 arithmetic capability.
head: Config: alpha=0.000005, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
head: Adam Optimizer #1 is created with AVX512 arithmetic capability.
head: Config: alpha=0.000005, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
worker3: Adam Optimizer #1 is created with AVX512 arithmetic capability.
worker3: Config: alpha=0.000005, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
worker3: Adam Optimizer #1 is created with AVX512 arithmetic capability.
worker3: Config: alpha=0.000005, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
worker3: Adam Optimizer #1 is created with AVX512 arithmetic capability.
worker3: Config: alpha=0.000005, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
worker1: Adam Optimizer #1 is created with AVX512 arithmetic capability.
worker1: Config: alpha=0.000005, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
worker1: [2024-06-14 07:25:34,776] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.6+b1e9b0fa, git-hash=b1e9b0fa, git-branch=main
worker1: [2024-06-14 07:25:34,798] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
worker1: [2024-06-14 07:25:34,799] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
worker1: [2024-06-14 07:25:34,799] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
worker1: [2024-06-14 07:25:34,809] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
worker1: [2024-06-14 07:25:34,809] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
worker1: [2024-06-14 07:25:34,809] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
worker1: [2024-06-14 07:25:34,809] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 3 optimizer
worker1: [2024-06-14 07:25:35,092] [INFO] [utils.py:791:see_memory_usage] Stage 3 initialize beginning
worker1: [2024-06-14 07:25:35,093] [INFO] [utils.py:792:see_memory_usage] MA 3.18 GB         Max_MA 3.74 GB         CA 12.62 GB         Max_CA 13 GB 
worker1: [2024-06-14 07:25:35,093] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 48.41 GB, percent = 2.6%
worker1: [2024-06-14 07:25:35,095] [INFO] [stage3.py:127:__init__] Reduce bucket size 500,000,000
worker1: [2024-06-14 07:25:35,095] [INFO] [stage3.py:128:__init__] Prefetch bucket size 30000000
worker1: [2024-06-14 07:25:35,362] [INFO] [utils.py:791:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
worker1: [2024-06-14 07:25:35,363] [INFO] [utils.py:792:see_memory_usage] MA 3.18 GB         Max_MA 3.18 GB         CA 12.62 GB         Max_CA 13 GB 
worker1: [2024-06-14 07:25:35,363] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 48.41 GB, percent = 2.6%
worker1: Parameter Offload: Total persistent parameters: 270336 in 66 params
worker1: [2024-06-14 07:25:35,662] [INFO] [utils.py:791:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
worker1: [2024-06-14 07:25:35,663] [INFO] [utils.py:792:see_memory_usage] MA 2.95 GB         Max_MA 3.19 GB         CA 12.62 GB         Max_CA 13 GB 
worker1: [2024-06-14 07:25:35,663] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 48.41 GB, percent = 2.6%
worker1: [2024-06-14 07:25:35,929] [INFO] [utils.py:791:see_memory_usage] Before creating fp16 partitions
worker1: [2024-06-14 07:25:35,930] [INFO] [utils.py:792:see_memory_usage] MA 2.95 GB         Max_MA 2.95 GB         CA 12.62 GB         Max_CA 13 GB 
worker1: [2024-06-14 07:25:35,931] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 48.41 GB, percent = 2.6%
worker1: [2024-06-14 07:25:36,820] [INFO] [utils.py:791:see_memory_usage] After creating fp16 partitions: 2
worker1: [2024-06-14 07:25:36,821] [INFO] [utils.py:792:see_memory_usage] MA 2.17 GB         Max_MA 2.95 GB         CA 12.62 GB         Max_CA 13 GB 
worker1: [2024-06-14 07:25:36,822] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 53.05 GB, percent = 2.8%
worker1: [2024-06-14 07:25:37,086] [INFO] [utils.py:791:see_memory_usage] Before creating fp32 partitions
worker1: [2024-06-14 07:25:37,087] [INFO] [utils.py:792:see_memory_usage] MA 2.17 GB         Max_MA 2.17 GB         CA 12.62 GB         Max_CA 13 GB 
worker1: [2024-06-14 07:25:37,087] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 53.05 GB, percent = 2.8%
worker1: [2024-06-14 07:25:37,488] [INFO] [utils.py:791:see_memory_usage] After creating fp32 partitions
worker1: [2024-06-14 07:25:37,489] [INFO] [utils.py:792:see_memory_usage] MA 2.17 GB         Max_MA 2.17 GB         CA 12.62 GB         Max_CA 13 GB 
worker1: [2024-06-14 07:25:37,489] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 54.59 GB, percent = 2.9%
worker1: [2024-06-14 07:25:38,063] [INFO] [utils.py:791:see_memory_usage] Before initializing optimizer states
worker1: [2024-06-14 07:25:38,063] [INFO] [utils.py:792:see_memory_usage] MA 2.17 GB         Max_MA 2.17 GB         CA 12.62 GB         Max_CA 13 GB 
worker1: [2024-06-14 07:25:38,064] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 72.61 GB, percent = 3.9%
worker1: [2024-06-14 07:25:38,758] [INFO] [utils.py:791:see_memory_usage] After initializing optimizer states
worker1: [2024-06-14 07:25:38,759] [INFO] [utils.py:792:see_memory_usage] MA 2.17 GB         Max_MA 2.17 GB         CA 12.62 GB         Max_CA 13 GB 
worker1: [2024-06-14 07:25:38,760] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 73.08 GB, percent = 3.9%
worker1: [2024-06-14 07:25:38,760] [INFO] [stage3.py:479:_setup_for_real_optimizer] optimizer state initialized
worker1: [2024-06-14 07:25:39,610] [INFO] [utils.py:791:see_memory_usage] After initializing ZeRO optimizer
worker1: [2024-06-14 07:25:39,611] [INFO] [utils.py:792:see_memory_usage] MA 3.1 GB         Max_MA 3.59 GB         CA 13.55 GB         Max_CA 14 GB 
worker1: [2024-06-14 07:25:39,612] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 76.17 GB, percent = 4.0%
worker1: [2024-06-14 07:25:39,612] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
worker1: [2024-06-14 07:25:39,612] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
worker1: [2024-06-14 07:25:39,612] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x7f19143dfc10>
worker1: [2024-06-14 07:25:39,612] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[5e-06, 5e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
worker1: [2024-06-14 07:25:39,613] [INFO] [config.py:984:print] DeepSpeedEngine configuration:
worker1: [2024-06-14 07:25:39,613] [INFO] [config.py:988:print]   activation_checkpointing_config  {
worker1:     "partition_activations": false, 
worker1:     "contiguous_memory_optimization": false, 
worker1:     "cpu_checkpointing": false, 
worker1:     "number_checkpoints": null, 
worker1:     "synchronize_checkpoint_boundary": false, 
worker1:     "profile": false
worker1: }
worker1: [2024-06-14 07:25:39,613] [INFO] [config.py:988:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
worker1: [2024-06-14 07:25:39,613] [INFO] [config.py:988:print]   amp_enabled .................. False
worker1: [2024-06-14 07:25:39,613] [INFO] [config.py:988:print]   amp_params ................... False
worker1: [2024-06-14 07:25:39,614] [INFO] [config.py:988:print]   autotuning_config ............ {
worker1:     "enabled": false, 
worker1:     "start_step": null, 
worker1:     "end_step": null, 
worker1:     "metric_path": null, 
worker1:     "arg_mappings": null, 
worker1:     "metric": "throughput", 
worker1:     "model_info": null, 
worker1:     "results_dir": "autotuning_results", 
worker1:     "exps_dir": "autotuning_exps", 
worker1:     "overwrite": true, 
worker1:     "fast": true, 
worker1:     "start_profile_step": 3, 
worker1:     "end_profile_step": 5, 
worker1:     "tuner_type": "gridsearch", 
worker1:     "tuner_early_stopping": 5, 
worker1:     "tuner_num_trials": 50, 
worker1:     "model_info_path": null, 
worker1:     "mp_size": 1, 
worker1:     "max_train_batch_size": null, 
worker1:     "min_train_batch_size": 1, 
worker1:     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
worker1:     "min_train_micro_batch_size_per_gpu": 1, 
worker1:     "num_tuning_micro_batch_sizes": 3
worker1: }
worker1: [2024-06-14 07:25:39,614] [INFO] [config.py:988:print]   bfloat16_enabled ............. False
worker1: [2024-06-14 07:25:39,614] [INFO] [config.py:988:print]   checkpoint_parallel_write_pipeline  False
worker1: [2024-06-14 07:25:39,614] [INFO] [config.py:988:print]   checkpoint_tag_validation_enabled  True
worker1: [2024-06-14 07:25:39,614] [INFO] [config.py:988:print]   checkpoint_tag_validation_fail  False
worker1: [2024-06-14 07:25:39,614] [INFO] [config.py:988:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f19142d0910>
worker1: [2024-06-14 07:25:39,614] [INFO] [config.py:988:print]   communication_data_type ...... None
worker1: [2024-06-14 07:25:39,614] [INFO] [config.py:988:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
worker1: [2024-06-14 07:25:39,614] [INFO] [config.py:988:print]   curriculum_enabled_legacy .... False
worker1: [2024-06-14 07:25:39,614] [INFO] [config.py:988:print]   curriculum_params_legacy ..... False
worker1: [2024-06-14 07:25:39,614] [INFO] [config.py:988:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
worker1: [2024-06-14 07:25:39,614] [INFO] [config.py:988:print]   data_efficiency_enabled ...... False
worker1: [2024-06-14 07:25:39,614] [INFO] [config.py:988:print]   dataloader_drop_last ......... False
worker1: [2024-06-14 07:25:39,614] [INFO] [config.py:988:print]   disable_allgather ............ False
worker1: [2024-06-14 07:25:39,614] [INFO] [config.py:988:print]   dump_state ................... False
worker1: [2024-06-14 07:25:39,614] [INFO] [config.py:988:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 100, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
worker1: [2024-06-14 07:25:39,614] [INFO] [config.py:988:print]   eigenvalue_enabled ........... False
worker1: [2024-06-14 07:25:39,614] [INFO] [config.py:988:print]   eigenvalue_gas_boundary_resolution  1
worker1: [2024-06-14 07:25:39,614] [INFO] [config.py:988:print]   eigenvalue_layer_name ........ bert.encoder.layer
worker1: [2024-06-14 07:25:39,614] [INFO] [config.py:988:print]   eigenvalue_layer_num ......... 0
worker1: [2024-06-14 07:25:39,614] [INFO] [config.py:988:print]   eigenvalue_max_iter .......... 100
worker1: [2024-06-14 07:25:39,614] [INFO] [config.py:988:print]   eigenvalue_stability ......... 1e-06
worker1: [2024-06-14 07:25:39,614] [INFO] [config.py:988:print]   eigenvalue_tol ............... 0.01
worker1: [2024-06-14 07:25:39,614] [INFO] [config.py:988:print]   eigenvalue_verbose ........... False
worker1: [2024-06-14 07:25:39,614] [INFO] [config.py:988:print]   elasticity_enabled ........... False
worker1: [2024-06-14 07:25:39,614] [INFO] [config.py:988:print]   flops_profiler_config ........ {
worker1:     "enabled": false, 
worker1:     "recompute_fwd_factor": 0.0, 
worker1:     "profile_step": 1, 
worker1:     "module_depth": -1, 
worker1:     "top_modules": 1, 
worker1:     "detailed": true, 
worker1:     "output_file": null
worker1: }
worker1: [2024-06-14 07:25:39,614] [INFO] [config.py:988:print]   fp16_auto_cast ............... False
worker1: [2024-06-14 07:25:39,614] [INFO] [config.py:988:print]   fp16_enabled ................. True
worker1: [2024-06-14 07:25:39,614] [INFO] [config.py:988:print]   fp16_master_weights_and_gradients  False
worker1: [2024-06-14 07:25:39,614] [INFO] [config.py:988:print]   global_rank .................. 0
worker1: [2024-06-14 07:25:39,614] [INFO] [config.py:988:print]   grad_accum_dtype ............. None
worker1: [2024-06-14 07:25:39,614] [INFO] [config.py:988:print]   gradient_accumulation_steps .. 8
worker1: [2024-06-14 07:25:39,614] [INFO] [config.py:988:print]   gradient_clipping ............ 1.0
worker1: [2024-06-14 07:25:39,614] [INFO] [config.py:988:print]   gradient_predivide_factor .... 1.0
worker1: [2024-06-14 07:25:39,614] [INFO] [config.py:988:print]   graph_harvesting ............. False
worker1: [2024-06-14 07:25:39,614] [INFO] [config.py:988:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
worker1: [2024-06-14 07:25:39,614] [INFO] [config.py:988:print]   initial_dynamic_scale ........ 65536
worker1: [2024-06-14 07:25:39,614] [INFO] [config.py:988:print]   load_universal_checkpoint .... False
worker1: [2024-06-14 07:25:39,614] [INFO] [config.py:988:print]   loss_scale ................... 0
worker1: [2024-06-14 07:25:39,614] [INFO] [config.py:988:print]   memory_breakdown ............. False
worker1: [2024-06-14 07:25:39,614] [INFO] [config.py:988:print]   mics_hierarchial_params_gather  False
worker1: [2024-06-14 07:25:39,614] [INFO] [config.py:988:print]   mics_shard_size .............. -1
worker1: [2024-06-14 07:25:39,615] [INFO] [config.py:988:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='step3_tensorboard/ds_tensorboard_logs/', job_name='step3_critic_tensorboard') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
worker1: [2024-06-14 07:25:39,615] [INFO] [config.py:988:print]   nebula_config ................ {
worker1:     "enabled": false, 
worker1:     "persistent_storage_path": null, 
worker1:     "persistent_time_interval": 100, 
worker1:     "num_of_version_in_retention": 2, 
worker1:     "enable_nebula_load": true, 
worker1:     "load_path": null
worker1: }
worker1: [2024-06-14 07:25:39,615] [INFO] [config.py:988:print]   optimizer_legacy_fusion ...... False
worker1: [2024-06-14 07:25:39,615] [INFO] [config.py:988:print]   optimizer_name ............... None
worker1: [2024-06-14 07:25:39,615] [INFO] [config.py:988:print]   optimizer_params ............. None
worker1: [2024-06-14 07:25:39,615] [INFO] [config.py:988:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
worker1: [2024-06-14 07:25:39,615] [INFO] [config.py:988:print]   pld_enabled .................. False
worker1: [2024-06-14 07:25:39,615] [INFO] [config.py:988:print]   pld_params ................... False
worker1: [2024-06-14 07:25:39,615] [INFO] [config.py:988:print]   prescale_gradients ........... False
worker1: [2024-06-14 07:25:39,615] [INFO] [config.py:988:print]   scheduler_name ............... None
worker1: [2024-06-14 07:25:39,615] [INFO] [config.py:988:print]   scheduler_params ............. None
worker1: [2024-06-14 07:25:39,615] [INFO] [config.py:988:print]   seq_parallel_communication_data_type  torch.float32
worker1: [2024-06-14 07:25:39,615] [INFO] [config.py:988:print]   sparse_attention ............. None
worker1: [2024-06-14 07:25:39,615] [INFO] [config.py:988:print]   sparse_gradients_enabled ..... False
worker1: [2024-06-14 07:25:39,615] [INFO] [config.py:988:print]   steps_per_print .............. 10
worker1: [2024-06-14 07:25:39,615] [INFO] [config.py:988:print]   train_batch_size ............. 1024
worker1: [2024-06-14 07:25:39,615] [INFO] [config.py:988:print]   train_micro_batch_size_per_gpu  8
worker1: [2024-06-14 07:25:39,615] [INFO] [config.py:988:print]   use_data_before_expert_parallel_  False
worker1: [2024-06-14 07:25:39,615] [INFO] [config.py:988:print]   use_node_local_storage ....... False
worker1: [2024-06-14 07:25:39,615] [INFO] [config.py:988:print]   wall_clock_breakdown ......... False
worker1: [2024-06-14 07:25:39,615] [INFO] [config.py:988:print]   weight_quantization_config ... None
worker1: [2024-06-14 07:25:39,615] [INFO] [config.py:988:print]   world_size ................... 16
worker1: [2024-06-14 07:25:39,615] [INFO] [config.py:988:print]   zero_allow_untested_optimizer  False
worker1: [2024-06-14 07:25:39,615] [INFO] [config.py:988:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=30000000 param_persistence_threshold=10000 model_persistence_threshold=sys.maxsize max_live_parameters=30000000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=False pipeline_loading_checkpoint=False override_module_apply=True
worker1: [2024-06-14 07:25:39,615] [INFO] [config.py:988:print]   zero_enabled ................. True
worker1: [2024-06-14 07:25:39,615] [INFO] [config.py:988:print]   zero_force_ds_cpu_optimizer .. True
worker1: [2024-06-14 07:25:39,615] [INFO] [config.py:988:print]   zero_optimization_stage ...... 3
worker1: [2024-06-14 07:25:39,615] [INFO] [config.py:974:print_user_config]   json = {
worker1:     "train_batch_size": 1.024000e+03, 
worker1:     "train_micro_batch_size_per_gpu": 8, 
worker1:     "steps_per_print": 10, 
worker1:     "zero_optimization": {
worker1:         "stage": 3, 
worker1:         "offload_param": {
worker1:             "device": "cpu"
worker1:         }, 
worker1:         "offload_optimizer": {
worker1:             "device": "cpu"
worker1:         }, 
worker1:         "stage3_param_persistence_threshold": 1.000000e+04, 
worker1:         "stage3_max_live_parameters": 3.000000e+07, 
worker1:         "stage3_prefetch_bucket_size": 3.000000e+07, 
worker1:         "memory_efficient_linear": false
worker1:     }, 
worker1:     "fp16": {
worker1:         "enabled": true, 
worker1:         "loss_scale_window": 100
worker1:     }, 
worker1:     "gradient_clipping": 1.0, 
worker1:     "prescale_gradients": false, 
worker1:     "wall_clock_breakdown": false, 
worker1:     "hybrid_engine": {
worker1:         "enabled": false, 
worker1:         "max_out_tokens": 512, 
worker1:         "inference_tp_size": 1, 
worker1:         "release_inference_cache": false, 
worker1:         "pin_parameters": true, 
worker1:         "tp_gather_partition_size": 8
worker1:     }, 
worker1:     "tensorboard": {
worker1:         "enabled": false, 
worker1:         "output_path": "step3_tensorboard/ds_tensorboard_logs/", 
worker1:         "job_name": "step3_critic_tensorboard"
worker1:     }
worker1: }
worker1: *****************[end] Initialized Critic Model [end] (duration: 11.36s)******************
worker1: ************************[start] Initializing Reward Model [start] ************************
worker1: [2024-06-14 07:25:39,751] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1162, num_elems = 26.69B
worker3: Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.15s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.21s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.27s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.30s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.42s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.44s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.47s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.48s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.35s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.35s/it]
head: Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.16s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.27s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.31s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.33s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.40s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.41s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.44s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.44s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.34s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.35s/it]
worker2: Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.16s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.26s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.32s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.33s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.40s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.42s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.43s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.40s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.34s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.35s/it]
head: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.34s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.35s/it]
worker2: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.35s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.35s/it]
worker2: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.34s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.35s/it]
worker2: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.36s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.35s/it]
head: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.34s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.35s/it]
head: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.36s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.35s/it]
worker1: Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.27s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.27s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.35s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:03,  1.86s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.41s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.46s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.47s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.71s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.35s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.36s/it]
worker1: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.35s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.36s/it]
worker3: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.37s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.36s/it]
worker1: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.36s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.37s/it]
worker3: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.38s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.39s/it]
worker3: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.39s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.40s/it]
worker1: Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.48s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.56s/it]
worker1: >Creating model from_config took 4.863620281219482 seconds
worker1: [2024-06-14 07:25:44,479] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.6+b1e9b0fa, git-hash=b1e9b0fa, git-branch=main
worker1: [2024-06-14 07:25:44,501] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
worker1: [2024-06-14 07:25:44,503] [INFO] [logging.py:96:log_dist] [Rank 0] Creating ZeRO Offload
worker1: [2024-06-14 07:25:44,782] [INFO] [utils.py:791:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
worker1: [2024-06-14 07:25:44,783] [INFO] [utils.py:792:see_memory_usage] MA 4.17 GB         Max_MA 4.74 GB         CA 13.61 GB         Max_CA 14 GB 
worker1: [2024-06-14 07:25:44,783] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 76.2 GB, percent = 4.1%
worker1: Parameter Offload: Total persistent parameters: 270336 in 66 params
worker1: [2024-06-14 07:25:45,114] [INFO] [utils.py:791:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
worker1: [2024-06-14 07:25:45,115] [INFO] [utils.py:792:see_memory_usage] MA 3.94 GB         Max_MA 4.19 GB         CA 13.61 GB         Max_CA 14 GB 
worker1: [2024-06-14 07:25:45,115] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 76.2 GB, percent = 4.1%
worker1: [2024-06-14 07:25:45,116] [INFO] [config.py:984:print] DeepSpeedEngine configuration:
worker1: [2024-06-14 07:25:45,116] [INFO] [config.py:988:print]   activation_checkpointing_config  {
worker1:     "partition_activations": false, 
worker1:     "contiguous_memory_optimization": false, 
worker1:     "cpu_checkpointing": false, 
worker1:     "number_checkpoints": null, 
worker1:     "synchronize_checkpoint_boundary": false, 
worker1:     "profile": false
worker1: }
worker1: [2024-06-14 07:25:45,116] [INFO] [config.py:988:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
worker1: [2024-06-14 07:25:45,116] [INFO] [config.py:988:print]   amp_enabled .................. False
worker1: [2024-06-14 07:25:45,116] [INFO] [config.py:988:print]   amp_params ................... False
worker1: [2024-06-14 07:25:45,117] [INFO] [config.py:988:print]   autotuning_config ............ {
worker1:     "enabled": false, 
worker1:     "start_step": null, 
worker1:     "end_step": null, 
worker1:     "metric_path": null, 
worker1:     "arg_mappings": null, 
worker1:     "metric": "throughput", 
worker1:     "model_info": null, 
worker1:     "results_dir": "autotuning_results", 
worker1:     "exps_dir": "autotuning_exps", 
worker1:     "overwrite": true, 
worker1:     "fast": true, 
worker1:     "start_profile_step": 3, 
worker1:     "end_profile_step": 5, 
worker1:     "tuner_type": "gridsearch", 
worker1:     "tuner_early_stopping": 5, 
worker1:     "tuner_num_trials": 50, 
worker1:     "model_info_path": null, 
worker1:     "mp_size": 1, 
worker1:     "max_train_batch_size": null, 
worker1:     "min_train_batch_size": 1, 
worker1:     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
worker1:     "min_train_micro_batch_size_per_gpu": 1, 
worker1:     "num_tuning_micro_batch_sizes": 3
worker1: }
worker1: [2024-06-14 07:25:45,117] [INFO] [config.py:988:print]   bfloat16_enabled ............. False
worker1: [2024-06-14 07:25:45,117] [INFO] [config.py:988:print]   checkpoint_parallel_write_pipeline  False
worker1: [2024-06-14 07:25:45,117] [INFO] [config.py:988:print]   checkpoint_tag_validation_enabled  True
worker1: [2024-06-14 07:25:45,117] [INFO] [config.py:988:print]   checkpoint_tag_validation_fail  False
worker1: [2024-06-14 07:25:45,117] [INFO] [config.py:988:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f18f4447310>
worker1: [2024-06-14 07:25:45,117] [INFO] [config.py:988:print]   communication_data_type ...... None
worker1: [2024-06-14 07:25:45,117] [INFO] [config.py:988:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
worker1: [2024-06-14 07:25:45,117] [INFO] [config.py:988:print]   curriculum_enabled_legacy .... False
worker1: [2024-06-14 07:25:45,117] [INFO] [config.py:988:print]   curriculum_params_legacy ..... False
worker1: [2024-06-14 07:25:45,117] [INFO] [config.py:988:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
worker1: [2024-06-14 07:25:45,117] [INFO] [config.py:988:print]   data_efficiency_enabled ...... False
worker1: [2024-06-14 07:25:45,117] [INFO] [config.py:988:print]   dataloader_drop_last ......... False
worker1: [2024-06-14 07:25:45,117] [INFO] [config.py:988:print]   disable_allgather ............ False
worker1: [2024-06-14 07:25:45,117] [INFO] [config.py:988:print]   dump_state ................... False
worker1: [2024-06-14 07:25:45,117] [INFO] [config.py:988:print]   dynamic_loss_scale_args ...... None
worker1: [2024-06-14 07:25:45,117] [INFO] [config.py:988:print]   eigenvalue_enabled ........... False
worker1: [2024-06-14 07:25:45,117] [INFO] [config.py:988:print]   eigenvalue_gas_boundary_resolution  1
worker1: [2024-06-14 07:25:45,117] [INFO] [config.py:988:print]   eigenvalue_layer_name ........ bert.encoder.layer
worker1: [2024-06-14 07:25:45,117] [INFO] [config.py:988:print]   eigenvalue_layer_num ......... 0
worker1: [2024-06-14 07:25:45,117] [INFO] [config.py:988:print]   eigenvalue_max_iter .......... 100
worker1: [2024-06-14 07:25:45,117] [INFO] [config.py:988:print]   eigenvalue_stability ......... 1e-06
worker1: [2024-06-14 07:25:45,117] [INFO] [config.py:988:print]   eigenvalue_tol ............... 0.01
worker1: [2024-06-14 07:25:45,117] [INFO] [config.py:988:print]   eigenvalue_verbose ........... False
worker1: [2024-06-14 07:25:45,117] [INFO] [config.py:988:print]   elasticity_enabled ........... False
worker1: [2024-06-14 07:25:45,117] [INFO] [config.py:988:print]   flops_profiler_config ........ {
worker1:     "enabled": false, 
worker1:     "recompute_fwd_factor": 0.0, 
worker1:     "profile_step": 1, 
worker1:     "module_depth": -1, 
worker1:     "top_modules": 1, 
worker1:     "detailed": true, 
worker1:     "output_file": null
worker1: }
worker1: [2024-06-14 07:25:45,117] [INFO] [config.py:988:print]   fp16_auto_cast ............... False
worker1: [2024-06-14 07:25:45,117] [INFO] [config.py:988:print]   fp16_enabled ................. True
worker1: [2024-06-14 07:25:45,117] [INFO] [config.py:988:print]   fp16_master_weights_and_gradients  False
worker1: [2024-06-14 07:25:45,117] [INFO] [config.py:988:print]   global_rank .................. 0
worker1: [2024-06-14 07:25:45,117] [INFO] [config.py:988:print]   grad_accum_dtype ............. None
worker1: [2024-06-14 07:25:45,117] [INFO] [config.py:988:print]   gradient_accumulation_steps .. 8
worker1: [2024-06-14 07:25:45,117] [INFO] [config.py:988:print]   gradient_clipping ............ 1.0
worker1: [2024-06-14 07:25:45,117] [INFO] [config.py:988:print]   gradient_predivide_factor .... 1.0
worker1: [2024-06-14 07:25:45,117] [INFO] [config.py:988:print]   graph_harvesting ............. False
worker1: [2024-06-14 07:25:45,117] [INFO] [config.py:988:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
worker1: [2024-06-14 07:25:45,117] [INFO] [config.py:988:print]   initial_dynamic_scale ........ 65536
worker1: [2024-06-14 07:25:45,117] [INFO] [config.py:988:print]   load_universal_checkpoint .... False
worker1: [2024-06-14 07:25:45,117] [INFO] [config.py:988:print]   loss_scale ................... 0
worker1: [2024-06-14 07:25:45,117] [INFO] [config.py:988:print]   memory_breakdown ............. False
worker1: [2024-06-14 07:25:45,117] [INFO] [config.py:988:print]   mics_hierarchial_params_gather  False
worker1: [2024-06-14 07:25:45,117] [INFO] [config.py:988:print]   mics_shard_size .............. -1
worker1: [2024-06-14 07:25:45,117] [INFO] [config.py:988:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
worker1: [2024-06-14 07:25:45,117] [INFO] [config.py:988:print]   nebula_config ................ {
worker1:     "enabled": false, 
worker1:     "persistent_storage_path": null, 
worker1:     "persistent_time_interval": 100, 
worker1:     "num_of_version_in_retention": 2, 
worker1:     "enable_nebula_load": true, 
worker1:     "load_path": null
worker1: }
worker1: [2024-06-14 07:25:45,118] [INFO] [config.py:988:print]   optimizer_legacy_fusion ...... False
worker1: [2024-06-14 07:25:45,118] [INFO] [config.py:988:print]   optimizer_name ............... None
worker1: [2024-06-14 07:25:45,118] [INFO] [config.py:988:print]   optimizer_params ............. None
worker1: [2024-06-14 07:25:45,118] [INFO] [config.py:988:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
worker1: [2024-06-14 07:25:45,118] [INFO] [config.py:988:print]   pld_enabled .................. False
worker1: [2024-06-14 07:25:45,118] [INFO] [config.py:988:print]   pld_params ................... False
worker1: [2024-06-14 07:25:45,118] [INFO] [config.py:988:print]   prescale_gradients ........... False
worker1: [2024-06-14 07:25:45,118] [INFO] [config.py:988:print]   scheduler_name ............... None
worker1: [2024-06-14 07:25:45,118] [INFO] [config.py:988:print]   scheduler_params ............. None
worker1: [2024-06-14 07:25:45,118] [INFO] [config.py:988:print]   seq_parallel_communication_data_type  torch.float32
worker1: [2024-06-14 07:25:45,118] [INFO] [config.py:988:print]   sparse_attention ............. None
worker1: [2024-06-14 07:25:45,118] [INFO] [config.py:988:print]   sparse_gradients_enabled ..... False
worker1: [2024-06-14 07:25:45,118] [INFO] [config.py:988:print]   steps_per_print .............. 10
worker1: [2024-06-14 07:25:45,118] [INFO] [config.py:988:print]   train_batch_size ............. 1024
worker1: [2024-06-14 07:25:45,118] [INFO] [config.py:988:print]   train_micro_batch_size_per_gpu  8
worker1: [2024-06-14 07:25:45,118] [INFO] [config.py:988:print]   use_data_before_expert_parallel_  False
worker1: [2024-06-14 07:25:45,118] [INFO] [config.py:988:print]   use_node_local_storage ....... False
worker1: [2024-06-14 07:25:45,118] [INFO] [config.py:988:print]   wall_clock_breakdown ......... False
worker1: [2024-06-14 07:25:45,118] [INFO] [config.py:988:print]   weight_quantization_config ... None
worker1: [2024-06-14 07:25:45,118] [INFO] [config.py:988:print]   world_size ................... 16
worker1: [2024-06-14 07:25:45,118] [INFO] [config.py:988:print]   zero_allow_untested_optimizer  False
worker1: [2024-06-14 07:25:45,118] [INFO] [config.py:988:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=10000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=False pipeline_loading_checkpoint=False override_module_apply=True
worker1: [2024-06-14 07:25:45,118] [INFO] [config.py:988:print]   zero_enabled ................. True
worker1: [2024-06-14 07:25:45,118] [INFO] [config.py:988:print]   zero_force_ds_cpu_optimizer .. True
worker1: [2024-06-14 07:25:45,118] [INFO] [config.py:988:print]   zero_optimization_stage ...... 3
worker1: [2024-06-14 07:25:45,118] [INFO] [config.py:974:print_user_config]   json = {
worker1:     "train_batch_size": 1.024000e+03, 
worker1:     "train_micro_batch_size_per_gpu": 8, 
worker1:     "steps_per_print": 10, 
worker1:     "zero_optimization": {
worker1:         "stage": 3, 
worker1:         "stage3_param_persistence_threshold": 1.000000e+04, 
worker1:         "offload_param": {
worker1:             "device": "cpu"
worker1:         }, 
worker1:         "memory_efficient_linear": false
worker1:     }, 
worker1:     "fp16": {
worker1:         "enabled": true
worker1:     }, 
worker1:     "gradient_clipping": 1.0, 
worker1:     "prescale_gradients": false, 
worker1:     "wall_clock_breakdown": false
worker1: }
worker1: ******************[end] Initialized Reward Model [end] (duration: 5.50s)******************
worker1: ***** Running training (total_iters=43) *****
worker1: Beginning of Epoch 1/1, Total Generation Batches 349
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
worker1:   warnings.warn(
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
worker1:   warnings.warn(
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
worker1:   warnings.warn(
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
head:   warnings.warn(
worker3: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
worker3:   warnings.warn(
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
worker1:   warnings.warn(
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
head:   warnings.warn(
worker2: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
worker2:   warnings.warn(
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
worker1:   warnings.warn(
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
worker3: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
worker2: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
head:   warnings.warn(
worker3:   warnings.warn(
worker2:   warnings.warn(
worker1:   warnings.warn(
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
worker3: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
worker2: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
head:   warnings.warn(
worker3:   warnings.warn(
worker2:   warnings.warn(
worker1:   warnings.warn(
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
worker3: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
worker2: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
head:   warnings.warn(
worker3:   warnings.warn(
worker2:   warnings.warn(
worker1:   warnings.warn(
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
worker3: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
worker2: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
head:   warnings.warn(
worker3:   warnings.warn(
worker2:   warnings.warn(
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
worker3: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
worker2: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
head:   warnings.warn(
worker3:   warnings.warn(
worker2:   warnings.warn(
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
worker3: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
worker2: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
head:   warnings.warn(
worker3:   warnings.warn(
worker2:   warnings.warn(
worker3: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
worker3:   warnings.warn(
worker2: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
worker2:   warnings.warn(
worker1: ------------------------------------------------------
worker1: Free memory : 60.224792 (GigaBytes)  
worker1: Total memory: 79.346863 (GigaBytes)  
worker1: Requested memory: 22.500000 (GigaBytes) 
worker1: Setting maximum total tokens (input + output) to 2048 
worker1: WorkSpace: 0x7f0540000000 
worker1: ------------------------------------------------------
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
head:   warnings.warn(
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
head:   warnings.warn(
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
worker1:   warnings.warn(
worker3: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
worker3:   warnings.warn(
worker2: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
worker2:   warnings.warn(
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
head:   warnings.warn(
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
head:   warnings.warn(
worker2: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
worker2:   warnings.warn(
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
worker1:   warnings.warn(
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
worker2: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
worker3: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
head:   warnings.warn(
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
worker2:   warnings.warn(
worker3:   warnings.warn(
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
worker1:   warnings.warn(
worker2: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
worker3: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
head:   warnings.warn(
worker2:   warnings.warn(
worker3:   warnings.warn(
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
worker2: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
worker3: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
worker1:   warnings.warn(
head:   warnings.warn(
worker2:   warnings.warn(
worker3:   warnings.warn(
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
worker2: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
worker3: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
worker1:   warnings.warn(
head:   warnings.warn(
worker2:   warnings.warn(
worker3:   warnings.warn(
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
worker1:   warnings.warn(
worker3: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
worker2: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
worker3:   warnings.warn(
worker2:   warnings.warn(
worker1:   warnings.warn(
worker3: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
worker2: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
worker3:   warnings.warn(
worker2:   warnings.warn(
worker1:   warnings.warn(
worker3: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
worker3:   warnings.warn(
worker1: |E2E latency=50.83s |Gather latency=0.49s (0.96%) |Generate time=41.23s (81.12%) |Training time=0.00s (0.00%) |Others=9.60 (18.88%)|CurSamplesPerSec=2.52 |AvgSamplesPerSec=2.52
worker1: |E2E latency=50.84s |Gather latency=0.75s (1.47%) |Generate time=40.65s (79.96%) |Training time=0.00s (0.00%) |Others=10.19 (20.04%)|CurSamplesPerSec=2.52 |AvgSamplesPerSec=2.52
worker1: |E2E latency=52.99s |Gather latency=0.84s (1.59%) |Generate time=40.76s (76.91%) |Training time=0.00s (0.00%) |Others=12.23 (23.09%)|CurSamplesPerSec=2.42 |AvgSamplesPerSec=2.48
worker1: |E2E latency=51.72s |Gather latency=0.83s (1.60%) |Generate time=40.19s (77.70%) |Training time=0.00s (0.00%) |Others=11.53 (22.30%)|CurSamplesPerSec=2.47 |AvgSamplesPerSec=2.48
worker1: |E2E latency=52.30s |Gather latency=0.83s (1.59%) |Generate time=40.61s (77.66%) |Training time=0.00s (0.00%) |Others=11.68 (22.34%)|CurSamplesPerSec=2.45 |AvgSamplesPerSec=2.47
worker1: |E2E latency=51.58s |Gather latency=0.88s (1.70%) |Generate time=40.38s (78.29%) |Training time=0.00s (0.00%) |Others=11.20 (21.71%)|CurSamplesPerSec=2.48 |AvgSamplesPerSec=2.48
worker1: |E2E latency=52.47s |Gather latency=0.84s (1.60%) |Generate time=40.70s (77.56%) |Training time=0.00s (0.00%) |Others=11.77 (22.44%)|CurSamplesPerSec=2.44 |AvgSamplesPerSec=2.47
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:173343 [2] NCCL INFO Using network IBext
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:285889 [0] NCCL INFO Using network IBext
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:173345 [0] NCCL INFO Using network IBext
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:165767 [2] NCCL INFO Using network IBext
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170623:173342 [3] NCCL INFO Using network IBext
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190807:195178 [3] NCCL INFO Using network IBext
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:195180 [2] NCCL INFO Using network IBext
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170621:173344 [1] NCCL INFO Using network IBext
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:195179 [0] NCCL INFO Using network IBext
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163003:165768 [3] NCCL INFO Using network IBext
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283113:285890 [1] NCCL INFO Using network IBext
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:285891 [2] NCCL INFO Using network IBext
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283115:285892 [3] NCCL INFO Using network IBext
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163001:165765 [1] NCCL INFO Using network IBext
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:165766 [0] NCCL INFO Using network IBext
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190805:195181 [1] NCCL INFO Using network IBext
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190807:195178 [3] NCCL INFO comm 0x7f87d4078410 rank 15 nranks 16 cudaDev 3 nvmlDev 3 busId 6a000 commId 0x373ff5e8ed960245 - Init START
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:195180 [2] NCCL INFO comm 0x7f09bc0793f0 rank 14 nranks 16 cudaDev 2 nvmlDev 2 busId 65000 commId 0x373ff5e8ed960245 - Init START
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190805:195181 [1] NCCL INFO comm 0x7ef514078b80 rank 13 nranks 16 cudaDev 1 nvmlDev 1 busId 46000 commId 0x373ff5e8ed960245 - Init START
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283115:285892 [3] NCCL INFO comm 0x7f2f68073090 rank 3 nranks 16 cudaDev 3 nvmlDev 3 busId 6a000 commId 0x373ff5e8ed960245 - Init START
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:195179 [0] NCCL INFO comm 0x7f9fcc074700 rank 12 nranks 16 cudaDev 0 nvmlDev 0 busId 40000 commId 0x373ff5e8ed960245 - Init START
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:285891 [2] NCCL INFO comm 0x7f0b94071dd0 rank 2 nranks 16 cudaDev 2 nvmlDev 2 busId 65000 commId 0x373ff5e8ed960245 - Init START
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283113:285890 [1] NCCL INFO comm 0x7fa154072e40 rank 1 nranks 16 cudaDev 1 nvmlDev 1 busId 46000 commId 0x373ff5e8ed960245 - Init START
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:165766 [0] NCCL INFO comm 0x7f36880788d0 rank 4 nranks 16 cudaDev 0 nvmlDev 0 busId 4d000 commId 0x373ff5e8ed960245 - Init START
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170623:173342 [3] NCCL INFO comm 0x7f7d00079780 rank 11 nranks 16 cudaDev 3 nvmlDev 3 busId 6f000 commId 0x373ff5e8ed960245 - Init START
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:173345 [0] NCCL INFO comm 0x7ef584078cf0 rank 8 nranks 16 cudaDev 0 nvmlDev 0 busId 4d000 commId 0x373ff5e8ed960245 - Init START
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:285889 [0] NCCL INFO comm 0x7f0c10078cb0 rank 0 nranks 16 cudaDev 0 nvmlDev 0 busId 40000 commId 0x373ff5e8ed960245 - Init START
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170621:173344 [1] NCCL INFO comm 0x7f21fc076650 rank 9 nranks 16 cudaDev 1 nvmlDev 1 busId 52000 commId 0x373ff5e8ed960245 - Init START
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:173343 [2] NCCL INFO comm 0x7f6b60076a10 rank 10 nranks 16 cudaDev 2 nvmlDev 2 busId 69000 commId 0x373ff5e8ed960245 - Init START
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:165767 [2] NCCL INFO comm 0x7fa65c078d00 rank 6 nranks 16 cudaDev 2 nvmlDev 2 busId 69000 commId 0x373ff5e8ed960245 - Init START
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163003:165768 [3] NCCL INFO comm 0x7f584c076790 rank 7 nranks 16 cudaDev 3 nvmlDev 3 busId 6f000 commId 0x373ff5e8ed960245 - Init START
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163001:165765 [1] NCCL INFO comm 0x7f5910076910 rank 5 nranks 16 cudaDev 1 nvmlDev 1 busId 52000 commId 0x373ff5e8ed960245 - Init START
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:285891 [2] NCCL INFO NCCL_TOPO_FILE set by environment to /var/run/nvidia-topologyd/virtualTopology.xml
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283115:285892 [3] NCCL INFO NCCL_TOPO_FILE set by environment to /var/run/nvidia-topologyd/virtualTopology.xml
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283113:285890 [1] NCCL INFO NCCL_TOPO_FILE set by environment to /var/run/nvidia-topologyd/virtualTopology.xml
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:285889 [0] NCCL INFO NCCL_TOPO_FILE set by environment to /var/run/nvidia-topologyd/virtualTopology.xml
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190805:195181 [1] NCCL INFO Setting affinity for GPU 1 to ffffffff,00000000,ffffffff
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190807:195178 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,00000000,ffffffff
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:195179 [0] NCCL INFO Setting affinity for GPU 0 to ffffffff,00000000,ffffffff
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:195180 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff,00000000,ffffffff
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:165767 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff,00000000,ffffffff
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163003:165768 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,00000000,ffffffff
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:165766 [0] NCCL INFO Setting affinity for GPU 0 to ffffffff,00000000,ffffffff
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163001:165765 [1] NCCL INFO Setting affinity for GPU 1 to ffffffff,00000000,ffffffff
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170623:173342 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,00000000,ffffffff
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170621:173344 [1] NCCL INFO Setting affinity for GPU 1 to ffffffff,00000000,ffffffff
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:173345 [0] NCCL INFO Setting affinity for GPU 0 to ffffffff,00000000,ffffffff
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:173343 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff,00000000,ffffffff
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:285891 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff,00000000,ffffffff
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283113:285890 [1] NCCL INFO Setting affinity for GPU 1 to ffffffff,00000000,ffffffff
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:285889 [0] NCCL INFO Setting affinity for GPU 0 to ffffffff,00000000,ffffffff
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283115:285892 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,00000000,ffffffff
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283113:285890 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] -1/-1/-1->1->0 [3] 2/-1/-1->1->0
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283113:285890 [1] NCCL INFO P2P Chunksize set to 131072
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283113:285890 [1] NCCL INFO Channel 01/0 : 1[1] -> 6[2] [send] via NET/IBext/0/GDRDMA
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283113:285890 [1] NCCL INFO Channel 03/0 : 1[1] -> 6[2] [send] via NET/IBext/0/GDRDMA
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:285891 [2] NCCL INFO Trees [0] 3/8/-1->2->-1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->4 [3] 3/-1/-1->2->1
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283115:285892 [3] NCCL INFO Trees [0] 0/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] 0/-1/-1->3->2 [3] -1/-1/-1->3->2
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283115:285892 [3] NCCL INFO P2P Chunksize set to 131072
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:285891 [2] NCCL INFO P2P Chunksize set to 131072
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:285889 [0] NCCL INFO Channel 00/04 :    0   3   4   7   6   5   8  11  10   9  12  15  14  13   2   1
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:285889 [0] NCCL INFO Channel 01/04 :    0   3   2   1   6   5   4   7  10   9   8  11  14  13  12  15
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:285889 [0] NCCL INFO Channel 02/04 :    0   3   4   7   6   5   8  11  10   9  12  15  14  13   2   1
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:285889 [0] NCCL INFO Channel 03/04 :    0   3   2   1   6   5   4   7  10   9   8  11  14  13  12  15
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:285889 [0] NCCL INFO Trees [0] 1/-1/-1->0->3 [1] 1/10/-1->0->-1 [2] 1/-1/-1->0->3 [3] 1/-1/-1->0->6
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:285889 [0] NCCL INFO P2P Chunksize set to 131072
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170621:173344 [1] NCCL INFO Trees [0] 10/4/-1->9->8 [1] -1/-1/-1->9->8 [2] 10/-1/-1->9->8 [3] -1/-1/-1->9->8
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:173343 [2] NCCL INFO Trees [0] 11/-1/-1->10->9 [1] 11/14/-1->10->0 [2] 11/-1/-1->10->9 [3] 11/-1/-1->10->7
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170623:173342 [3] NCCL INFO Trees [0] -1/-1/-1->11->10 [1] 8/6/-1->11->10 [2] -1/-1/-1->11->10 [3] 8/-1/-1->11->10
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163003:165768 [3] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] 4/-1/-1->7->6 [2] -1/-1/-1->7->6 [3] 4/10/-1->7->6
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190807:195178 [3] NCCL INFO Trees [0] -1/-1/-1->15->14 [1] 12/-1/-1->15->14 [2] -1/-1/-1->15->14 [3] 12/-1/-1->15->14
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:165766 [0] NCCL INFO Trees [0] 5/-1/-1->4->9 [1] 5/-1/-1->4->7 [2] 5/2/-1->4->12 [3] 5/-1/-1->4->7
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:173343 [2] NCCL INFO P2P Chunksize set to 131072
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190807:195178 [3] NCCL INFO P2P Chunksize set to 131072
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:165767 [2] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->11 [2] 7/-1/-1->6->5 [3] 7/0/-1->6->14
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163001:165765 [1] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] -1/-1/-1->5->4 [2] 6/8/-1->5->4 [3] -1/-1/-1->5->4
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:165767 [2] NCCL INFO P2P Chunksize set to 131072
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:165766 [0] NCCL INFO P2P Chunksize set to 131072
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163003:165768 [3] NCCL INFO P2P Chunksize set to 131072
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163001:165765 [1] NCCL INFO P2P Chunksize set to 131072
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:195180 [2] NCCL INFO Trees [0] 15/-1/-1->14->13 [1] 15/-1/-1->14->10 [2] 15/-1/-1->14->13 [3] 15/6/-1->14->-1
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170621:173344 [1] NCCL INFO P2P Chunksize set to 131072
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170623:173342 [3] NCCL INFO P2P Chunksize set to 131072
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:173345 [0] NCCL INFO Trees [0] 9/12/-1->8->2 [1] 9/-1/-1->8->11 [2] 9/-1/-1->8->5 [3] 9/-1/-1->8->11
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:173345 [0] NCCL INFO P2P Chunksize set to 131072
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:195180 [2] NCCL INFO P2P Chunksize set to 131072
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190805:195181 [1] NCCL INFO Trees [0] 14/-1/-1->13->12 [1] -1/-1/-1->13->12 [2] 14/-1/-1->13->12 [3] -1/-1/-1->13->12
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190805:195181 [1] NCCL INFO P2P Chunksize set to 131072
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:195179 [0] NCCL INFO Trees [0] 13/-1/-1->12->8 [1] 13/-1/-1->12->15 [2] 13/4/-1->12->-1 [3] 13/-1/-1->12->15
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:195179 [0] NCCL INFO P2P Chunksize set to 131072
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:173343 [2] NCCL INFO Channel 01/0 : 7[3] -> 10[2] [receive] via NET/IBext/1/GDRDMA
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:173343 [2] NCCL INFO Channel 03/0 : 7[3] -> 10[2] [receive] via NET/IBext/1/GDRDMA
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:173345 [0] NCCL INFO Channel 00/0 : 5[1] -> 8[0] [receive] via NET/IBext/0/GDRDMA
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:173345 [0] NCCL INFO Channel 02/0 : 5[1] -> 8[0] [receive] via NET/IBext/0/GDRDMA
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:173345 [0] NCCL INFO Channel 00/0 : 8[0] -> 11[3] via P2P/IPC/read
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283115:285892 [3] NCCL INFO Channel 00/0 : 3[3] -> 4[0] [send] via NET/IBext/1/GDRDMA
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:285891 [2] NCCL INFO Channel 00/0 : 13[1] -> 2[2] [receive] via NET/IBext/1/GDRDMA
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190805:195181 [1] NCCL INFO Channel 00/0 : 13[1] -> 2[2] [send] via NET/IBext/0/GDRDMA
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163001:165765 [1] NCCL INFO Channel 00/0 : 5[1] -> 8[0] [send] via NET/IBext/0/GDRDMA
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:285889 [0] NCCL INFO Channel 01/0 : 15[3] -> 0[0] [receive] via NET/IBext/0/GDRDMA
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:195179 [0] NCCL INFO Channel 00/0 : 9[1] -> 12[0] [receive] via NET/IBext/0/GDRDMA
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190805:195181 [1] NCCL INFO Channel 02/0 : 13[1] -> 2[2] [send] via NET/IBext/0/GDRDMA
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283115:285892 [3] NCCL INFO Channel 02/0 : 3[3] -> 4[0] [send] via NET/IBext/1/GDRDMA
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:285891 [2] NCCL INFO Channel 02/0 : 13[1] -> 2[2] [receive] via NET/IBext/1/GDRDMA
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:195179 [0] NCCL INFO Channel 02/0 : 9[1] -> 12[0] [receive] via NET/IBext/0/GDRDMA
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190807:195178 [3] NCCL INFO Channel 01/0 : 15[3] -> 0[0] [send] via NET/IBext/1/GDRDMA
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:285889 [0] NCCL INFO Channel 03/0 : 15[3] -> 0[0] [receive] via NET/IBext/0/GDRDMA
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:195179 [0] NCCL INFO Channel 00/0 : 12[0] -> 15[3] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163001:165765 [1] NCCL INFO Channel 02/0 : 5[1] -> 8[0] [send] via NET/IBext/0/GDRDMA
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:195180 [2] NCCL INFO Channel 01/0 : 11[3] -> 14[2] [receive] via NET/IBext/1/GDRDMA
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190807:195178 [3] NCCL INFO Channel 03/0 : 15[3] -> 0[0] [send] via NET/IBext/1/GDRDMA
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:165766 [0] NCCL INFO Channel 00/0 : 3[3] -> 4[0] [receive] via NET/IBext/0/GDRDMA
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:165767 [2] NCCL INFO Channel 01/0 : 1[1] -> 6[2] [receive] via NET/IBext/1/GDRDMA
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:195180 [2] NCCL INFO Channel 03/0 : 11[3] -> 14[2] [receive] via NET/IBext/1/GDRDMA
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:165767 [2] NCCL INFO Channel 03/0 : 1[1] -> 6[2] [receive] via NET/IBext/1/GDRDMA
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:165766 [0] NCCL INFO Channel 02/0 : 3[3] -> 4[0] [receive] via NET/IBext/0/GDRDMA
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:173345 [0] NCCL INFO Channel 01/0 : 8[0] -> 11[3] via P2P/IPC/read
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170621:173344 [1] NCCL INFO Channel 00/0 : 9[1] -> 12[0] [send] via NET/IBext/0/GDRDMA
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:285889 [0] NCCL INFO Channel 00/0 : 0[0] -> 3[3] via P2P/IPC/read
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170621:173344 [1] NCCL INFO Channel 02/0 : 9[1] -> 12[0] [send] via NET/IBext/0/GDRDMA
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:165766 [0] NCCL INFO Channel 00/0 : 4[0] -> 7[3] via P2P/IPC/read
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:173345 [0] NCCL INFO Channel 02/0 : 8[0] -> 11[3] via P2P/IPC/read
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:195179 [0] NCCL INFO Channel 01/0 : 12[0] -> 15[3] via P2P/IPC/read
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:173345 [0] NCCL INFO Channel 03/0 : 8[0] -> 11[3] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:165766 [0] NCCL INFO Channel 01/0 : 4[0] -> 7[3] via P2P/IPC/read
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:195179 [0] NCCL INFO Channel 02/0 : 12[0] -> 15[3] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163003:165768 [3] NCCL INFO Channel 01/0 : 7[3] -> 10[2] [send] via NET/IBext/1/GDRDMA
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:165767 [2] NCCL INFO Channel 00/0 : 6[2] -> 5[1] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163003:165768 [3] NCCL INFO Channel 03/0 : 7[3] -> 10[2] [send] via NET/IBext/1/GDRDMA
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:285889 [0] NCCL INFO Channel 01/0 : 0[0] -> 3[3] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:165766 [0] NCCL INFO Channel 02/0 : 4[0] -> 7[3] via P2P/IPC/read
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170623:173342 [3] NCCL INFO Channel 01/0 : 11[3] -> 14[2] [send] via NET/IBext/1/GDRDMA
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170623:173342 [3] NCCL INFO Channel 03/0 : 11[3] -> 14[2] [send] via NET/IBext/1/GDRDMA
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:195179 [0] NCCL INFO Channel 03/0 : 12[0] -> 15[3] via P2P/IPC/read
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170623:173342 [3] NCCL INFO Channel 00/0 : 11[3] -> 10[2] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:165767 [2] NCCL INFO Channel 01/0 : 6[2] -> 5[1] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:165766 [0] NCCL INFO Channel 03/0 : 4[0] -> 7[3] via P2P/IPC/read
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:285889 [0] NCCL INFO Channel 02/0 : 0[0] -> 3[3] via P2P/IPC/read
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:285891 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/IPC/read
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170623:173342 [3] NCCL INFO Channel 02/0 : 11[3] -> 10[2] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:165767 [2] NCCL INFO Channel 02/0 : 6[2] -> 5[1] via P2P/IPC/read
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:173343 [2] NCCL INFO Channel 00/0 : 10[2] -> 9[1] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163003:165768 [3] NCCL INFO Channel 00/0 : 7[3] -> 6[2] via P2P/IPC/read
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190805:195181 [1] NCCL INFO Channel 01/0 : 13[1] -> 12[0] via P2P/IPC/read
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:285889 [0] NCCL INFO Channel 03/0 : 0[0] -> 3[3] via P2P/IPC/read
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190807:195178 [3] NCCL INFO Channel 00/0 : 15[3] -> 14[2] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:165767 [2] NCCL INFO Channel 03/0 : 6[2] -> 5[1] via P2P/IPC/read
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:285891 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/IPC/read
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:173343 [2] NCCL INFO Channel 01/0 : 10[2] -> 9[1] via P2P/IPC/read
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:195180 [2] NCCL INFO Channel 00/0 : 14[2] -> 13[1] via P2P/IPC/read
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:173343 [2] NCCL INFO Channel 02/0 : 10[2] -> 9[1] via P2P/IPC/read
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190805:195181 [1] NCCL INFO Channel 03/0 : 13[1] -> 12[0] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163003:165768 [3] NCCL INFO Channel 02/0 : 7[3] -> 6[2] via P2P/IPC/read
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190807:195178 [3] NCCL INFO Channel 02/0 : 15[3] -> 14[2] via P2P/IPC/read
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:173343 [2] NCCL INFO Channel 03/0 : 10[2] -> 9[1] via P2P/IPC/read
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283113:285890 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/IPC/read
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:285891 [2] NCCL INFO Channel 02/0 : 2[2] -> 1[1] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163001:165765 [1] NCCL INFO Channel 01/0 : 5[1] -> 4[0] via P2P/IPC/read
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283115:285892 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/IPC/read
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:195180 [2] NCCL INFO Channel 01/0 : 14[2] -> 13[1] via P2P/IPC/read
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170621:173344 [1] NCCL INFO Channel 01/0 : 9[1] -> 8[0] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163001:165765 [1] NCCL INFO Channel 03/0 : 5[1] -> 4[0] via P2P/IPC/read
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:195180 [2] NCCL INFO Channel 02/0 : 14[2] -> 13[1] via P2P/IPC/read
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283113:285890 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/IPC/read
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:285891 [2] NCCL INFO Channel 03/0 : 2[2] -> 1[1] via P2P/IPC/read
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283115:285892 [3] NCCL INFO Channel 03/0 : 3[3] -> 2[2] via P2P/IPC/read
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170621:173344 [1] NCCL INFO Channel 03/0 : 9[1] -> 8[0] via P2P/IPC/read
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:195180 [2] NCCL INFO Channel 03/0 : 14[2] -> 13[1] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163003:165768 [3] NCCL INFO Connected all rings
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163001:165765 [1] NCCL INFO Connected all rings
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190805:195181 [1] NCCL INFO Connected all rings
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:165767 [2] NCCL INFO Connected all rings
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190807:195178 [3] NCCL INFO Connected all rings
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170621:173344 [1] NCCL INFO Connected all rings
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170623:173342 [3] NCCL INFO Connected all rings
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163001:165765 [1] NCCL INFO Channel 00/0 : 5[1] -> 6[2] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:165767 [2] NCCL INFO Channel 00/0 : 6[2] -> 7[3] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:165766 [0] NCCL INFO Connected all rings
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:165766 [0] NCCL INFO Channel 00/0 : 4[0] -> 5[1] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163001:165765 [1] NCCL INFO Channel 02/0 : 5[1] -> 6[2] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:165767 [2] NCCL INFO Channel 01/0 : 6[2] -> 7[3] via P2P/IPC/read
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:173343 [2] NCCL INFO Connected all rings
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:165767 [2] NCCL INFO Channel 02/0 : 6[2] -> 7[3] via P2P/IPC/read
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:195179 [0] NCCL INFO Connected all rings
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:195179 [0] NCCL INFO Channel 00/0 : 12[0] -> 13[1] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:165767 [2] NCCL INFO Channel 03/0 : 6[2] -> 7[3] via P2P/IPC/read
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190805:195181 [1] NCCL INFO Channel 00/0 : 13[1] -> 14[2] via P2P/IPC/read
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:195180 [2] NCCL INFO Connected all rings
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:195179 [0] NCCL INFO Channel 01/0 : 12[0] -> 13[1] via P2P/IPC/read
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190805:195181 [1] NCCL INFO Channel 02/0 : 13[1] -> 14[2] via P2P/IPC/read
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:173343 [2] NCCL INFO Channel 00/0 : 10[2] -> 11[3] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:165767 [2] NCCL INFO Channel 01/0 : 6[2] -> 11[3] [send] via NET/IBext/1/GDRDMA
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:173345 [0] NCCL INFO Connected all rings
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:173345 [0] NCCL INFO Channel 00/0 : 8[0] -> 9[1] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163003:165768 [3] NCCL INFO Channel 03/0 : 10[2] -> 7[3] [receive] via NET/IBext/1/GDRDMA
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163003:165768 [3] NCCL INFO Channel 01/0 : 7[3] -> 4[0] via P2P/IPC/read
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:195179 [0] NCCL INFO Channel 02/0 : 12[0] -> 13[1] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163003:165768 [3] NCCL INFO Channel 03/0 : 7[3] -> 4[0] via P2P/IPC/read
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170621:173344 [1] NCCL INFO Channel 00/0 : 9[1] -> 10[2] via P2P/IPC/read
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:195180 [2] NCCL INFO Channel 00/0 : 14[2] -> 15[3] via P2P/IPC/read
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:195179 [0] NCCL INFO Channel 03/0 : 12[0] -> 13[1] via P2P/IPC/read
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283115:285892 [3] NCCL INFO Connected all rings
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:173343 [2] NCCL INFO Channel 01/0 : 10[2] -> 11[3] via P2P/IPC/read
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:173345 [0] NCCL INFO Channel 01/0 : 8[0] -> 9[1] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:165766 [0] NCCL INFO Channel 01/0 : 4[0] -> 5[1] via P2P/IPC/read
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:195180 [2] NCCL INFO Channel 01/0 : 14[2] -> 15[3] via P2P/IPC/read
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:195179 [0] NCCL INFO Channel 00/0 : 8[0] -> 12[0] [receive] via NET/IBext/0/GDRDMA
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170621:173344 [1] NCCL INFO Channel 02/0 : 9[1] -> 10[2] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:165766 [0] NCCL INFO Channel 02/0 : 4[0] -> 5[1] via P2P/IPC/read
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:173343 [2] NCCL INFO Channel 02/0 : 10[2] -> 11[3] via P2P/IPC/read
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:173345 [0] NCCL INFO Channel 02/0 : 8[0] -> 9[1] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:165766 [0] NCCL INFO Channel 03/0 : 4[0] -> 5[1] via P2P/IPC/read
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:173343 [2] NCCL INFO Channel 03/0 : 10[2] -> 11[3] via P2P/IPC/read
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283113:285890 [1] NCCL INFO Connected all rings
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163001:165765 [1] NCCL INFO Channel 02/0 : 8[0] -> 5[1] [receive] via NET/IBext/0/GDRDMA
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:173345 [0] NCCL INFO Channel 03/0 : 8[0] -> 9[1] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:165766 [0] NCCL INFO Channel 02/0 : 2[2] -> 4[0] [receive] via NET/IBext/0/GDRDMA
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:173343 [2] NCCL INFO Channel 01/0 : 10[2] -> 14[2] [send] via NET/IBext/1/GDRDMA
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170623:173342 [3] NCCL INFO Channel 01/0 : 6[2] -> 11[3] [receive] via NET/IBext/1/GDRDMA
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170621:173344 [1] NCCL INFO Channel 00/0 : 4[0] -> 9[1] [receive] via NET/IBext/0/GDRDMA
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:173345 [0] NCCL INFO Channel 00/0 : 8[0] -> 12[0] [send] via NET/IBext/0/GDRDMA
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170623:173342 [3] NCCL INFO Channel 01/0 : 11[3] -> 6[2] [send] via NET/IBext/1/GDRDMA
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:173345 [0] NCCL INFO Channel 00/0 : 2[2] -> 8[0] [receive] via NET/IBext/0/GDRDMA
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:165767 [2] NCCL INFO Channel 03/0 : 0[0] -> 6[2] [receive] via NET/IBext/1/GDRDMA
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:195179 [0] NCCL INFO Channel 02/0 : 4[0] -> 12[0] [receive] via NET/IBext/0/GDRDMA
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:195179 [0] NCCL INFO Channel 02/0 : 12[0] -> 4[0] [send] via NET/IBext/0/GDRDMA
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:285891 [2] NCCL INFO Connected all rings
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:285889 [0] NCCL INFO Connected all rings
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:285889 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/IPC/read
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:285889 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/IPC/read
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:285891 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/IPC/read
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:195180 [2] NCCL INFO Channel 02/0 : 14[2] -> 15[3] via P2P/IPC/read
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283113:285890 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/IPC/read
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:195180 [2] NCCL INFO Channel 03/0 : 14[2] -> 15[3] via P2P/IPC/read
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:285889 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/IPC/read
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:285891 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/IPC/read
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283113:285890 [1] NCCL INFO Channel 03/0 : 1[1] -> 2[2] via P2P/IPC/read
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190805:195181 [1] NCCL INFO Channel 00/0 : 13[1] -> 12[0] via P2P/IPC/read
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190807:195178 [3] NCCL INFO Channel 01/0 : 15[3] -> 12[0] via P2P/IPC/read
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:195180 [2] NCCL INFO Channel 01/0 : 10[2] -> 14[2] [receive] via NET/IBext/1/GDRDMA
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:195180 [2] NCCL INFO Channel 03/0 : 6[2] -> 14[2] [receive] via NET/IBext/1/GDRDMA
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:173343 [2] NCCL INFO Channel 01/0 : 10[2] -> 0[0] [send] via NET/IBext/1/GDRDMA
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:195180 [2] NCCL INFO Channel 03/0 : 14[2] -> 6[2] [send] via NET/IBext/1/GDRDMA
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190805:195181 [1] NCCL INFO Channel 02/0 : 13[1] -> 12[0] via P2P/IPC/read
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190807:195178 [3] NCCL INFO Channel 03/0 : 15[3] -> 12[0] via P2P/IPC/read
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:285889 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/IPC/read
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:285891 [2] NCCL INFO Channel 02/0 : 2[2] -> 3[3] via P2P/IPC/read
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:285891 [2] NCCL INFO Channel 03/0 : 2[2] -> 3[3] via P2P/IPC/read
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:285889 [0] NCCL INFO Channel 01/0 : 10[2] -> 0[0] [receive] via NET/IBext/0/GDRDMA
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:285889 [0] NCCL INFO Channel 03/0 : 0[0] -> 6[2] [send] via NET/IBext/0/GDRDMA
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:173343 [2] NCCL INFO Channel 01/0 : 0[0] -> 10[2] [receive] via NET/IBext/1/GDRDMA
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:285889 [0] NCCL INFO Channel 03/0 : 6[2] -> 0[0] [receive] via NET/IBext/0/GDRDMA
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:285889 [0] NCCL INFO Channel 01/0 : 0[0] -> 10[2] [send] via NET/IBext/0/GDRDMA
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:165767 [2] NCCL INFO Channel 03/0 : 14[2] -> 6[2] [receive] via NET/IBext/1/GDRDMA
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:165767 [2] NCCL INFO Channel 03/0 : 6[2] -> 14[2] [send] via NET/IBext/1/GDRDMA
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:165767 [2] NCCL INFO Channel 03/0 : 6[2] -> 0[0] [send] via NET/IBext/1/GDRDMA
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:195180 [2] NCCL INFO Channel 01/0 : 14[2] -> 10[2] [send] via NET/IBext/1/GDRDMA
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:173343 [2] NCCL INFO Channel 01/0 : 14[2] -> 10[2] [receive] via NET/IBext/1/GDRDMA
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:165767 [2] NCCL INFO Channel 01/0 : 11[3] -> 6[2] [receive] via NET/IBext/1/GDRDMA
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:173343 [2] NCCL INFO Channel 03/0 : 10[2] -> 7[3] [send] via NET/IBext/1/GDRDMA
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170623:173342 [3] NCCL INFO Channel 01/0 : 11[3] -> 8[0] via P2P/IPC/read
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170623:173342 [3] NCCL INFO Channel 03/0 : 11[3] -> 8[0] via P2P/IPC/read
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283113:285890 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/IPC/read
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283115:285892 [3] NCCL INFO Channel 00/0 : 3[3] -> 0[0] via P2P/IPC/read
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:285891 [2] NCCL INFO Channel 02/0 : 2[2] -> 4[0] [send] via NET/IBext/1/GDRDMA
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:285891 [2] NCCL INFO Channel 00/0 : 2[2] -> 8[0] [send] via NET/IBext/1/GDRDMA
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:165766 [0] NCCL INFO Channel 00/0 : 4[0] -> 9[1] [send] via NET/IBext/0/GDRDMA
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:285891 [2] NCCL INFO Channel 00/0 : 8[0] -> 2[2] [receive] via NET/IBext/1/GDRDMA
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:173345 [0] NCCL INFO Channel 00/0 : 8[0] -> 2[2] [send] via NET/IBext/0/GDRDMA
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:165766 [0] NCCL INFO Channel 02/0 : 12[0] -> 4[0] [receive] via NET/IBext/0/GDRDMA
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170621:173344 [1] NCCL INFO Channel 00/0 : 9[1] -> 4[0] [send] via NET/IBext/0/GDRDMA
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:165766 [0] NCCL INFO Channel 02/0 : 4[0] -> 12[0] [send] via NET/IBext/0/GDRDMA
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:285891 [2] NCCL INFO Channel 02/0 : 4[0] -> 2[2] [receive] via NET/IBext/1/GDRDMA
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:173345 [0] NCCL INFO Channel 00/0 : 12[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283113:285890 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/IPC/read
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283115:285892 [3] NCCL INFO Channel 02/0 : 3[3] -> 0[0] via P2P/IPC/read
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:195179 [0] NCCL INFO Channel 00/0 : 12[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:165766 [0] NCCL INFO Channel 00/0 : 9[1] -> 4[0] [receive] via NET/IBext/0/GDRDMA
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170621:173344 [1] NCCL INFO Channel 00/0 : 9[1] -> 8[0] via P2P/IPC/read
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283115:285892 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/IPC/read
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170621:173344 [1] NCCL INFO Channel 02/0 : 9[1] -> 8[0] via P2P/IPC/read
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190807:195178 [3] NCCL INFO Channel 01/0 : 15[3] -> 14[2] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163003:165768 [3] NCCL INFO Channel 01/0 : 7[3] -> 6[2] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:165766 [0] NCCL INFO Channel 02/0 : 4[0] -> 2[2] [send] via NET/IBext/0/GDRDMA
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283115:285892 [3] NCCL INFO Channel 02/0 : 3[3] -> 2[2] via P2P/IPC/read
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:173345 [0] NCCL INFO Channel 02/0 : 8[0] -> 5[1] [send] via NET/IBext/0/GDRDMA
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163003:165768 [3] NCCL INFO Channel 03/0 : 7[3] -> 6[2] via P2P/IPC/read
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170623:173342 [3] NCCL INFO Channel 01/0 : 11[3] -> 10[2] via P2P/IPC/read
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190807:195178 [3] NCCL INFO Channel 03/0 : 15[3] -> 14[2] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163001:165765 [1] NCCL INFO Channel 00/0 : 5[1] -> 4[0] via P2P/IPC/read
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170623:173342 [3] NCCL INFO Channel 03/0 : 11[3] -> 10[2] via P2P/IPC/read
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163001:165765 [1] NCCL INFO Channel 02/0 : 5[1] -> 4[0] via P2P/IPC/read
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190805:195181 [1] NCCL INFO Connected all trees
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190805:195181 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190805:195181 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190807:195178 [3] NCCL INFO Connected all trees
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190807:195178 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190807:195178 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283113:285890 [1] NCCL INFO Connected all trees
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283113:285890 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283113:285890 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283115:285892 [3] NCCL INFO Connected all trees
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283115:285892 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283115:285892 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170623:173342 [3] NCCL INFO Connected all trees
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170623:173342 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170623:173342 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170621:173344 [1] NCCL INFO Connected all trees
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170621:173344 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170621:173344 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:195180 [2] NCCL INFO Connected all trees
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:195180 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:195180 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:195179 [0] NCCL INFO Connected all trees
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:195179 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:195179 [0] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:285889 [0] NCCL INFO Connected all trees
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:285889 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:285889 [0] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:173345 [0] NCCL INFO Connected all trees
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:173345 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:173345 [0] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:165767 [2] NCCL INFO Connected all trees
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:165767 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:165767 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:173343 [2] NCCL INFO Connected all trees
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:173343 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:173343 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:285891 [2] NCCL INFO Connected all trees
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:285891 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:285891 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:165766 [0] NCCL INFO Connected all trees
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:165766 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:165766 [0] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163001:165765 [1] NCCL INFO Connected all trees
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163001:165765 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163001:165765 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190805:195181 [1] NCCL INFO comm 0x7ef514078b80 rank 13 nranks 16 cudaDev 1 nvmlDev 1 busId 46000 commId 0x373ff5e8ed960245 - Init COMPLETE
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190804:195179 [0] NCCL INFO comm 0x7f9fcc074700 rank 12 nranks 16 cudaDev 0 nvmlDev 0 busId 40000 commId 0x373ff5e8ed960245 - Init COMPLETE
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190807:195178 [3] NCCL INFO comm 0x7f87d4078410 rank 15 nranks 16 cudaDev 3 nvmlDev 3 busId 6a000 commId 0x373ff5e8ed960245 - Init COMPLETE
head: t-20240614131131-qkptl-ray-job-head-lx2fh:190806:195180 [2] NCCL INFO comm 0x7f09bc0793f0 rank 14 nranks 16 cudaDev 2 nvmlDev 2 busId 65000 commId 0x373ff5e8ed960245 - Init COMPLETE
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163003:165768 [3] NCCL INFO Connected all trees
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163003:165768 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163003:165768 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170620:173345 [0] NCCL INFO comm 0x7ef584078cf0 rank 8 nranks 16 cudaDev 0 nvmlDev 0 busId 4d000 commId 0x373ff5e8ed960245 - Init COMPLETE
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170622:173343 [2] NCCL INFO comm 0x7f6b60076a10 rank 10 nranks 16 cudaDev 2 nvmlDev 2 busId 69000 commId 0x373ff5e8ed960245 - Init COMPLETE
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170623:173342 [3] NCCL INFO comm 0x7f7d00079780 rank 11 nranks 16 cudaDev 3 nvmlDev 3 busId 6f000 commId 0x373ff5e8ed960245 - Init COMPLETE
worker3: t-20240614131131-qkptl-ray-job-worker-worker3-gknjl:170621:173344 [1] NCCL INFO comm 0x7f21fc076650 rank 9 nranks 16 cudaDev 1 nvmlDev 1 busId 52000 commId 0x373ff5e8ed960245 - Init COMPLETE
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283112:285889 [0] NCCL INFO comm 0x7f0c10078cb0 rank 0 nranks 16 cudaDev 0 nvmlDev 0 busId 40000 commId 0x373ff5e8ed960245 - Init COMPLETE
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283114:285891 [2] NCCL INFO comm 0x7f0b94071dd0 rank 2 nranks 16 cudaDev 2 nvmlDev 2 busId 65000 commId 0x373ff5e8ed960245 - Init COMPLETE
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283115:285892 [3] NCCL INFO comm 0x7f2f68073090 rank 3 nranks 16 cudaDev 3 nvmlDev 3 busId 6a000 commId 0x373ff5e8ed960245 - Init COMPLETE
worker1: t-20240614131131-qkptl-ray-job-worker-worker1-kbgml:283113:285890 [1] NCCL INFO comm 0x7fa154072e40 rank 1 nranks 16 cudaDev 1 nvmlDev 1 busId 46000 commId 0x373ff5e8ed960245 - Init COMPLETE
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163002:165767 [2] NCCL INFO comm 0x7fa65c078d00 rank 6 nranks 16 cudaDev 2 nvmlDev 2 busId 69000 commId 0x373ff5e8ed960245 - Init COMPLETE
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163000:165766 [0] NCCL INFO comm 0x7f36880788d0 rank 4 nranks 16 cudaDev 0 nvmlDev 0 busId 4d000 commId 0x373ff5e8ed960245 - Init COMPLETE
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163001:165765 [1] NCCL INFO comm 0x7f5910076910 rank 5 nranks 16 cudaDev 1 nvmlDev 1 busId 52000 commId 0x373ff5e8ed960245 - Init COMPLETE
worker2: t-20240614131131-qkptl-ray-job-worker-worker2-stsdn:163003:165768 [3] NCCL INFO comm 0x7f584c076790 rank 7 nranks 16 cudaDev 3 nvmlDev 3 busId 6f000 commId 0x373ff5e8ed960245 - Init COMPLETE
worker1: Invalidate trace cache @ step 454: expected module 1, but got module 453
worker1: Invalidate trace cache @ step 452: expected module 908, but got module 907
worker1: [2024-06-14 07:34:45,014] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1
worker1: [2024-06-14 07:34:58,017] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1
worker1: ========== [step:7] start ==========
worker1: Epoch: 0 | Step: 7 | PPO Epoch: 1 | Actor Loss: 0.0019377699645701796 | Critic Loss: 0.008707805012818426 | Unsupervised Loss: 0.0
worker1: End-to-End => Latency: 468.99s, TFLOPs: 35.60, Samples/sec: 2.18, Time/seq 0.46s, Batch Size: 1024, Total Seq. Length: 2048
worker1: Generation => Latency: 41.25s, Per-token Latency 40.29 ms, TFLOPs: 5.62, BW: -1.00 GB/sec, Answer Seq. Length: 1024
worker1: Training   => Latency: 138.95s, TFLOPs: 106.82
worker1: Actor Model Parameters => 6.738 B, Critic Model Parameters => 6.607 B
worker1: Average reward score: -0.4599609375 | EMA reward score: -0.4599609375
worker1: -------------------------------------------------------------------------------------
worker1: |E2E latency=189.87s |Gather latency=0.81s (0.43%) |Generate time=40.44s (21.30%) |Training time=134.77s (70.98%) |Others=14.66 (7.72%)|CurSamplesPerSec=0.67 |AvgSamplesPerSec=1.85
worker1: |E2E latency=50.41s |Gather latency=0.85s (1.68%) |Generate time=39.65s (78.66%) |Training time=0.00s (0.00%) |Others=10.76 (21.34%)|CurSamplesPerSec=2.54 |AvgSamplesPerSec=1.91
worker1: Invalidate trace cache @ step 454: expected module 453, but got module 1
worker1: Invalidate trace cache @ step 452: expected module 907, but got module 908
worker1: |E2E latency=50.15s |Gather latency=1.00s (2.00%) |Generate time=39.95s (79.67%) |Training time=0.00s (0.00%) |Others=10.19 (20.33%)|CurSamplesPerSec=2.55 |AvgSamplesPerSec=1.96
worker1: |E2E latency=51.09s |Gather latency=0.83s (1.62%) |Generate time=40.01s (78.31%) |Training time=0.00s (0.00%) |Others=11.08 (21.69%)|CurSamplesPerSec=2.51 |AvgSamplesPerSec=2.00
worker1: |E2E latency=52.48s |Gather latency=0.81s (1.54%) |Generate time=40.21s (76.62%) |Training time=0.00s (0.00%) |Others=12.27 (23.38%)|CurSamplesPerSec=2.44 |AvgSamplesPerSec=2.03
worker1: |E2E latency=51.00s |Gather latency=0.93s (1.83%) |Generate time=40.21s (78.85%) |Training time=0.00s (0.00%) |Others=10.79 (21.15%)|CurSamplesPerSec=2.51 |AvgSamplesPerSec=2.06
worker1: |E2E latency=51.64s |Gather latency=0.78s (1.51%) |Generate time=40.04s (77.54%) |Training time=0.00s (0.00%) |Others=11.60 (22.46%)|CurSamplesPerSec=2.48 |AvgSamplesPerSec=2.09
worker1: |E2E latency=51.60s |Gather latency=1.14s (2.20%) |Generate time=40.25s (78.00%) |Training time=0.00s (0.00%) |Others=11.35 (22.00%)|CurSamplesPerSec=2.48 |AvgSamplesPerSec=2.11
worker1: [2024-06-14 07:43:49,163] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
worker1: [2024-06-14 07:44:01,937] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
worker1: [step:7][stage:generate-experience][dt:327.37]
worker1: [step:7][stage:actor-inference][dt:13.48]
worker1: [step:7][stage:ref-inference][dt:13.69]
worker1: [step:7][stage:reward-inference][dt:33.2]
worker1: [step:7][stage:critic-inference][dt:13.08]
worker1: [step:7][stage:total-generate][dt:407.19]
worker1: [step:7][stage:actor-train][dt:65.81]
worker1: [step:7][stage:critic-train][dt:67.83]
worker1: [step:7][stage:total-train][dt:134.3]
worker1: ========== [step:7] end, E2E [dt:543.92] ==========
worker1: ========== [step:15] start ==========
worker1: Epoch: 0 | Step: 15 | PPO Epoch: 1 | Actor Loss: 0.003393487713765353 | Critic Loss: 0.009072272339835763 | Unsupervised Loss: 0.0
worker1: End-to-End => Latency: 459.98s, TFLOPs: 36.30, Samples/sec: 2.23, Time/seq 0.45s, Batch Size: 1024, Total Seq. Length: 2048
worker1: Generation => Latency: 40.71s, Per-token Latency 39.76 ms, TFLOPs: 5.70, BW: -1.00 GB/sec, Answer Seq. Length: 1024
worker1: Training   => Latency: 134.30s, TFLOPs: 110.52
worker1: Actor Model Parameters => 6.738 B, Critic Model Parameters => 6.607 B
worker1: Average reward score: -0.463134765625 | EMA reward score: -0.46027832031250004
worker1: -------------------------------------------------------------------------------------
worker1: |E2E latency=185.56s |Gather latency=0.86s (0.46%) |Generate time=39.85s (21.48%) |Training time=131.30s (70.76%) |Others=14.41 (7.76%)|CurSamplesPerSec=0.69 |AvgSamplesPerSec=1.87
worker1: |E2E latency=50.86s |Gather latency=0.82s (1.62%) |Generate time=39.47s (77.61%) |Training time=0.00s (0.00%) |Others=11.39 (22.39%)|CurSamplesPerSec=2.52 |AvgSamplesPerSec=1.90
worker1: Invalidate trace cache @ step 454: expected module 453, but got module 1
worker1: Invalidate trace cache @ step 452: expected module 907, but got module 908
worker1: |E2E latency=50.54s |Gather latency=1.01s (1.99%) |Generate time=40.36s (79.86%) |Training time=0.00s (0.00%) |Others=10.18 (20.14%)|CurSamplesPerSec=2.53 |AvgSamplesPerSec=1.92
worker1: |E2E latency=50.47s |Gather latency=0.80s (1.59%) |Generate time=39.56s (78.37%) |Training time=0.00s (0.00%) |Others=10.92 (21.63%)|CurSamplesPerSec=2.54 |AvgSamplesPerSec=1.95
worker1: |E2E latency=51.05s |Gather latency=0.80s (1.58%) |Generate time=39.58s (77.53%) |Training time=0.00s (0.00%) |Others=11.47 (22.47%)|CurSamplesPerSec=2.51 |AvgSamplesPerSec=1.97
worker1: |E2E latency=50.73s |Gather latency=0.81s (1.59%) |Generate time=39.62s (78.11%) |Training time=0.00s (0.00%) |Others=11.11 (21.89%)|CurSamplesPerSec=2.52 |AvgSamplesPerSec=1.99
worker1: |E2E latency=50.59s |Gather latency=0.82s (1.62%) |Generate time=39.59s (78.26%) |Training time=0.00s (0.00%) |Others=11.00 (21.74%)|CurSamplesPerSec=2.53 |AvgSamplesPerSec=2.01
worker1: |E2E latency=50.41s |Gather latency=0.82s (1.63%) |Generate time=39.39s (78.15%) |Training time=0.00s (0.00%) |Others=11.02 (21.85%)|CurSamplesPerSec=2.54 |AvgSamplesPerSec=2.03
worker1: [2024-06-14 07:52:50,745] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384
worker1: [2024-06-14 07:53:03,062] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384
worker1: [step:15][stage:generate-experience][dt:323.9]
worker1: [step:15][stage:actor-inference][dt:13.36]
worker1: [step:15][stage:ref-inference][dt:13.82]
worker1: [step:15][stage:reward-inference][dt:32.14]
worker1: [step:15][stage:critic-inference][dt:12.76]
worker1: [step:15][stage:total-generate][dt:402.56]
worker1: [step:15][stage:actor-train][dt:66.31]
worker1: [step:15][stage:critic-train][dt:68.85]
worker1: [step:15][stage:total-train][dt:135.89]
worker1: ========== [step:15] end, E2E [dt:541.12] ==========
worker1: ========== [step:23] start ==========
worker1: Epoch: 0 | Step: 23 | PPO Epoch: 1 | Actor Loss: 0.001503070059698075 | Critic Loss: 0.0087060650694184 | Unsupervised Loss: 0.0
worker1: End-to-End => Latency: 459.44s, TFLOPs: 36.34, Samples/sec: 2.23, Time/seq 0.45s, Batch Size: 1024, Total Seq. Length: 2048
worker1: Generation => Latency: 40.44s, Per-token Latency 39.50 ms, TFLOPs: 5.73, BW: -1.00 GB/sec, Answer Seq. Length: 1024
worker1: Training   => Latency: 135.90s, TFLOPs: 109.22
worker1: Actor Model Parameters => 6.738 B, Critic Model Parameters => 6.607 B
worker1: Average reward score: -0.4619140625 | EMA reward score: -0.46044189453125006
worker1: -------------------------------------------------------------------------------------
worker1: |E2E latency=186.47s |Gather latency=0.87s (0.46%) |Generate time=39.57s (21.22%) |Training time=132.91s (71.28%) |Others=13.98 (7.50%)|CurSamplesPerSec=0.69 |AvgSamplesPerSec=1.88
worker1: |E2E latency=51.21s |Gather latency=0.80s (1.57%) |Generate time=39.34s (76.83%) |Training time=0.00s (0.00%) |Others=11.87 (23.17%)|CurSamplesPerSec=2.50 |AvgSamplesPerSec=1.89
worker1: Invalidate trace cache @ step 454: expected module 453, but got module 1
worker1: Invalidate trace cache @ step 452: expected module 907, but got module 908
worker1: |E2E latency=49.85s |Gather latency=0.93s (1.86%) |Generate time=39.48s (79.20%) |Training time=0.00s (0.00%) |Others=10.37 (20.80%)|CurSamplesPerSec=2.57 |AvgSamplesPerSec=1.91
worker1: |E2E latency=51.61s |Gather latency=0.93s (1.80%) |Generate time=39.45s (76.45%) |Training time=0.00s (0.00%) |Others=12.15 (23.55%)|CurSamplesPerSec=2.48 |AvgSamplesPerSec=1.93
worker1: |E2E latency=49.38s |Gather latency=0.81s (1.64%) |Generate time=39.50s (80.00%) |Training time=0.00s (0.00%) |Others=9.88 (20.00%)|CurSamplesPerSec=2.59 |AvgSamplesPerSec=1.95
worker1: |E2E latency=49.38s |Gather latency=0.85s (1.72%) |Generate time=39.36s (79.70%) |Training time=0.00s (0.00%) |Others=10.02 (20.30%)|CurSamplesPerSec=2.59 |AvgSamplesPerSec=1.96
worker1: |E2E latency=51.26s |Gather latency=0.83s (1.61%) |Generate time=39.44s (76.93%) |Training time=0.00s (0.00%) |Others=11.82 (23.07%)|CurSamplesPerSec=2.50 |AvgSamplesPerSec=1.98
worker1: |E2E latency=49.41s |Gather latency=0.81s (1.64%) |Generate time=39.38s (79.70%) |Training time=0.00s (0.00%) |Others=10.03 (20.30%)|CurSamplesPerSec=2.59 |AvgSamplesPerSec=1.99
worker3: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py:1330: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
worker3:   total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
worker2: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py:1330: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py:1330: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
worker1:   total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
worker3: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py:1330: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
worker3:   total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
worker3: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py:1330: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
worker3:   total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
worker2:   total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
worker2: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py:1330: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
worker2:   total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
worker3: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py:1330: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
worker2: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py:1330: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py:1330: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py:1330: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
worker3:   total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
worker2:   total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
head:   total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
worker1:   total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
worker2: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py:1330: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py:1330: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
worker1:   total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py:1330: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
head:   total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
worker1: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py:1330: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
worker1:   total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py:1330: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
head:   total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
worker2:   total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
head: /fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py:1330: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
head:   total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
worker1: [2024-06-14 08:01:49,193] [WARNING] [stage3.py:1991:step] 32 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
worker1: [2024-06-14 08:02:02,349] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192
worker1: [step:23][stage:generate-experience][dt:322.13]
worker1: [step:23][stage:actor-inference][dt:13.33]
worker1: [step:23][stage:ref-inference][dt:14.67]
worker1: [step:23][stage:reward-inference][dt:30.48]
worker1: [step:23][stage:critic-inference][dt:12.87]
worker1: [step:23][stage:total-generate][dt:400.14]
worker1: [step:23][stage:actor-train][dt:67.0]
worker1: [step:23][stage:critic-train][dt:68.87]
worker1: [step:23][stage:total-train][dt:136.59]
worker1: ========== [step:23] end, E2E [dt:539.29] ==========
worker1: ========== [step:31] start ==========
worker1: Epoch: 0 | Step: 31 | PPO Epoch: 1 | Actor Loss: 0.004348794813267887 | Critic Loss: 0.01058241561986506 | Unsupervised Loss: 0.0
worker1: End-to-End => Latency: 458.36s, TFLOPs: 36.43, Samples/sec: 2.23, Time/seq 0.45s, Batch Size: 1024, Total Seq. Length: 2048
worker1: Generation => Latency: 40.22s, Per-token Latency 39.28 ms, TFLOPs: 5.77, BW: -1.00 GB/sec, Answer Seq. Length: 1024
worker1: Training   => Latency: 136.59s, TFLOPs: 108.67
worker1: Actor Model Parameters => 6.738 B, Critic Model Parameters => 6.607 B
worker1: Average reward score: -0.493896484375 | EMA reward score: -0.46378735351562506
worker1: -------------------------------------------------------------------------------------
worker1: |E2E latency=187.19s |Gather latency=0.80s (0.43%) |Generate time=39.42s (21.06%) |Training time=132.96s (71.03%) |Others=14.81 (7.91%)|CurSamplesPerSec=0.68 |AvgSamplesPerSec=1.88
worker1: |E2E latency=50.32s |Gather latency=0.96s (1.91%) |Generate time=39.20s (77.90%) |Training time=0.00s (0.00%) |Others=11.12 (22.10%)|CurSamplesPerSec=2.54 |AvgSamplesPerSec=1.90
worker1: Invalidate trace cache @ step 454: expected module 453, but got module 1
worker1: Invalidate trace cache @ step 452: expected module 907, but got module 908
worker1: |E2E latency=49.54s |Gather latency=0.91s (1.83%) |Generate time=39.45s (79.63%) |Training time=0.00s (0.00%) |Others=10.09 (20.37%)|CurSamplesPerSec=2.58 |AvgSamplesPerSec=1.91
worker1: |E2E latency=50.21s |Gather latency=0.93s (1.85%) |Generate time=39.26s (78.20%) |Training time=0.00s (0.00%) |Others=10.94 (21.80%)|CurSamplesPerSec=2.55 |AvgSamplesPerSec=1.93
worker1: |E2E latency=49.38s |Gather latency=0.80s (1.63%) |Generate time=39.39s (79.78%) |Training time=0.00s (0.00%) |Others=9.99 (20.22%)|CurSamplesPerSec=2.59 |AvgSamplesPerSec=1.94
worker1: |E2E latency=51.82s |Gather latency=0.94s (1.81%) |Generate time=39.31s (75.86%) |Training time=0.00s (0.00%) |Others=12.51 (24.14%)|CurSamplesPerSec=2.47 |AvgSamplesPerSec=1.95
worker1: |E2E latency=49.89s |Gather latency=0.89s (1.79%) |Generate time=39.46s (79.09%) |Training time=0.00s (0.00%) |Others=10.43 (20.91%)|CurSamplesPerSec=2.57 |AvgSamplesPerSec=1.96
worker1: |E2E latency=49.24s |Gather latency=0.76s (1.55%) |Generate time=39.30s (79.82%) |Training time=0.00s (0.00%) |Others=9.94 (20.18%)|CurSamplesPerSec=2.60 |AvgSamplesPerSec=1.98
worker1: [2024-06-14 08:10:45,357] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192
worker1: [2024-06-14 08:10:58,560] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096
worker1: [step:31][stage:generate-experience][dt:321.73]
worker1: [step:31][stage:actor-inference][dt:13.25]
worker1: [step:31][stage:ref-inference][dt:13.69]
worker1: [step:31][stage:reward-inference][dt:31.09]
worker1: [step:31][stage:critic-inference][dt:12.83]
worker1: [step:31][stage:total-generate][dt:398.96]
worker1: [step:31][stage:actor-train][dt:65.66]
worker1: [step:31][stage:critic-train][dt:68.27]
worker1: [step:31][stage:total-train][dt:134.63]
worker1: ========== [step:31] end, E2E [dt:536.21] ==========
worker1: ========== [step:39] start ==========
worker1: Epoch: 0 | Step: 39 | PPO Epoch: 1 | Actor Loss: -4.005586743354797 | Critic Loss: 36.46196174621582 | Unsupervised Loss: 0.0
worker1: End-to-End => Latency: 455.90s, TFLOPs: 36.63, Samples/sec: 2.25, Time/seq 0.45s, Batch Size: 1024, Total Seq. Length: 2048
worker1: Generation => Latency: 40.16s, Per-token Latency 39.22 ms, TFLOPs: 5.78, BW: -1.00 GB/sec, Answer Seq. Length: 1024
worker1: Training   => Latency: 134.63s, TFLOPs: 110.25
worker1: Actor Model Parameters => 6.738 B, Critic Model Parameters => 6.607 B
worker1: Average reward score: -0.4716796875 | EMA reward score: -0.46457658691406256
worker1: -------------------------------------------------------------------------------------
worker1: |E2E latency=185.81s |Gather latency=0.92s (0.49%) |Generate time=39.24s (21.12%) |Training time=131.73s (70.89%) |Others=14.84 (7.99%)|CurSamplesPerSec=0.69 |AvgSamplesPerSec=1.89
worker1: |E2E latency=51.78s |Gather latency=0.99s (1.90%) |Generate time=39.27s (75.84%) |Training time=0.00s (0.00%) |Others=12.51 (24.16%)|CurSamplesPerSec=2.47 |AvgSamplesPerSec=1.90
worker1: Invalidate trace cache @ step 454: expected module 453, but got module 1
worker1: Invalidate trace cache @ step 452: expected module 907, but got module 908
worker1: |E2E latency=49.56s |Gather latency=1.04s (2.10%) |Generate time=39.23s (79.15%) |Training time=0.00s (0.00%) |Others=10.33 (20.85%)|CurSamplesPerSec=2.58 |AvgSamplesPerSec=1.91
worker1: |E2E latency=50.55s |Gather latency=0.97s (1.93%) |Generate time=39.31s (77.77%) |Training time=0.00s (0.00%) |Others=11.24 (22.23%)|CurSamplesPerSec=2.53 |AvgSamplesPerSec=1.92
worker1: |E2E latency=50.32s |Gather latency=0.79s (1.56%) |Generate time=39.26s (78.02%) |Training time=0.00s (0.00%) |Others=11.06 (21.98%)|CurSamplesPerSec=2.54 |AvgSamplesPerSec=1.93
worker1: |E2E latency=50.32s |Gather latency=0.77s (1.52%) |Generate time=39.34s (78.20%) |Training time=0.00s (0.00%) |Others=10.97 (21.80%)|CurSamplesPerSec=2.54 |AvgSamplesPerSec=1.94
worker1: |E2E latency=49.14s |Gather latency=0.81s (1.64%) |Generate time=39.22s (79.82%) |Training time=0.00s (0.00%) |Others=9.92 (20.18%)|CurSamplesPerSec=2.61 |AvgSamplesPerSec=1.95
worker1: |E2E latency=50.87s |Gather latency=0.91s (1.78%) |Generate time=39.38s (77.42%) |Training time=0.00s (0.00%) |Others=11.49 (22.58%)|CurSamplesPerSec=2.52 |AvgSamplesPerSec=1.96
worker1: [2024-06-14 08:19:43,682] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096
worker1: [2024-06-14 08:19:55,946] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096, reducing to 2048
worker1: [step:39][stage:generate-experience][dt:321.29]
worker1: [step:39][stage:actor-inference][dt:13.31]
worker1: [step:39][stage:ref-inference][dt:16.37]
worker1: [step:39][stage:reward-inference][dt:29.43]
worker1: [step:39][stage:critic-inference][dt:12.9]
worker1: [step:39][stage:total-generate][dt:399.83]
worker1: [step:39][stage:actor-train][dt:66.03]
worker1: [step:39][stage:critic-train][dt:68.25]
worker1: [step:39][stage:total-train][dt:134.95]
worker1: ========== [step:39] end, E2E [dt:537.39] ==========
worker1: ========== [step:47] start ==========
worker1: Epoch: 0 | Step: 47 | PPO Epoch: 1 | Actor Loss: -4.991079479455948 | Critic Loss: 40.097984790802 | Unsupervised Loss: 0.0
worker1: End-to-End => Latency: 455.06s, TFLOPs: 36.69, Samples/sec: 2.25, Time/seq 0.44s, Batch Size: 1024, Total Seq. Length: 2048
worker1: Generation => Latency: 40.01s, Per-token Latency 39.08 ms, TFLOPs: 5.80, BW: -1.00 GB/sec, Answer Seq. Length: 1024
worker1: Training   => Latency: 134.95s, TFLOPs: 109.98
worker1: Actor Model Parameters => 6.738 B, Critic Model Parameters => 6.607 B
worker1: Average reward score: -0.47802734375 | EMA reward score: -0.4659216625976563
worker1: -------------------------------------------------------------------------------------
worker1: |E2E latency=184.86s |Gather latency=0.77s (0.42%) |Generate time=39.24s (21.23%) |Training time=131.81s (71.30%) |Others=13.81 (7.47%)|CurSamplesPerSec=0.69 |AvgSamplesPerSec=1.89
worker1: |E2E latency=50.91s |Gather latency=0.84s (1.65%) |Generate time=39.35s (77.30%) |Training time=0.00s (0.00%) |Others=11.55 (22.70%)|CurSamplesPerSec=2.51 |AvgSamplesPerSec=1.90
worker1: Invalidate trace cache @ step 454: expected module 453, but got module 1
worker1: Invalidate trace cache @ step 452: expected module 907, but got module 908
worker1: |E2E latency=49.95s |Gather latency=1.21s (2.43%) |Generate time=39.50s (79.08%) |Training time=0.00s (0.00%) |Others=10.45 (20.92%)|CurSamplesPerSec=2.56 |AvgSamplesPerSec=1.91
worker1: |E2E latency=49.91s |Gather latency=0.90s (1.81%) |Generate time=39.21s (78.57%) |Training time=0.00s (0.00%) |Others=10.70 (21.43%)|CurSamplesPerSec=2.56 |AvgSamplesPerSec=1.92
worker1: |E2E latency=50.38s |Gather latency=0.83s (1.66%) |Generate time=39.41s (78.21%) |Training time=0.00s (0.00%) |Others=10.98 (21.79%)|CurSamplesPerSec=2.54 |AvgSamplesPerSec=1.93
worker1: |E2E latency=49.42s |Gather latency=0.85s (1.72%) |Generate time=39.39s (79.70%) |Training time=0.00s (0.00%) |Others=10.03 (20.30%)|CurSamplesPerSec=2.59 |AvgSamplesPerSec=1.94
worker1: |E2E latency=49.43s |Gather latency=0.86s (1.74%) |Generate time=39.44s (79.80%) |Training time=0.00s (0.00%) |Others=9.98 (20.20%)|CurSamplesPerSec=2.59 |AvgSamplesPerSec=1.95
worker1: |E2E latency=50.13s |Gather latency=0.84s (1.67%) |Generate time=39.40s (78.60%) |Training time=0.00s (0.00%) |Others=10.73 (21.40%)|CurSamplesPerSec=2.55 |AvgSamplesPerSec=1.96
worker1: [2024-06-14 08:28:38,339] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096, reducing to 2048
worker1: [2024-06-14 08:28:50,624] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2048, reducing to 1024
worker1: [step:47][stage:generate-experience][dt:322.25]
worker1: [step:47][stage:actor-inference][dt:13.18]
worker1: [step:47][stage:ref-inference][dt:15.53]
worker1: [step:47][stage:reward-inference][dt:26.42]
worker1: [step:47][stage:critic-inference][dt:12.9]
worker1: [step:47][stage:total-generate][dt:396.95]
worker1: [step:47][stage:actor-train][dt:66.16]
worker1: [step:47][stage:critic-train][dt:68.27]
worker1: [step:47][stage:total-train][dt:135.15]
worker1: ========== [step:47] end, E2E [dt:534.68] ==========
worker1: ========== [step:55] start ==========
worker1: Epoch: 0 | Step: 55 | PPO Epoch: 1 | Actor Loss: -4.069307237863541 | Critic Loss: 35.489586353302 | Unsupervised Loss: 0.0
worker1: End-to-End => Latency: 456.87s, TFLOPs: 36.55, Samples/sec: 2.24, Time/seq 0.45s, Batch Size: 1024, Total Seq. Length: 2048
worker1: Generation => Latency: 40.21s, Per-token Latency 39.27 ms, TFLOPs: 5.77, BW: -1.00 GB/sec, Answer Seq. Length: 1024
worker1: Training   => Latency: 135.15s, TFLOPs: 109.82
worker1: Actor Model Parameters => 6.738 B, Critic Model Parameters => 6.607 B
worker1: Average reward score: -0.4541015625 | EMA reward score: -0.4647396525878907
worker1: -------------------------------------------------------------------------------------
worker1: |E2E latency=184.56s |Gather latency=0.90s (0.49%) |Generate time=39.31s (21.30%) |Training time=131.33s (71.16%) |Others=13.92 (7.54%)|CurSamplesPerSec=0.69 |AvgSamplesPerSec=1.89
worker1: |E2E latency=50.98s |Gather latency=0.87s (1.70%) |Generate time=39.36s (77.21%) |Training time=0.00s (0.00%) |Others=11.62 (22.79%)|CurSamplesPerSec=2.51 |AvgSamplesPerSec=1.90
worker1: Invalidate trace cache @ step 454: expected module 453, but got module 1
worker1: Invalidate trace cache @ step 452: expected module 907, but got module 908
worker1: |E2E latency=49.68s |Gather latency=0.88s (1.77%) |Generate time=39.47s (79.43%) |Training time=0.00s (0.00%) |Others=10.22 (20.57%)|CurSamplesPerSec=2.58 |AvgSamplesPerSec=1.91
worker1: |E2E latency=49.74s |Gather latency=0.86s (1.72%) |Generate time=39.24s (78.89%) |Training time=0.00s (0.00%) |Others=10.50 (21.11%)|CurSamplesPerSec=2.57 |AvgSamplesPerSec=1.92
worker1: |E2E latency=49.93s |Gather latency=0.83s (1.67%) |Generate time=39.42s (78.95%) |Training time=0.00s (0.00%) |Others=10.51 (21.05%)|CurSamplesPerSec=2.56 |AvgSamplesPerSec=1.93
worker1: |E2E latency=49.75s |Gather latency=0.83s (1.68%) |Generate time=39.38s (79.16%) |Training time=0.00s (0.00%) |Others=10.37 (20.84%)|CurSamplesPerSec=2.57 |AvgSamplesPerSec=1.93
worker1: |E2E latency=51.82s |Gather latency=0.78s (1.50%) |Generate time=39.47s (76.17%) |Training time=0.00s (0.00%) |Others=12.35 (23.83%)|CurSamplesPerSec=2.47 |AvgSamplesPerSec=1.94
worker1: |E2E latency=49.22s |Gather latency=0.81s (1.65%) |Generate time=39.33s (79.89%) |Training time=0.00s (0.00%) |Others=9.90 (20.11%)|CurSamplesPerSec=2.60 |AvgSamplesPerSec=1.95
worker1: [2024-06-14 08:37:34,035] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2048, reducing to 1024
worker1: [2024-06-14 08:37:46,681] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1024, reducing to 512
worker1: [step:55][stage:generate-experience][dt:321.84]
worker1: [step:55][stage:actor-inference][dt:13.2]
worker1: [step:55][stage:ref-inference][dt:14.49]
worker1: [step:55][stage:reward-inference][dt:30.43]
worker1: [step:55][stage:critic-inference][dt:12.84]
worker1: [step:55][stage:total-generate][dt:399.4]
worker1: [step:55][stage:actor-train][dt:65.45]
worker1: [step:55][stage:critic-train][dt:67.91]
worker1: [step:55][stage:total-train][dt:134.07]
worker1: ========== [step:55] end, E2E [dt:536.06] ==========
worker1: ========== [step:63] start ==========
worker1: Epoch: 0 | Step: 63 | PPO Epoch: 1 | Actor Loss: -4.928847163915634 | Critic Loss: 40.38346338272095 | Unsupervised Loss: 0.0
worker1: End-to-End => Latency: 456.67s, TFLOPs: 36.57, Samples/sec: 2.24, Time/seq 0.45s, Batch Size: 1024, Total Seq. Length: 2048
worker1: Generation => Latency: 40.32s, Per-token Latency 39.38 ms, TFLOPs: 5.75, BW: -1.00 GB/sec, Answer Seq. Length: 1024
worker1: Training   => Latency: 134.08s, TFLOPs: 110.71
worker1: Actor Model Parameters => 6.738 B, Critic Model Parameters => 6.607 B
worker1: Average reward score: -0.47705078125 | EMA reward score: -0.4659707654541016
worker1: -------------------------------------------------------------------------------------
worker1: |E2E latency=184.94s |Gather latency=0.82s (0.44%) |Generate time=39.50s (21.36%) |Training time=131.12s (70.89%) |Others=14.33 (7.75%)|CurSamplesPerSec=0.69 |AvgSamplesPerSec=1.90
worker1: |E2E latency=50.74s |Gather latency=0.78s (1.54%) |Generate time=39.24s (77.33%) |Training time=0.00s (0.00%) |Others=11.50 (22.67%)|CurSamplesPerSec=2.52 |AvgSamplesPerSec=1.90
worker1: Invalidate trace cache @ step 454: expected module 453, but got module 1
worker1: Invalidate trace cache @ step 452: expected module 907, but got module 908
worker1: |E2E latency=49.93s |Gather latency=1.01s (2.02%) |Generate time=39.54s (79.19%) |Training time=0.00s (0.00%) |Others=10.39 (20.81%)|CurSamplesPerSec=2.56 |AvgSamplesPerSec=1.91
worker1: |E2E latency=49.90s |Gather latency=1.07s (2.14%) |Generate time=39.28s (78.72%) |Training time=0.00s (0.00%) |Others=10.62 (21.28%)|CurSamplesPerSec=2.57 |AvgSamplesPerSec=1.92
worker1: |E2E latency=49.27s |Gather latency=0.81s (1.64%) |Generate time=39.36s (79.88%) |Training time=0.00s (0.00%) |Others=9.91 (20.12%)|CurSamplesPerSec=2.60 |AvgSamplesPerSec=1.93
worker1: |E2E latency=50.01s |Gather latency=0.81s (1.62%) |Generate time=39.32s (78.63%) |Training time=0.00s (0.00%) |Others=10.69 (21.37%)|CurSamplesPerSec=2.56 |AvgSamplesPerSec=1.93
worker1: |E2E latency=49.30s |Gather latency=0.80s (1.61%) |Generate time=39.33s (79.77%) |Training time=0.00s (0.00%) |Others=9.97 (20.23%)|CurSamplesPerSec=2.60 |AvgSamplesPerSec=1.94
worker1: |E2E latency=50.52s |Gather latency=0.87s (1.72%) |Generate time=39.35s (77.89%) |Training time=0.00s (0.00%) |Others=11.17 (22.11%)|CurSamplesPerSec=2.53 |AvgSamplesPerSec=1.95
worker1: [2024-06-14 08:46:28,045] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1024, reducing to 512
worker1: [2024-06-14 08:46:40,365] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 512, reducing to 256
worker1: [step:63][stage:generate-experience][dt:321.69]
worker1: [step:63][stage:actor-inference][dt:13.21]
worker1: [step:63][stage:ref-inference][dt:15.46]
worker1: [step:63][stage:reward-inference][dt:26.67]
worker1: [step:63][stage:critic-inference][dt:12.74]
worker1: [step:63][stage:total-generate][dt:396.25]
worker1: [step:63][stage:actor-train][dt:66.49]
worker1: [step:63][stage:critic-train][dt:67.49]
worker1: [step:63][stage:total-train][dt:134.74]
worker1: ========== [step:63] end, E2E [dt:533.68] ==========
worker1: ========== [step:71] start ==========
worker1: Epoch: 0 | Step: 71 | PPO Epoch: 1 | Actor Loss: -4.405260056257248 | Critic Loss: 36.668280839920044 | Unsupervised Loss: 0.0
worker1: End-to-End => Latency: 455.87s, TFLOPs: 36.63, Samples/sec: 2.25, Time/seq 0.45s, Batch Size: 1024, Total Seq. Length: 2048
worker1: Generation => Latency: 40.14s, Per-token Latency 39.20 ms, TFLOPs: 5.78, BW: -1.00 GB/sec, Answer Seq. Length: 1024
worker1: Training   => Latency: 134.74s, TFLOPs: 110.16
worker1: Actor Model Parameters => 6.738 B, Critic Model Parameters => 6.607 B
worker1: Average reward score: -0.4697265625 | EMA reward score: -0.4663463451586914
worker1: -------------------------------------------------------------------------------------
worker1: |E2E latency=184.02s |Gather latency=0.88s (0.48%) |Generate time=39.26s (21.34%) |Training time=130.82s (71.09%) |Others=13.94 (7.57%)|CurSamplesPerSec=0.70 |AvgSamplesPerSec=1.90
worker1: |E2E latency=50.36s |Gather latency=0.85s (1.69%) |Generate time=39.39s (78.22%) |Training time=0.00s (0.00%) |Others=10.97 (21.78%)|CurSamplesPerSec=2.54 |AvgSamplesPerSec=1.90
worker1: Invalidate trace cache @ step 454: expected module 453, but got module 1
worker1: Invalidate trace cache @ step 452: expected module 907, but got module 908
worker1: |E2E latency=49.57s |Gather latency=1.01s (2.04%) |Generate time=39.40s (79.48%) |Training time=0.00s (0.00%) |Others=10.17 (20.52%)|CurSamplesPerSec=2.58 |AvgSamplesPerSec=1.91
worker1: |E2E latency=49.44s |Gather latency=0.80s (1.63%) |Generate time=39.30s (79.49%) |Training time=0.00s (0.00%) |Others=10.14 (20.51%)|CurSamplesPerSec=2.59 |AvgSamplesPerSec=1.92
worker1: |E2E latency=50.47s |Gather latency=0.84s (1.67%) |Generate time=39.43s (78.12%) |Training time=0.00s (0.00%) |Others=11.04 (21.88%)|CurSamplesPerSec=2.54 |AvgSamplesPerSec=1.92
worker1: |E2E latency=50.46s |Gather latency=0.82s (1.62%) |Generate time=39.49s (78.25%) |Training time=0.00s (0.00%) |Others=10.98 (21.75%)|CurSamplesPerSec=2.54 |AvgSamplesPerSec=1.93
worker1: |E2E latency=49.47s |Gather latency=0.83s (1.68%) |Generate time=39.51s (79.87%) |Training time=0.00s (0.00%) |Others=9.96 (20.13%)|CurSamplesPerSec=2.59 |AvgSamplesPerSec=1.94
worker1: |E2E latency=50.49s |Gather latency=0.80s (1.58%) |Generate time=39.39s (78.02%) |Training time=0.00s (0.00%) |Others=11.10 (21.98%)|CurSamplesPerSec=2.54 |AvgSamplesPerSec=1.94
worker1: [2024-06-14 08:55:23,984] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 512, reducing to 256
worker1: [2024-06-14 08:55:23,985] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=9, lr=[9.637128268676638e-06, 9.637128268676638e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
worker1: [2024-06-14 08:55:23,985] [INFO] [timer.py:260:stop] epoch=0/micro_step=80/global_step=10, RunningAvgSamplesPerSec=15.473083018415794, CurrSamplesPerSec=15.417604047677768, MemAllocated=6.16GB, MaxMemAllocated=33.12GB
worker1: [2024-06-14 08:55:36,228] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 256, reducing to 128
worker1: [2024-06-14 08:55:36,229] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=10, lr=[5e-06, 5e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
worker1: [step:71][stage:generate-experience][dt:322.18]
worker1: [step:71][stage:actor-inference][dt:13.32]
worker1: [step:71][stage:ref-inference][dt:14.55]
worker1: [step:71][stage:reward-inference][dt:28.45]
worker1: [step:71][stage:critic-inference][dt:12.83]
worker1: [step:71][stage:total-generate][dt:397.93]
worker1: [step:71][stage:actor-train][dt:66.43]
worker1: [step:71][stage:critic-train][dt:68.14]
worker1: [step:71][stage:total-train][dt:135.32]
worker1: ========== [step:71] end, E2E [dt:535.86] ==========
worker1: ========== [step:79] start ==========
worker1: Epoch: 0 | Step: 79 | PPO Epoch: 1 | Actor Loss: -3.429445207118988 | Critic Loss: 32.75256967544556 | Unsupervised Loss: 0.0
worker1: End-to-End => Latency: 457.93s, TFLOPs: 36.46, Samples/sec: 2.24, Time/seq 0.45s, Batch Size: 1024, Total Seq. Length: 2048
worker1: Generation => Latency: 40.33s, Per-token Latency 39.38 ms, TFLOPs: 5.75, BW: -1.00 GB/sec, Answer Seq. Length: 1024
worker1: Training   => Latency: 135.33s, TFLOPs: 109.68
worker1: Actor Model Parameters => 6.738 B, Critic Model Parameters => 6.607 B
worker1: Average reward score: -0.4619140625 | EMA reward score: -0.46590311689282227
worker1: -------------------------------------------------------------------------------------
worker1: |E2E latency=185.61s |Gather latency=0.87s (0.47%) |Generate time=39.46s (21.26%) |Training time=132.26s (71.26%) |Others=13.89 (7.48%)|CurSamplesPerSec=0.69 |AvgSamplesPerSec=1.90
worker1: |E2E latency=50.92s |Gather latency=0.83s (1.63%) |Generate time=39.39s (77.35%) |Training time=0.00s (0.00%) |Others=11.53 (22.65%)|CurSamplesPerSec=2.51 |AvgSamplesPerSec=1.91
worker1: Invalidate trace cache @ step 454: expected module 453, but got module 1
worker1: Invalidate trace cache @ step 452: expected module 907, but got module 908
worker1: |E2E latency=49.73s |Gather latency=1.16s (2.34%) |Generate time=39.26s (78.94%) |Training time=0.00s (0.00%) |Others=10.47 (21.06%)|CurSamplesPerSec=2.57 |AvgSamplesPerSec=1.91
worker1: |E2E latency=51.21s |Gather latency=1.12s (2.18%) |Generate time=39.40s (76.94%) |Training time=0.00s (0.00%) |Others=11.81 (23.06%)|CurSamplesPerSec=2.50 |AvgSamplesPerSec=1.92
worker1: |E2E latency=50.99s |Gather latency=0.95s (1.87%) |Generate time=39.41s (77.29%) |Training time=0.00s (0.00%) |Others=11.58 (22.71%)|CurSamplesPerSec=2.51 |AvgSamplesPerSec=1.92
worker1: |E2E latency=50.12s |Gather latency=0.88s (1.76%) |Generate time=39.35s (78.51%) |Training time=0.00s (0.00%) |Others=10.77 (21.49%)|CurSamplesPerSec=2.55 |AvgSamplesPerSec=1.93
worker1: |E2E latency=49.37s |Gather latency=1.00s (2.03%) |Generate time=39.32s (79.65%) |Training time=0.00s (0.00%) |Others=10.05 (20.35%)|CurSamplesPerSec=2.59 |AvgSamplesPerSec=1.93
worker1: |E2E latency=51.22s |Gather latency=0.88s (1.72%) |Generate time=39.39s (76.92%) |Training time=0.00s (0.00%) |Others=11.82 (23.08%)|CurSamplesPerSec=2.50 |AvgSamplesPerSec=1.94
worker1: [2024-06-14 09:04:22,763] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 256, reducing to 128
worker1: [2024-06-14 09:04:35,004] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 128, reducing to 64
worker1: [step:79][stage:generate-experience][dt:322.71]
worker1: [step:79][stage:actor-inference][dt:13.46]
worker1: [step:79][stage:ref-inference][dt:15.18]
worker1: [step:79][stage:reward-inference][dt:30.66]
worker1: [step:79][stage:critic-inference][dt:12.88]
worker1: [step:79][stage:total-generate][dt:401.29]
worker1: [step:79][stage:actor-train][dt:65.94]
worker1: [step:79][stage:critic-train][dt:68.27]
worker1: [step:79][stage:total-train][dt:134.88]
worker1: ========== [step:79] end, E2E [dt:538.78] ==========
worker1: ========== [step:87] start ==========
worker1: Epoch: 0 | Step: 87 | PPO Epoch: 1 | Actor Loss: -4.099749960005283 | Critic Loss: 36.56810402870178 | Unsupervised Loss: 0.0
worker1: End-to-End => Latency: 457.75s, TFLOPs: 36.48, Samples/sec: 2.24, Time/seq 0.45s, Batch Size: 1024, Total Seq. Length: 2048
worker1: Generation => Latency: 40.36s, Per-token Latency 39.41 ms, TFLOPs: 5.75, BW: -1.00 GB/sec, Answer Seq. Length: 1024
worker1: Training   => Latency: 134.89s, TFLOPs: 110.04
worker1: Actor Model Parameters => 6.738 B, Critic Model Parameters => 6.607 B
worker1: Average reward score: -0.4658203125 | EMA reward score: -0.46589483645354
worker1: -------------------------------------------------------------------------------------
worker1: |E2E latency=185.22s |Gather latency=0.94s (0.51%) |Generate time=39.42s (21.28%) |Training time=131.92s (71.22%) |Others=13.88 (7.49%)|CurSamplesPerSec=0.69 |AvgSamplesPerSec=1.90
worker1: |E2E latency=52.47s |Gather latency=0.88s (1.67%) |Generate time=39.35s (75.00%) |Training time=0.00s (0.00%) |Others=13.12 (25.00%)|CurSamplesPerSec=2.44 |AvgSamplesPerSec=1.90
worker1: Invalidate trace cache @ step 454: expected module 453, but got module 1
worker1: Invalidate trace cache @ step 452: expected module 907, but got module 908
worker1: |E2E latency=49.84s |Gather latency=1.21s (2.43%) |Generate time=39.44s (79.13%) |Training time=0.00s (0.00%) |Others=10.40 (20.87%)|CurSamplesPerSec=2.57 |AvgSamplesPerSec=1.91
worker1: |E2E latency=50.71s |Gather latency=1.08s (2.14%) |Generate time=39.32s (77.54%) |Training time=0.00s (0.00%) |Others=11.39 (22.46%)|CurSamplesPerSec=2.52 |AvgSamplesPerSec=1.91
worker1: |E2E latency=49.34s |Gather latency=0.86s (1.73%) |Generate time=39.41s (79.87%) |Training time=0.00s (0.00%) |Others=9.93 (20.13%)|CurSamplesPerSec=2.59 |AvgSamplesPerSec=1.92
worker1: |E2E latency=49.69s |Gather latency=0.87s (1.76%) |Generate time=39.32s (79.14%) |Training time=0.00s (0.00%) |Others=10.37 (20.86%)|CurSamplesPerSec=2.58 |AvgSamplesPerSec=1.93
worker1: |E2E latency=50.71s |Gather latency=0.80s (1.57%) |Generate time=39.59s (78.07%) |Training time=0.00s (0.00%) |Others=11.12 (21.93%)|CurSamplesPerSec=2.52 |AvgSamplesPerSec=1.93
worker1: |E2E latency=49.18s |Gather latency=0.90s (1.82%) |Generate time=39.20s (79.70%) |Training time=0.00s (0.00%) |Others=9.98 (20.30%)|CurSamplesPerSec=2.60 |AvgSamplesPerSec=1.94
worker1: [2024-06-14 09:13:18,132] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 128, reducing to 64
worker1: [2024-06-14 09:13:31,542] [WARNING] [stage3.py:1991:step] 102 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
worker1: [step:87][stage:generate-experience][dt:322.54]
worker1: [step:87][stage:actor-inference][dt:13.27]
worker1: [step:87][stage:ref-inference][dt:16.94]
worker1: [step:87][stage:reward-inference][dt:26.68]
worker1: [step:87][stage:critic-inference][dt:12.85]
worker1: [step:87][stage:total-generate][dt:398.47]
worker1: [step:87][stage:actor-train][dt:66.29]
worker1: [step:87][stage:critic-train][dt:68.44]
worker1: [step:87][stage:total-train][dt:135.43]
worker1: ========== [step:87] end, E2E [dt:536.54] ==========
worker1: ========== [step:95] start ==========
worker1: Epoch: 0 | Step: 95 | PPO Epoch: 1 | Actor Loss: -4.528084456920624 | Critic Loss: 38.95994234085083 | Unsupervised Loss: 0.0
worker1: End-to-End => Latency: 457.99s, TFLOPs: 36.46, Samples/sec: 2.24, Time/seq 0.45s, Batch Size: 1024, Total Seq. Length: 2048
worker1: Generation => Latency: 40.32s, Per-token Latency 39.37 ms, TFLOPs: 5.75, BW: -1.00 GB/sec, Answer Seq. Length: 1024
worker1: Training   => Latency: 135.43s, TFLOPs: 109.59
worker1: Actor Model Parameters => 6.738 B, Critic Model Parameters => 6.607 B
worker1: Average reward score: -0.47021484375 | EMA reward score: -0.466326837183186
worker1: -------------------------------------------------------------------------------------
worker1: |E2E latency=184.60s |Gather latency=0.93s (0.50%) |Generate time=39.39s (21.34%) |Training time=130.18s (70.52%) |Others=15.03 (8.14%)|CurSamplesPerSec=0.69 |AvgSamplesPerSec=1.90
worker1: |E2E latency=50.64s |Gather latency=0.85s (1.68%) |Generate time=39.13s (77.27%) |Training time=0.00s (0.00%) |Others=11.51 (22.73%)|CurSamplesPerSec=2.53 |AvgSamplesPerSec=1.91
worker1: Invalidate trace cache @ step 454: expected module 453, but got module 1
worker1: Invalidate trace cache @ step 452: expected module 907, but got module 908
worker1: |E2E latency=49.83s |Gather latency=1.05s (2.11%) |Generate time=39.43s (79.13%) |Training time=0.00s (0.00%) |Others=10.40 (20.87%)|CurSamplesPerSec=2.57 |AvgSamplesPerSec=1.91
worker1: |E2E latency=50.91s |Gather latency=1.01s (1.99%) |Generate time=39.28s (77.15%) |Training time=0.00s (0.00%) |Others=11.63 (22.85%)|CurSamplesPerSec=2.51 |AvgSamplesPerSec=1.91
worker1: |E2E latency=50.63s |Gather latency=0.82s (1.62%) |Generate time=39.36s (77.75%) |Training time=0.00s (0.00%) |Others=11.26 (22.25%)|CurSamplesPerSec=2.53 |AvgSamplesPerSec=1.92
worker1: |E2E latency=49.68s |Gather latency=0.90s (1.82%) |Generate time=39.39s (79.28%) |Training time=0.00s (0.00%) |Others=10.29 (20.72%)|CurSamplesPerSec=2.58 |AvgSamplesPerSec=1.92
worker1: |E2E latency=51.03s |Gather latency=0.90s (1.76%) |Generate time=39.32s (77.05%) |Training time=0.00s (0.00%) |Others=11.71 (22.95%)|CurSamplesPerSec=2.51 |AvgSamplesPerSec=1.93
worker1: |E2E latency=51.10s |Gather latency=0.86s (1.68%) |Generate time=39.38s (77.06%) |Training time=0.00s (0.00%) |Others=11.72 (22.94%)|CurSamplesPerSec=2.50 |AvgSamplesPerSec=1.93
worker1: [2024-06-14 09:22:16,066] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 64, reducing to 32
worker1: [2024-06-14 09:22:28,298] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 64, reducing to 32
worker1: [step:95][stage:generate-experience][dt:322.07]
worker1: [step:95][stage:actor-inference][dt:13.26]
worker1: [step:95][stage:ref-inference][dt:14.85]
worker1: [step:95][stage:reward-inference][dt:31.12]
worker1: [step:95][stage:critic-inference][dt:12.79]
worker1: [step:95][stage:total-generate][dt:400.37]
worker1: [step:95][stage:actor-train][dt:65.42]
worker1: [step:95][stage:critic-train][dt:67.52]
worker1: [step:95][stage:total-train][dt:133.76]
worker1: ========== [step:95] end, E2E [dt:536.76] ==========
worker1: ========== [step:103] start ==========
worker1: Epoch: 0 | Step: 103 | PPO Epoch: 1 | Actor Loss: -4.727197617292404 | Critic Loss: 39.56245231628418 | Unsupervised Loss: 0.0
worker1: End-to-End => Latency: 456.85s, TFLOPs: 36.55, Samples/sec: 2.24, Time/seq 0.45s, Batch Size: 1024, Total Seq. Length: 2048
worker1: Generation => Latency: 40.39s, Per-token Latency 39.44 ms, TFLOPs: 5.74, BW: -1.00 GB/sec, Answer Seq. Length: 1024
worker1: Training   => Latency: 133.76s, TFLOPs: 110.97
worker1: Actor Model Parameters => 6.738 B, Critic Model Parameters => 6.607 B
worker1: Average reward score: -0.4677734375 | EMA reward score: -0.4664714972148674
worker1: -------------------------------------------------------------------------------------
worker1: |E2E latency=182.93s |Gather latency=0.88s (0.48%) |Generate time=39.50s (21.59%) |Training time=129.62s (70.86%) |Others=13.81 (7.55%)|CurSamplesPerSec=0.70 |AvgSamplesPerSec=1.90
worker1: |E2E latency=51.06s |Gather latency=0.84s (1.64%) |Generate time=39.38s (77.11%) |Training time=0.00s (0.00%) |Others=11.69 (22.89%)|CurSamplesPerSec=2.51 |AvgSamplesPerSec=1.91
worker1: Invalidate trace cache @ step 454: expected module 453, but got module 1
worker1: Invalidate trace cache @ step 452: expected module 907, but got module 908
worker1: |E2E latency=49.63s |Gather latency=0.91s (1.84%) |Generate time=39.55s (79.68%) |Training time=0.00s (0.00%) |Others=10.09 (20.32%)|CurSamplesPerSec=2.58 |AvgSamplesPerSec=1.91
worker1: |E2E latency=51.07s |Gather latency=0.87s (1.71%) |Generate time=39.56s (77.47%) |Training time=0.00s (0.00%) |Others=11.51 (22.53%)|CurSamplesPerSec=2.51 |AvgSamplesPerSec=1.91
worker1: |E2E latency=51.94s |Gather latency=0.97s (1.88%) |Generate time=39.63s (76.31%) |Training time=0.00s (0.00%) |Others=12.30 (23.69%)|CurSamplesPerSec=2.46 |AvgSamplesPerSec=1.92
worker1: |E2E latency=49.73s |Gather latency=0.80s (1.60%) |Generate time=39.43s (79.28%) |Training time=0.00s (0.00%) |Others=10.31 (20.72%)|CurSamplesPerSec=2.57 |AvgSamplesPerSec=1.92
worker1: |E2E latency=49.46s |Gather latency=0.91s (1.85%) |Generate time=39.45s (79.76%) |Training time=0.00s (0.00%) |Others=10.01 (20.24%)|CurSamplesPerSec=2.59 |AvgSamplesPerSec=1.93
worker1: |E2E latency=49.44s |Gather latency=0.87s (1.75%) |Generate time=39.37s (79.63%) |Training time=0.00s (0.00%) |Others=10.07 (20.37%)|CurSamplesPerSec=2.59 |AvgSamplesPerSec=1.93
worker1: [2024-06-14 09:31:14,310] [WARNING] [stage3.py:1991:step] 87 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
worker1: [2024-06-14 09:31:27,621] [WARNING] [stage3.py:1991:step] 18 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
worker1: [step:103][stage:generate-experience][dt:322.91]
worker1: [step:103][stage:actor-inference][dt:13.31]
worker1: [step:103][stage:ref-inference][dt:13.72]
worker1: [step:103][stage:reward-inference][dt:30.95]
worker1: [step:103][stage:critic-inference][dt:12.85]
worker1: [step:103][stage:total-generate][dt:400.16]
worker1: [step:103][stage:actor-train][dt:66.81]
worker1: [step:103][stage:critic-train][dt:69.0]
worker1: [step:103][stage:total-train][dt:136.54]
worker1: ========== [step:103] end, E2E [dt:539.32] ==========
worker1: ========== [step:111] start ==========
worker1: Epoch: 0 | Step: 111 | PPO Epoch: 1 | Actor Loss: -4.199266314506531 | Critic Loss: 36.65987300872803 | Unsupervised Loss: 0.0
worker1: End-to-End => Latency: 459.53s, TFLOPs: 36.34, Samples/sec: 2.23, Time/seq 0.45s, Batch Size: 1024, Total Seq. Length: 2048
worker1: Generation => Latency: 40.37s, Per-token Latency 39.43 ms, TFLOPs: 5.74, BW: -1.00 GB/sec, Answer Seq. Length: 1024
worker1: Training   => Latency: 136.54s, TFLOPs: 108.71
worker1: Actor Model Parameters => 6.738 B, Critic Model Parameters => 6.607 B
worker1: Average reward score: -0.461669921875 | EMA reward score: -0.46599133968088063
worker1: -------------------------------------------------------------------------------------
worker1: |E2E latency=186.99s |Gather latency=0.94s (0.50%) |Generate time=39.44s (21.09%) |Training time=132.51s (70.87%) |Others=15.04 (8.04%)|CurSamplesPerSec=0.68 |AvgSamplesPerSec=1.90
worker1: |E2E latency=49.81s |Gather latency=1.03s (2.06%) |Generate time=39.08s (78.47%) |Training time=0.00s (0.00%) |Others=10.72 (21.53%)|CurSamplesPerSec=2.57 |AvgSamplesPerSec=1.91
worker1: Invalidate trace cache @ step 454: expected module 453, but got module 1
worker1: Invalidate trace cache @ step 452: expected module 907, but got module 908
worker1: |E2E latency=49.14s |Gather latency=0.79s (1.61%) |Generate time=39.21s (79.79%) |Training time=0.00s (0.00%) |Others=9.93 (20.21%)|CurSamplesPerSec=2.60 |AvgSamplesPerSec=1.91
worker1: |E2E latency=49.95s |Gather latency=0.85s (1.70%) |Generate time=39.15s (78.37%) |Training time=0.00s (0.00%) |Others=10.80 (21.63%)|CurSamplesPerSec=2.56 |AvgSamplesPerSec=1.91
worker1: |E2E latency=50.53s |Gather latency=0.82s (1.62%) |Generate time=39.10s (77.38%) |Training time=0.00s (0.00%) |Others=11.43 (22.62%)|CurSamplesPerSec=2.53 |AvgSamplesPerSec=1.92
worker1: |E2E latency=49.46s |Gather latency=0.88s (1.79%) |Generate time=39.39s (79.63%) |Training time=0.00s (0.00%) |Others=10.07 (20.37%)|CurSamplesPerSec=2.59 |AvgSamplesPerSec=1.92
worker1: |E2E latency=49.19s |Gather latency=0.84s (1.72%) |Generate time=39.19s (79.69%) |Training time=0.00s (0.00%) |Others=9.99 (20.31%)|CurSamplesPerSec=2.60 |AvgSamplesPerSec=1.93
worker1: |E2E latency=50.57s |Gather latency=1.05s (2.08%) |Generate time=39.07s (77.25%) |Training time=0.00s (0.00%) |Others=11.50 (22.75%)|CurSamplesPerSec=2.53 |AvgSamplesPerSec=1.93
worker1: [2024-06-14 09:40:09,185] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32, reducing to 16
worker1: [2024-06-14 09:40:21,445] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32, reducing to 16
worker1: [step:111][stage:generate-experience][dt:320.62]
worker1: [step:111][stage:actor-inference][dt:13.21]
worker1: [step:111][stage:ref-inference][dt:13.15]
worker1: [step:111][stage:reward-inference][dt:30.61]
worker1: [step:111][stage:critic-inference][dt:12.85]
worker1: [step:111][stage:total-generate][dt:396.14]
worker1: [step:111][stage:actor-train][dt:65.81]
worker1: [step:111][stage:critic-train][dt:68.47]
worker1: [step:111][stage:total-train][dt:134.99]
worker1: ========== [step:111] end, E2E [dt:533.82] ==========
worker1: ========== [step:119] start ==========
worker1: Epoch: 0 | Step: 119 | PPO Epoch: 1 | Actor Loss: nan | Critic Loss: nan | Unsupervised Loss: 0.0
worker1: End-to-End => Latency: 456.29s, TFLOPs: 36.60, Samples/sec: 2.24, Time/seq 0.45s, Batch Size: 1024, Total Seq. Length: 2048
worker1: Generation => Latency: 40.16s, Per-token Latency 39.22 ms, TFLOPs: 5.77, BW: -1.00 GB/sec, Answer Seq. Length: 1024
worker1: Training   => Latency: 134.99s, TFLOPs: 109.95
worker1: Actor Model Parameters => 6.738 B, Critic Model Parameters => 6.607 B
worker1: Average reward score: -0.468505859375 | EMA reward score: -0.46624279165029253
worker1: -------------------------------------------------------------------------------------
worker1: |E2E latency=185.19s |Gather latency=0.91s (0.49%) |Generate time=39.25s (21.20%) |Training time=132.05s (71.30%) |Others=13.89 (7.50%)|CurSamplesPerSec=0.69 |AvgSamplesPerSec=1.90
worker1: |E2E latency=50.48s |Gather latency=0.98s (1.94%) |Generate time=39.09s (77.43%) |Training time=0.00s (0.00%) |Others=11.39 (22.57%)|CurSamplesPerSec=2.54 |AvgSamplesPerSec=1.91
worker1: Invalidate trace cache @ step 454: expected module 453, but got module 1
worker1: Invalidate trace cache @ step 452: expected module 907, but got module 908
worker1: |E2E latency=49.62s |Gather latency=0.95s (1.92%) |Generate time=39.22s (79.03%) |Training time=0.00s (0.00%) |Others=10.40 (20.97%)|CurSamplesPerSec=2.58 |AvgSamplesPerSec=1.91
worker1: |E2E latency=50.60s |Gather latency=0.87s (1.72%) |Generate time=39.19s (77.45%) |Training time=0.00s (0.00%) |Others=11.41 (22.55%)|CurSamplesPerSec=2.53 |AvgSamplesPerSec=1.91
worker1: |E2E latency=49.18s |Gather latency=0.80s (1.62%) |Generate time=39.25s (79.82%) |Training time=0.00s (0.00%) |Others=9.93 (20.18%)|CurSamplesPerSec=2.60 |AvgSamplesPerSec=1.92
worker1: |E2E latency=49.45s |Gather latency=0.79s (1.59%) |Generate time=39.16s (79.19%) |Training time=0.00s (0.00%) |Others=10.29 (20.81%)|CurSamplesPerSec=2.59 |AvgSamplesPerSec=1.92
worker1: |E2E latency=50.51s |Gather latency=0.77s (1.53%) |Generate time=39.25s (77.71%) |Training time=0.00s (0.00%) |Others=11.26 (22.29%)|CurSamplesPerSec=2.53 |AvgSamplesPerSec=1.93
worker1: |E2E latency=49.26s |Gather latency=0.78s (1.58%) |Generate time=39.30s (79.78%) |Training time=0.00s (0.00%) |Others=9.96 (20.22%)|CurSamplesPerSec=2.60 |AvgSamplesPerSec=1.93
worker1: [2024-06-14 09:49:00,523] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16, reducing to 8
worker1: [2024-06-14 09:49:12,834] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16, reducing to 8
worker1: [step:119][stage:generate-experience][dt:320.56]
worker1: [step:119][stage:actor-inference][dt:12.97]
worker1: [step:119][stage:ref-inference][dt:15.19]
worker1: [step:119][stage:reward-inference][dt:26.96]
worker1: [step:119][stage:critic-inference][dt:12.71]
worker1: [step:119][stage:total-generate][dt:395.32]
worker1: [step:119][stage:actor-train][dt:66.11]
worker1: [step:119][stage:critic-train][dt:66.45]
worker1: [step:119][stage:total-train][dt:133.31]
worker1: ========== [step:119] end, E2E [dt:531.39] ==========
worker1: ========== [step:127] start ==========
worker1: Epoch: 0 | Step: 127 | PPO Epoch: 1 | Actor Loss: nan | Critic Loss: nan | Unsupervised Loss: 0.0
worker1: End-to-End => Latency: 454.62s, TFLOPs: 36.73, Samples/sec: 2.25, Time/seq 0.44s, Batch Size: 1024, Total Seq. Length: 2048
worker1: Generation => Latency: 40.16s, Per-token Latency 39.22 ms, TFLOPs: 5.77, BW: -1.00 GB/sec, Answer Seq. Length: 1024
worker1: Training   => Latency: 133.32s, TFLOPs: 111.34
worker1: Actor Model Parameters => 6.738 B, Critic Model Parameters => 6.607 B
worker1: Average reward score: -0.47021484375 | EMA reward score: -0.46663999686026325
worker1: -------------------------------------------------------------------------------------
worker1: |E2E latency=182.29s |Gather latency=0.95s (0.52%) |Generate time=39.21s (21.51%) |Training time=129.07s (70.80%) |Others=14.02 (7.69%)|CurSamplesPerSec=0.70 |AvgSamplesPerSec=1.90
worker1: |E2E latency=50.95s |Gather latency=0.88s (1.72%) |Generate time=39.16s (76.87%) |Training time=0.00s (0.00%) |Others=11.78 (23.13%)|CurSamplesPerSec=2.51 |AvgSamplesPerSec=1.91
worker1: Invalidate trace cache @ step 454: expected module 453, but got module 1
worker1: Invalidate trace cache @ step 452: expected module 907, but got module 908
worker1: |E2E latency=50.32s |Gather latency=1.14s (2.26%) |Generate time=40.12s (79.74%) |Training time=0.00s (0.00%) |Others=10.20 (20.26%)|CurSamplesPerSec=2.54 |AvgSamplesPerSec=1.91
worker1: |E2E latency=50.79s |Gather latency=0.80s (1.58%) |Generate time=39.14s (77.05%) |Training time=0.00s (0.00%) |Others=11.65 (22.95%)|CurSamplesPerSec=2.52 |AvgSamplesPerSec=1.91
worker1: |E2E latency=49.29s |Gather latency=0.90s (1.83%) |Generate time=39.29s (79.70%) |Training time=0.00s (0.00%) |Others=10.00 (20.30%)|CurSamplesPerSec=2.60 |AvgSamplesPerSec=1.92
worker1: |E2E latency=50.34s |Gather latency=0.81s (1.62%) |Generate time=39.32s (78.09%) |Training time=0.00s (0.00%) |Others=11.03 (21.91%)|CurSamplesPerSec=2.54 |AvgSamplesPerSec=1.92
worker1: |E2E latency=50.28s |Gather latency=0.88s (1.75%) |Generate time=39.16s (77.88%) |Training time=0.00s (0.00%) |Others=11.12 (22.12%)|CurSamplesPerSec=2.55 |AvgSamplesPerSec=1.93
worker1: |E2E latency=50.12s |Gather latency=0.89s (1.78%) |Generate time=39.32s (78.45%) |Training time=0.00s (0.00%) |Others=10.80 (21.55%)|CurSamplesPerSec=2.55 |AvgSamplesPerSec=1.93
worker1: [2024-06-14 09:57:55,926] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8, reducing to 4
worker1: [2024-06-14 09:58:08,225] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8, reducing to 4
worker1: [step:127][stage:generate-experience][dt:321.94]
worker1: [step:127][stage:actor-inference][dt:13.41]
worker1: [step:127][stage:ref-inference][dt:15.07]
worker1: [step:127][stage:reward-inference][dt:28.91]
worker1: [step:127][stage:critic-inference][dt:12.72]
worker1: [step:127][stage:total-generate][dt:398.28]
worker1: [step:127][stage:actor-train][dt:65.02]
worker1: [step:127][stage:critic-train][dt:68.6]
worker1: [step:127][stage:total-train][dt:134.37]
worker1: ========== [step:127] end, E2E [dt:535.39] ==========
worker1: ========== [step:135] start ==========
worker1: Epoch: 0 | Step: 135 | PPO Epoch: 1 | Actor Loss: nan | Critic Loss: nan | Unsupervised Loss: 0.0
worker1: End-to-End => Latency: 455.43s, TFLOPs: 36.66, Samples/sec: 2.25, Time/seq 0.44s, Batch Size: 1024, Total Seq. Length: 2048
worker1: Generation => Latency: 40.13s, Per-token Latency 39.19 ms, TFLOPs: 5.78, BW: -1.00 GB/sec, Answer Seq. Length: 1024
worker1: Training   => Latency: 134.38s, TFLOPs: 110.46
worker1: Actor Model Parameters => 6.738 B, Critic Model Parameters => 6.607 B
worker1: Average reward score: -0.467529296875 | EMA reward score: -0.46672892686173695
worker1: -------------------------------------------------------------------------------------
worker1: |E2E latency=183.30s |Gather latency=0.95s (0.52%) |Generate time=39.18s (21.37%) |Training time=130.12s (70.99%) |Others=14.01 (7.64%)|CurSamplesPerSec=0.70 |AvgSamplesPerSec=1.90
worker1: |E2E latency=50.97s |Gather latency=0.82s (1.62%) |Generate time=39.21s (76.92%) |Training time=0.00s (0.00%) |Others=11.77 (23.08%)|CurSamplesPerSec=2.51 |AvgSamplesPerSec=1.91
worker1: Invalidate trace cache @ step 454: expected module 453, but got module 1
worker1: Invalidate trace cache @ step 452: expected module 907, but got module 908
worker1: |E2E latency=50.01s |Gather latency=1.09s (2.17%) |Generate time=39.72s (79.41%) |Training time=0.00s (0.00%) |Others=10.30 (20.59%)|CurSamplesPerSec=2.56 |AvgSamplesPerSec=1.91
worker1: |E2E latency=49.80s |Gather latency=0.91s (1.83%) |Generate time=39.24s (78.79%) |Training time=0.00s (0.00%) |Others=10.56 (21.21%)|CurSamplesPerSec=2.57 |AvgSamplesPerSec=1.91
worker1: |E2E latency=50.07s |Gather latency=0.87s (1.74%) |Generate time=39.30s (78.49%) |Training time=0.00s (0.00%) |Others=10.77 (21.51%)|CurSamplesPerSec=2.56 |AvgSamplesPerSec=1.92
worker1: |E2E latency=50.38s |Gather latency=0.92s (1.83%) |Generate time=39.31s (78.03%) |Training time=0.00s (0.00%) |Others=11.07 (21.97%)|CurSamplesPerSec=2.54 |AvgSamplesPerSec=1.92
worker1: |E2E latency=49.21s |Gather latency=0.89s (1.81%) |Generate time=39.22s (79.69%) |Training time=0.00s (0.00%) |Others=10.00 (20.31%)|CurSamplesPerSec=2.60 |AvgSamplesPerSec=1.92
worker1: |E2E latency=50.41s |Gather latency=0.82s (1.62%) |Generate time=39.22s (77.81%) |Training time=0.00s (0.00%) |Others=11.19 (22.19%)|CurSamplesPerSec=2.54 |AvgSamplesPerSec=1.93
worker1: [2024-06-14 10:06:50,047] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4, reducing to 2
worker1: [2024-06-14 10:07:02,250] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4, reducing to 2
worker1: [step:135][stage:generate-experience][dt:321.83]
worker1: [step:135][stage:actor-inference][dt:13.29]
worker1: [step:135][stage:ref-inference][dt:14.97]
worker1: [step:135][stage:reward-inference][dt:27.99]
worker1: [step:135][stage:critic-inference][dt:12.66]
worker1: [step:135][stage:total-generate][dt:396.9]
worker1: [step:135][stage:actor-train][dt:66.92]
worker1: [step:135][stage:critic-train][dt:66.63]
worker1: [step:135][stage:total-train][dt:134.36]
worker1: ========== [step:135] end, E2E [dt:534.02] ==========
worker1: ========== [step:143] start ==========
worker1: Epoch: 0 | Step: 143 | PPO Epoch: 1 | Actor Loss: nan | Critic Loss: nan | Unsupervised Loss: 0.0
worker1: End-to-End => Latency: 456.67s, TFLOPs: 36.57, Samples/sec: 2.24, Time/seq 0.45s, Batch Size: 1024, Total Seq. Length: 2048
worker1: Generation => Latency: 40.29s, Per-token Latency 39.34 ms, TFLOPs: 5.76, BW: -1.00 GB/sec, Answer Seq. Length: 1024
worker1: Training   => Latency: 134.36s, TFLOPs: 110.47
worker1: Actor Model Parameters => 6.738 B, Critic Model Parameters => 6.607 B
worker1: Average reward score: -0.482666015625 | EMA reward score: -0.46832263573806326
worker1: -------------------------------------------------------------------------------------
worker1: |E2E latency=183.15s |Gather latency=1.09s (0.60%) |Generate time=39.20s (21.40%) |Training time=130.15s (71.06%) |Others=13.81 (7.54%)|CurSamplesPerSec=0.70 |AvgSamplesPerSec=1.90
worker1: |E2E latency=50.04s |Gather latency=0.83s (1.67%) |Generate time=39.18s (78.29%) |Training time=0.00s (0.00%) |Others=10.86 (21.71%)|CurSamplesPerSec=2.56 |AvgSamplesPerSec=1.91
worker1: Invalidate trace cache @ step 454: expected module 453, but got module 1
worker1: Invalidate trace cache @ step 452: expected module 907, but got module 908
worker1: |E2E latency=49.37s |Gather latency=0.87s (1.75%) |Generate time=39.27s (79.55%) |Training time=0.00s (0.00%) |Others=10.10 (20.45%)|CurSamplesPerSec=2.59 |AvgSamplesPerSec=1.91
worker1: |E2E latency=49.69s |Gather latency=0.89s (1.78%) |Generate time=39.25s (78.99%) |Training time=0.00s (0.00%) |Others=10.44 (21.01%)|CurSamplesPerSec=2.58 |AvgSamplesPerSec=1.91
worker1: |E2E latency=49.20s |Gather latency=0.89s (1.81%) |Generate time=39.23s (79.72%) |Training time=0.00s (0.00%) |Others=9.98 (20.28%)|CurSamplesPerSec=2.60 |AvgSamplesPerSec=1.92
worker1: |E2E latency=50.84s |Gather latency=0.84s (1.66%) |Generate time=39.32s (77.33%) |Training time=0.00s (0.00%) |Others=11.53 (22.67%)|CurSamplesPerSec=2.52 |AvgSamplesPerSec=1.92
worker1: |E2E latency=49.53s |Gather latency=0.99s (1.99%) |Generate time=39.14s (79.01%) |Training time=0.00s (0.00%) |Others=10.40 (20.99%)|CurSamplesPerSec=2.58 |AvgSamplesPerSec=1.92
worker1: |E2E latency=49.22s |Gather latency=0.81s (1.64%) |Generate time=39.26s (79.77%) |Training time=0.00s (0.00%) |Others=9.96 (20.23%)|CurSamplesPerSec=2.60 |AvgSamplesPerSec=1.93
worker1: [2024-06-14 10:15:41,210] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2, reducing to 1
worker1: [2024-06-14 10:15:53,480] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2, reducing to 1
worker1: [step:143][stage:generate-experience][dt:320.99]
worker1: [step:143][stage:actor-inference][dt:13.31]
worker1: [step:143][stage:ref-inference][dt:13.09]
worker1: [step:143][stage:reward-inference][dt:27.92]
worker1: [step:143][stage:critic-inference][dt:12.79]
worker1: [step:143][stage:total-generate][dt:394.2]
worker1: [step:143][stage:actor-train][dt:65.15]
worker1: [step:143][stage:critic-train][dt:68.42]
worker1: [step:143][stage:total-train][dt:134.26]
worker1: ========== [step:143] end, E2E [dt:531.23] ==========
worker1: ========== [step:151] start ==========
worker1: Epoch: 0 | Step: 151 | PPO Epoch: 1 | Actor Loss: nan | Critic Loss: nan | Unsupervised Loss: 0.0
worker1: End-to-End => Latency: 456.16s, TFLOPs: 36.61, Samples/sec: 2.24, Time/seq 0.45s, Batch Size: 1024, Total Seq. Length: 2048
worker1: Generation => Latency: 40.24s, Per-token Latency 39.29 ms, TFLOPs: 5.76, BW: -1.00 GB/sec, Answer Seq. Length: 1024
worker1: Training   => Latency: 134.26s, TFLOPs: 110.55
worker1: Actor Model Parameters => 6.738 B, Critic Model Parameters => 6.607 B
worker1: Average reward score: -0.468505859375 | EMA reward score: -0.4683409581017569
worker1: -------------------------------------------------------------------------------------
worker1: |E2E latency=183.33s |Gather latency=0.95s (0.52%) |Generate time=39.29s (21.43%) |Training time=130.11s (70.97%) |Others=13.93 (7.60%)|CurSamplesPerSec=0.70 |AvgSamplesPerSec=1.91
worker1: |E2E latency=50.23s |Gather latency=0.80s (1.60%) |Generate time=39.30s (78.23%) |Training time=0.00s (0.00%) |Others=10.93 (21.77%)|CurSamplesPerSec=2.55 |AvgSamplesPerSec=1.91
worker1: Invalidate trace cache @ step 454: expected module 453, but got module 1
worker1: Invalidate trace cache @ step 452: expected module 907, but got module 908
worker1: |E2E latency=49.50s |Gather latency=0.85s (1.71%) |Generate time=39.41s (79.62%) |Training time=0.00s (0.00%) |Others=10.09 (20.38%)|CurSamplesPerSec=2.59 |AvgSamplesPerSec=1.91
worker1: |E2E latency=50.83s |Gather latency=0.85s (1.67%) |Generate time=39.35s (77.42%) |Training time=0.00s (0.00%) |Others=11.48 (22.58%)|CurSamplesPerSec=2.52 |AvgSamplesPerSec=1.92
worker1: |E2E latency=49.68s |Gather latency=0.79s (1.60%) |Generate time=39.31s (79.13%) |Training time=0.00s (0.00%) |Others=10.37 (20.87%)|CurSamplesPerSec=2.58 |AvgSamplesPerSec=1.92
worker1: |E2E latency=50.13s |Gather latency=0.86s (1.71%) |Generate time=39.25s (78.31%) |Training time=0.00s (0.00%) |Others=10.87 (21.69%)|CurSamplesPerSec=2.55 |AvgSamplesPerSec=1.92
worker1: |E2E latency=49.14s |Gather latency=0.86s (1.74%) |Generate time=39.17s (79.71%) |Training time=0.00s (0.00%) |Others=9.97 (20.29%)|CurSamplesPerSec=2.60 |AvgSamplesPerSec=1.92
worker1: |E2E latency=50.75s |Gather latency=1.10s (2.16%) |Generate time=39.26s (77.35%) |Training time=0.00s (0.00%) |Others=11.49 (22.65%)|CurSamplesPerSec=2.52 |AvgSamplesPerSec=1.93
worker3: Traceback (most recent call last):
worker3:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 697, in <module>
worker3:     main()
worker3:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 571, in main
worker3:     actor_loss, critic_loss = trainer.train_rlhf(exp_data)
worker3:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py", line 267, in train_rlhf
worker3:     self.actor_model.step()
worker3:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/hybrid_engine.py", line 452, in step
worker3:     super().step(lr_kwargs=lr_kwargs)
worker3:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2148, in step
worker3:     self._take_model_step(lr_kwargs)
worker3:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2054, in _take_model_step
worker3:     self.optimizer.step()
worker3:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
worker3:     ret_val = func(*args, **kwargs)
worker3:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1946, in step
worker3:     if self._overflow_check_and_loss_scale_update():
worker3:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
worker3:     ret_val = func(*args, **kwargs)
worker3:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1894, in _overflow_check_and_loss_scale_update
worker3:     self._update_scale(self.overflow)
worker3:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 2281, in _update_scale
worker3:     self.loss_scaler.update_scale(has_overflow)
worker3:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 175, in update_scale
worker3:     raise Exception(
worker3: Exception: Current loss scale already at minimum - cannot decrease scale anymore. Exiting run.
worker3: Traceback (most recent call last):
worker3:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 697, in <module>
worker3:     main()
worker3:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 571, in main
worker3:     actor_loss, critic_loss = trainer.train_rlhf(exp_data)
worker3:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py", line 267, in train_rlhf
worker3:     self.actor_model.step()
worker3:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/hybrid_engine.py", line 452, in step
worker3:     super().step(lr_kwargs=lr_kwargs)
worker3:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2148, in step
worker3:     self._take_model_step(lr_kwargs)
worker3:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2054, in _take_model_step
worker3:     self.optimizer.step()
worker3:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
worker3:     ret_val = func(*args, **kwargs)
worker3:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1946, in step
worker3:     if self._overflow_check_and_loss_scale_update():
worker3:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
worker3:     ret_val = func(*args, **kwargs)
worker3:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1894, in _overflow_check_and_loss_scale_update
worker3:     self._update_scale(self.overflow)
worker3:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 2281, in _update_scale
worker3:     self.loss_scaler.update_scale(has_overflow)
worker3:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 175, in update_scale
worker3:     raise Exception(
worker3: Exception: Current loss scale already at minimum - cannot decrease scale anymore. Exiting run.
worker3: Traceback (most recent call last):
worker3:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 697, in <module>
worker3:     main()
worker3:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 571, in main
worker3:     actor_loss, critic_loss = trainer.train_rlhf(exp_data)
worker3:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py", line 267, in train_rlhf
worker3:     self.actor_model.step()
worker3:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/hybrid_engine.py", line 452, in step
worker3:     super().step(lr_kwargs=lr_kwargs)
worker3:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2148, in step
worker3:     self._take_model_step(lr_kwargs)
worker3:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2054, in _take_model_step
worker3:     self.optimizer.step()
worker3:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
worker3:     ret_val = func(*args, **kwargs)
worker3:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1946, in step
worker3:     if self._overflow_check_and_loss_scale_update():
worker3:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
worker3:     ret_val = func(*args, **kwargs)
worker3:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1894, in _overflow_check_and_loss_scale_update
worker3:     self._update_scale(self.overflow)
worker3:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 2281, in _update_scale
worker3:     self.loss_scaler.update_scale(has_overflow)
worker3:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 175, in update_scale
worker3:     raise Exception(
worker3: Exception: Current loss scale already at minimum - cannot decrease scale anymore. Exiting run.
worker3: Traceback (most recent call last):
worker3:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 697, in <module>
worker3:     main()
worker3:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 571, in main
worker3:     actor_loss, critic_loss = trainer.train_rlhf(exp_data)
worker3:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py", line 267, in train_rlhf
worker3:     self.actor_model.step()
worker3:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/hybrid_engine.py", line 452, in step
worker3:     super().step(lr_kwargs=lr_kwargs)
worker3:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2148, in step
worker3:     self._take_model_step(lr_kwargs)
worker3:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2054, in _take_model_step
worker3:     self.optimizer.step()
worker3:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
worker3:     ret_val = func(*args, **kwargs)
worker3:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1946, in step
worker3:     if self._overflow_check_and_loss_scale_update():
worker3:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
worker3:     ret_val = func(*args, **kwargs)
worker3:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1894, in _overflow_check_and_loss_scale_update
worker3:     self._update_scale(self.overflow)
worker3:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 2281, in _update_scale
worker3:     self.loss_scaler.update_scale(has_overflow)
worker3:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 175, in update_scale
worker3:     raise Exception(
worker3: Exception: Current loss scale already at minimum - cannot decrease scale anymore. Exiting run.
head: Traceback (most recent call last):
head:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 697, in <module>
head:     main()
head:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 571, in main
head:     actor_loss, critic_loss = trainer.train_rlhf(exp_data)
head:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py", line 267, in train_rlhf
head:     self.actor_model.step()
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/hybrid_engine.py", line 452, in step
head:     super().step(lr_kwargs=lr_kwargs)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2148, in step
head:     self._take_model_step(lr_kwargs)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2054, in _take_model_step
head:     self.optimizer.step()
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
head:     ret_val = func(*args, **kwargs)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1946, in step
head:     if self._overflow_check_and_loss_scale_update():
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
head:     ret_val = func(*args, **kwargs)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1894, in _overflow_check_and_loss_scale_update
head:     self._update_scale(self.overflow)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 2281, in _update_scale
head:     self.loss_scaler.update_scale(has_overflow)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 175, in update_scale
head:     raise Exception(
head: Exception: Current loss scale already at minimum - cannot decrease scale anymore. Exiting run.
head: Traceback (most recent call last):
head:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 697, in <module>
head:     main()
head:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 571, in main
head:     actor_loss, critic_loss = trainer.train_rlhf(exp_data)
head:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py", line 267, in train_rlhf
head:     self.actor_model.step()
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/hybrid_engine.py", line 452, in step
head:     super().step(lr_kwargs=lr_kwargs)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2148, in step
head:     self._take_model_step(lr_kwargs)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2054, in _take_model_step
head:     self.optimizer.step()
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
head:     ret_val = func(*args, **kwargs)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1946, in step
head:     if self._overflow_check_and_loss_scale_update():
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
head:     ret_val = func(*args, **kwargs)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1894, in _overflow_check_and_loss_scale_update
head:     self._update_scale(self.overflow)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 2281, in _update_scale
head:     self.loss_scaler.update_scale(has_overflow)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 175, in update_scale
head:     raise Exception(
head: Exception: Current loss scale already at minimum - cannot decrease scale anymore. Exiting run.
head: Traceback (most recent call last):
head:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 697, in <module>
head:     main()
head:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 571, in main
head:     actor_loss, critic_loss = trainer.train_rlhf(exp_data)
head:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py", line 267, in train_rlhf
head:     self.actor_model.step()
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/hybrid_engine.py", line 452, in step
head:     super().step(lr_kwargs=lr_kwargs)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2148, in step
head:     self._take_model_step(lr_kwargs)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2054, in _take_model_step
head:     self.optimizer.step()
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
head:     ret_val = func(*args, **kwargs)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1946, in step
head:     if self._overflow_check_and_loss_scale_update():
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
head:     ret_val = func(*args, **kwargs)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1894, in _overflow_check_and_loss_scale_update
head:     self._update_scale(self.overflow)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 2281, in _update_scale
head:     self.loss_scaler.update_scale(has_overflow)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 175, in update_scale
head:     raise Exception(
head: Exception: Current loss scale already at minimum - cannot decrease scale anymore. Exiting run.
head: Traceback (most recent call last):
head:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 697, in <module>
head:     main()
head:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 571, in main
head:     actor_loss, critic_loss = trainer.train_rlhf(exp_data)
head:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py", line 267, in train_rlhf
head:     self.actor_model.step()
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/hybrid_engine.py", line 452, in step
head:     super().step(lr_kwargs=lr_kwargs)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2148, in step
head:     self._take_model_step(lr_kwargs)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2054, in _take_model_step
head:     self.optimizer.step()
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
head:     ret_val = func(*args, **kwargs)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1946, in step
head:     if self._overflow_check_and_loss_scale_update():
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
head:     ret_val = func(*args, **kwargs)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1894, in _overflow_check_and_loss_scale_update
head:     self._update_scale(self.overflow)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 2281, in _update_scale
head:     self.loss_scaler.update_scale(has_overflow)
head:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 175, in update_scale
head:     raise Exception(
head: Exception: Current loss scale already at minimum - cannot decrease scale anymore. Exiting run.
worker1: Traceback (most recent call last):
worker1:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 697, in <module>
worker1:     main()
worker1:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 571, in main
worker1:     actor_loss, critic_loss = trainer.train_rlhf(exp_data)
worker1:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py", line 267, in train_rlhf
worker1:     self.actor_model.step()
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/hybrid_engine.py", line 452, in step
worker1:     super().step(lr_kwargs=lr_kwargs)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2148, in step
worker1:     self._take_model_step(lr_kwargs)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2054, in _take_model_step
worker1:     self.optimizer.step()
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
worker1:     ret_val = func(*args, **kwargs)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1946, in step
worker1:     if self._overflow_check_and_loss_scale_update():
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
worker1:     ret_val = func(*args, **kwargs)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1894, in _overflow_check_and_loss_scale_update
worker1:     self._update_scale(self.overflow)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 2281, in _update_scale
worker1:     self.loss_scaler.update_scale(has_overflow)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 175, in update_scale
worker1:     raise Exception(
worker1: Exception: Current loss scale already at minimum - cannot decrease scale anymore. Exiting run.
worker1: Traceback (most recent call last):
worker1:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 697, in <module>
worker1:     main()
worker1:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 571, in main
worker1:     actor_loss, critic_loss = trainer.train_rlhf(exp_data)
worker1:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py", line 267, in train_rlhf
worker1:     self.actor_model.step()
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/hybrid_engine.py", line 452, in step
worker1:     super().step(lr_kwargs=lr_kwargs)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2148, in step
worker1:     self._take_model_step(lr_kwargs)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2054, in _take_model_step
worker1:     self.optimizer.step()
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
worker1:     ret_val = func(*args, **kwargs)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1946, in step
worker1:     if self._overflow_check_and_loss_scale_update():
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
worker1:     ret_val = func(*args, **kwargs)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1894, in _overflow_check_and_loss_scale_update
worker1:     self._update_scale(self.overflow)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 2281, in _update_scale
worker1:     self.loss_scaler.update_scale(has_overflow)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 175, in update_scale
worker1:     raise Exception(
worker1: Exception: Current loss scale already at minimum - cannot decrease scale anymore. Exiting run.
worker1: Traceback (most recent call last):
worker1:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 697, in <module>
worker1:     main()
worker1:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 571, in main
worker1:     actor_loss, critic_loss = trainer.train_rlhf(exp_data)
worker1:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py", line 267, in train_rlhf
worker1:     self.actor_model.step()
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/hybrid_engine.py", line 452, in step
worker1:     super().step(lr_kwargs=lr_kwargs)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2148, in step
worker1:     self._take_model_step(lr_kwargs)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2054, in _take_model_step
worker1:     self.optimizer.step()
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
worker1:     ret_val = func(*args, **kwargs)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1946, in step
worker1:     if self._overflow_check_and_loss_scale_update():
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
worker1:     ret_val = func(*args, **kwargs)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1894, in _overflow_check_and_loss_scale_update
worker1:     self._update_scale(self.overflow)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 2281, in _update_scale
worker1:     self.loss_scaler.update_scale(has_overflow)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 175, in update_scale
worker1:     raise Exception(
worker1: Exception: Current loss scale already at minimum - cannot decrease scale anymore. Exiting run.
worker1: Traceback (most recent call last):
worker1:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 697, in <module>
worker1:     main()
worker1:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 571, in main
worker1:     actor_loss, critic_loss = trainer.train_rlhf(exp_data)
worker1:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py", line 267, in train_rlhf
worker1:     self.actor_model.step()
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/hybrid_engine.py", line 452, in step
worker1:     super().step(lr_kwargs=lr_kwargs)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2148, in step
worker1:     self._take_model_step(lr_kwargs)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2054, in _take_model_step
worker1:     self.optimizer.step()
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
worker1:     ret_val = func(*args, **kwargs)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1946, in step
worker1:     if self._overflow_check_and_loss_scale_update():
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
worker1:     ret_val = func(*args, **kwargs)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1894, in _overflow_check_and_loss_scale_update
worker1:     self._update_scale(self.overflow)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 2281, in _update_scale
worker1:     self.loss_scaler.update_scale(has_overflow)
worker1:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 175, in update_scale
worker1:     raise Exception(
worker1: Exception: Current loss scale already at minimum - cannot decrease scale anymore. Exiting run.
worker2: Traceback (most recent call last):
worker2:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 697, in <module>
worker2:     main()
worker2:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 571, in main
worker2:     actor_loss, critic_loss = trainer.train_rlhf(exp_data)
worker2:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py", line 267, in train_rlhf
worker2:     self.actor_model.step()
worker2:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/hybrid_engine.py", line 452, in step
worker2:     super().step(lr_kwargs=lr_kwargs)
worker2:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2148, in step
worker2:     self._take_model_step(lr_kwargs)
worker2:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2054, in _take_model_step
worker2:     self.optimizer.step()
worker2:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
worker2:     ret_val = func(*args, **kwargs)
worker2:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1946, in step
worker2:     if self._overflow_check_and_loss_scale_update():
worker2:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
worker2:     ret_val = func(*args, **kwargs)
worker2:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1894, in _overflow_check_and_loss_scale_update
worker2:     self._update_scale(self.overflow)
worker2:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 2281, in _update_scale
worker2:     self.loss_scaler.update_scale(has_overflow)
worker2:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 175, in update_scale
worker2:     raise Exception(
worker2: Exception: Current loss scale already at minimum - cannot decrease scale anymore. Exiting run.
worker2: Traceback (most recent call last):
worker2:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 697, in <module>
worker2:     main()
worker2:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 571, in main
worker2:     actor_loss, critic_loss = trainer.train_rlhf(exp_data)
worker2:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py", line 267, in train_rlhf
worker2:     self.actor_model.step()
worker2:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/hybrid_engine.py", line 452, in step
worker2:     super().step(lr_kwargs=lr_kwargs)
worker2:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2148, in step
worker2:     self._take_model_step(lr_kwargs)
worker2:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2054, in _take_model_step
worker2:     self.optimizer.step()
worker2:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
worker2:     ret_val = func(*args, **kwargs)
worker2:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1946, in step
worker2:     if self._overflow_check_and_loss_scale_update():
worker2:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
worker2:     ret_val = func(*args, **kwargs)
worker2:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1894, in _overflow_check_and_loss_scale_update
worker2:     self._update_scale(self.overflow)
worker2:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 2281, in _update_scale
worker2:     self.loss_scaler.update_scale(has_overflow)
worker2:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 175, in update_scale
worker2:     raise Exception(
worker2: Exception: Current loss scale already at minimum - cannot decrease scale anymore. Exiting run.
worker2: Traceback (most recent call last):
worker2:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 697, in <module>
worker2:     main()
worker2:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 571, in main
worker2:     actor_loss, critic_loss = trainer.train_rlhf(exp_data)
worker2:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py", line 267, in train_rlhf
worker2:     self.actor_model.step()
worker2:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/hybrid_engine.py", line 452, in step
worker2:     super().step(lr_kwargs=lr_kwargs)
worker2:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2148, in step
worker2:     self._take_model_step(lr_kwargs)
worker2:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2054, in _take_model_step
worker2:     self.optimizer.step()
worker2:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
worker2:     ret_val = func(*args, **kwargs)
worker2:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1946, in step
worker2:     if self._overflow_check_and_loss_scale_update():
worker2:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
worker2:     ret_val = func(*args, **kwargs)
worker2:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1894, in _overflow_check_and_loss_scale_update
worker2:     self._update_scale(self.overflow)
worker2:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 2281, in _update_scale
worker2:     self.loss_scaler.update_scale(has_overflow)
worker2:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 175, in update_scale
worker2:     raise Exception(
worker2: Exception: Current loss scale already at minimum - cannot decrease scale anymore. Exiting run.
worker2: Traceback (most recent call last):
worker2:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 697, in <module>
worker2:     main()
worker2:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py", line 571, in main
worker2:     actor_loss, critic_loss = trainer.train_rlhf(exp_data)
worker2:   File "/fs-computility/llm/chenyang2/DeepSpeedExamples-master/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py", line 267, in train_rlhf
worker2:     self.actor_model.step()
worker2:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/hybrid_engine.py", line 452, in step
worker2:     super().step(lr_kwargs=lr_kwargs)
worker2:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2148, in step
worker2:     self._take_model_step(lr_kwargs)
worker2:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2054, in _take_model_step
worker2:     self.optimizer.step()
worker2:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
worker2:     ret_val = func(*args, **kwargs)
worker2:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1946, in step
worker2:     if self._overflow_check_and_loss_scale_update():
worker2:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
worker2:     ret_val = func(*args, **kwargs)
worker2:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1894, in _overflow_check_and_loss_scale_update
worker2:     self._update_scale(self.overflow)
worker2:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 2281, in _update_scale
worker2:     self.loss_scaler.update_scale(has_overflow)
worker2:   File "/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 175, in update_scale
worker2:     raise Exception(
worker2: Exception: Current loss scale already at minimum - cannot decrease scale anymore. Exiting run.
worker3: [2024-06-14 10:24:39,633] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 170620
head: [2024-06-14 10:24:39,738] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 190804
worker2: [2024-06-14 10:24:40,172] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 163000
worker2: [2024-06-14 10:24:40,172] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 163001
worker1: [2024-06-14 10:24:40,182] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 283112
worker1: [2024-06-14 10:24:40,182] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 283113
worker1: [2024-06-14 10:24:40,183] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 283114
worker1: [2024-06-14 10:24:40,356] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 283115
worker2: [2024-06-14 10:24:40,625] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 163002
worker1: [2024-06-14 10:24:40,769] [ERROR] [launch.py:321:sigkill_handler] ['/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/bin/python', '-u', 'main.py', '--local_rank=3', '--data_path', 'Dahoas/full-hh-rlhf', '--data_split', '2,4,4', '--actor_model_name_or_path', '/fs-computility/llm/chenyang2/OpenLLMAI/Llama-2-7b-sft-model-ocra-500k', '--critic_model_name_or_path', '/fs-computility/llm/chenyang2/hf_model/OpenLLMAI/Llama-2-7b-rm-anthropic_hh-lmsys-oasst-webgpt/models--OpenLLMAI--Llama-2-7b-rm-anthropic_hh-lmsys-oasst-webgpt/snapshots/a982afeed00fac9767d53aecde5b88947b1be194', '--num_padding_at_beginning', '1', '--per_device_generation_batch_size', '8', '--per_device_training_batch_size', '8', '--generation_batches', '8', '--ppo_epochs', '1', '--max_answer_seq_len', '1024', '--max_prompt_seq_len', '1024', '--actor_learning_rate', '9.65e-6', '--critic_learning_rate', '5e-6', '--actor_weight_decay', '0.1', '--critic_weight_decay', '0.1', '--num_train_epochs', '1', '--lr_scheduler_type', 'cosine', '--offload', '--gradient_accumulation_steps', '8', '--actor_gradient_checkpointing', '--critic_gradient_checkpointing', '--actor_dropout', '0.0', '--num_warmup_steps', '0', '--deepspeed', '--seed', '1234', '--actor_zero_stage', '3', '--critic_zero_stage', '3', '--enable_hybrid_engine', '--output_dir', './output_step3_llama'] exits with return code = 1
head: [2024-06-14 10:24:40,779] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 190805
head: [2024-06-14 10:24:40,780] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 190806
head: [2024-06-14 10:24:40,781] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 190807
head: [2024-06-14 10:24:40,781] [ERROR] [launch.py:321:sigkill_handler] ['/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/bin/python', '-u', 'main.py', '--local_rank=3', '--data_path', 'Dahoas/full-hh-rlhf', '--data_split', '2,4,4', '--actor_model_name_or_path', '/fs-computility/llm/chenyang2/OpenLLMAI/Llama-2-7b-sft-model-ocra-500k', '--critic_model_name_or_path', '/fs-computility/llm/chenyang2/hf_model/OpenLLMAI/Llama-2-7b-rm-anthropic_hh-lmsys-oasst-webgpt/models--OpenLLMAI--Llama-2-7b-rm-anthropic_hh-lmsys-oasst-webgpt/snapshots/a982afeed00fac9767d53aecde5b88947b1be194', '--num_padding_at_beginning', '1', '--per_device_generation_batch_size', '8', '--per_device_training_batch_size', '8', '--generation_batches', '8', '--ppo_epochs', '1', '--max_answer_seq_len', '1024', '--max_prompt_seq_len', '1024', '--actor_learning_rate', '9.65e-6', '--critic_learning_rate', '5e-6', '--actor_weight_decay', '0.1', '--critic_weight_decay', '0.1', '--num_train_epochs', '1', '--lr_scheduler_type', 'cosine', '--offload', '--gradient_accumulation_steps', '8', '--actor_gradient_checkpointing', '--critic_gradient_checkpointing', '--actor_dropout', '0.0', '--num_warmup_steps', '0', '--deepspeed', '--seed', '1234', '--actor_zero_stage', '3', '--critic_zero_stage', '3', '--enable_hybrid_engine', '--output_dir', './output_step3_llama'] exits with return code = 1
worker3: [2024-06-14 10:24:40,784] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 170621
worker3: [2024-06-14 10:24:40,789] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 170622
worker3: [2024-06-14 10:24:40,794] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 170623
worker3: [2024-06-14 10:24:40,794] [ERROR] [launch.py:321:sigkill_handler] ['/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/bin/python', '-u', 'main.py', '--local_rank=3', '--data_path', 'Dahoas/full-hh-rlhf', '--data_split', '2,4,4', '--actor_model_name_or_path', '/fs-computility/llm/chenyang2/OpenLLMAI/Llama-2-7b-sft-model-ocra-500k', '--critic_model_name_or_path', '/fs-computility/llm/chenyang2/hf_model/OpenLLMAI/Llama-2-7b-rm-anthropic_hh-lmsys-oasst-webgpt/models--OpenLLMAI--Llama-2-7b-rm-anthropic_hh-lmsys-oasst-webgpt/snapshots/a982afeed00fac9767d53aecde5b88947b1be194', '--num_padding_at_beginning', '1', '--per_device_generation_batch_size', '8', '--per_device_training_batch_size', '8', '--generation_batches', '8', '--ppo_epochs', '1', '--max_answer_seq_len', '1024', '--max_prompt_seq_len', '1024', '--actor_learning_rate', '9.65e-6', '--critic_learning_rate', '5e-6', '--actor_weight_decay', '0.1', '--critic_weight_decay', '0.1', '--num_train_epochs', '1', '--lr_scheduler_type', 'cosine', '--offload', '--gradient_accumulation_steps', '8', '--actor_gradient_checkpointing', '--critic_gradient_checkpointing', '--actor_dropout', '0.0', '--num_warmup_steps', '0', '--deepspeed', '--seed', '1234', '--actor_zero_stage', '3', '--critic_zero_stage', '3', '--enable_hybrid_engine', '--output_dir', './output_step3_llama'] exits with return code = 1
worker2: [2024-06-14 10:24:41,039] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 163003
worker2: [2024-06-14 10:24:41,039] [ERROR] [launch.py:321:sigkill_handler] ['/fs-computility/llm/shared/rl3m_env/miniconda3-py39_4/envs/llm-opus-zhaohui/bin/python', '-u', 'main.py', '--local_rank=3', '--data_path', 'Dahoas/full-hh-rlhf', '--data_split', '2,4,4', '--actor_model_name_or_path', '/fs-computility/llm/chenyang2/OpenLLMAI/Llama-2-7b-sft-model-ocra-500k', '--critic_model_name_or_path', '/fs-computility/llm/chenyang2/hf_model/OpenLLMAI/Llama-2-7b-rm-anthropic_hh-lmsys-oasst-webgpt/models--OpenLLMAI--Llama-2-7b-rm-anthropic_hh-lmsys-oasst-webgpt/snapshots/a982afeed00fac9767d53aecde5b88947b1be194', '--num_padding_at_beginning', '1', '--per_device_generation_batch_size', '8', '--per_device_training_batch_size', '8', '--generation_batches', '8', '--ppo_epochs', '1', '--max_answer_seq_len', '1024', '--max_prompt_seq_len', '1024', '--actor_learning_rate', '9.65e-6', '--critic_learning_rate', '5e-6', '--actor_weight_decay', '0.1', '--critic_weight_decay', '0.1', '--num_train_epochs', '1', '--lr_scheduler_type', 'cosine', '--offload', '--gradient_accumulation_steps', '8', '--actor_gradient_checkpointing', '--critic_gradient_checkpointing', '--actor_dropout', '0.0', '--num_warmup_steps', '0', '--deepspeed', '--seed', '1234', '--actor_zero_stage', '3', '--critic_zero_stage', '3', '--enable_hybrid_engine', '--output_dir', './output_step3_llama'] exits with return code = 1
pdsh@t-20240614131131-qkptl-ray-job-head-lx2fh: head: ssh exited with exit code 1
pdsh@t-20240614131131-qkptl-ray-job-head-lx2fh: worker1: ssh exited with exit code 1
pdsh@t-20240614131131-qkptl-ray-job-head-lx2fh: worker3: ssh exited with exit code 1
pdsh@t-20240614131131-qkptl-ray-job-head-lx2fh: worker2: ssh exited with exit code 1
